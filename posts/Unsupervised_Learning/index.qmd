---
title: "Aprendizaje No Supervisado"
subtitle: "Python"
author: "Juan Isaula"
date: "2025-09-07"
categories: [Python]
image: "fondo.png"
---

Supongamos que tiene una colección de clientes con diversas características, como *edad, ubicación e historial financiero*, y deseas descubrir patrones y clasificarlos en grupos. O quizás tengas un conjunto de textos, como páginas de Wikipedia, y quieras segmentarlos en categorías en función de su contenido. Este es el mundo del aprendizaje no supervizado, llamado así porque no estás guiando, o supervisando, el descubrimiento de patrones mediante alguna tarea de predicción, sino descubriendo la estructura oculta a partir de datos no etiquetados. El aprendizaje no supervizado engloba diversas técnicas de aprendizaje automático, desde la agrupación hasta la reducción de dimensiones y la factorización de matrices. En este artículo, aprenderás los fundamentos de **aprendizaje no supervizado** e implementarás los algoritmos esenciales utilizando `scikit-learn` y `SciPy`. Aprenderás a agrupar, transformar, visualizar y extraer información de conjuntos de datos no etiquetados.

# Agrupación para la exploración de conjuntos de datos

El objetivo de esta sección es aprender a descubrir los grupos subyacentes (o clústeres) en un conjunto de datos. En esta sección aprendera a agrupar empresas utilizando sus cotizaciones bursátiles,y distinguir diferentes especies agrupando sus medidas.

## Aprendizaje no supervizado 

El aprendizaje no supervizado es una clase de técnica de aprendizaje automático para descrubrir patrones en los datos. Por ejemplo:

-   Encontrar los grupos naturales de clientes en función de sus historiales de compras o buscar:

-   Patrones y correlaciones entre estas compras, y utilizar estos patrones para expresar los datos en forma comprimida.

Estos son ejemplos de técnicas de aprendizaje no supervizadas llamadas *agrupación* y *reducción de dimensiones.*

> El aprendizaje no supervizado se define en oposición al aprendizaje supervizado.

-   Un ejemplo de ***aprendizaje supervisado*** es utilizar las medidas de los tumores para clasificarlos como benignos o cancerosos. En este caso, el descubrimiento de patrones es guiado o supervizado, de modo que los patrones son lo más utilies posible para predecir la etiqueta: benigno o canceroso.

-   El ***aprendizaje no supervisado***, por el contrario, es un aprendizaje sin etiquetas. Es puro descubrimiento de patrones, sin la guía de una tarea de predicción.

### Iris dataset 

El conjunto de datos iris consta de mediciones de muchas plantas iris de tres especies diferentes:

-   Setosa

-   Versicolor

-   Virginica

Hay cuatro medidas:

1.  Largo de pétalo (Petal length)

2.  Ancho de pétalo (Petal width)

3.  Largo de sépalo (Sepal length)

4.  Ancho de sépalo (sepal width)

Estas son las características del conjunto de datos.

#### Matrices, características y muestras 

Conjuntos de datos como este (iris) se escribiran como matrices numerosas bidimensionales.

-   Las columnas de la matriz corresponderán a las características.

-   Las medidas de plantas individuales son las muestras del conjunto de datos, estas corresponden a filas de la matriz.

#### Iris datases es 4-dimensional 

Las muestras del conjunto de datos iris tienen cuatro medidas y, por lo tanto, corresponden a puntos en un espacio de cuatro dimensiones. Es decir:

-   Dimensiones = número de características o features.

No podemos visualizar cuatro dimensiones directamente, pero utilizando técnicas de aprendizaje no supervizado aún podemos obtener información.

### K-Means Clustering 

Agruparemos estas muestras utilizando la agrupación de k-Means. K-Means encuentra un número específico de grupos en las muestras. Está implementado en la biblioteca de `scikit-learn` o `sklearn`.

Veamos KMeans en acción con el conjunto de datos iris.

```{python}
import pandas as pd         
from sklearn import datasets    # Librería utilizada para importar iris dataset
iris = datasets.load_iris()     # importamos el conjunto de datos iris

iris = pd.DataFrame(iris.data, columns = iris.feature_names) # Convertimos iris a dataframe
```

Para comenzar:

1.   Importamos KMeans de scikit-learn.

2.  Luego creamos un modelo KMeans, especificando la cantidad de clústeres que deseamos encontrar con `n_clusters` especificamos n_cluster = 3, ya que hay tres especies de iris.

3.  Posteriormente, llamamos el método de ajuste del modelo `.fit()`, pasando la matriz del iris dataset. Esto ajusta el modelo a los datos, localizando y recordando las regiones donde ocurren los diferentes grupos.

4.  Por último, podemos utilizar el método de predicción del modelo en este mismo conjunto de datos.

```{python}
from sklearn.cluster import KMeans

model = KMeans(n_clusters = 3, random_state = 42)
model.fit(iris)
```

```{python}
labels = model.predict(iris)
species_ = labels
print(labels)
```

Esto nos devuelve una etiqueta de grupo para cada muestra, que indica a qué grupo pertenece una muestra.

#### Etiquetas de clúster para nuevas muestras 

Si alguien viene con un algún dataset iris nuevo, kMeans puede determinar a qué grupos pertenecen sin tener que empezar de nuevo.

KMeans hace esto recordando la media (o promedio) de las muestras en cada grupo. Estos se llaman **centroides** se asignan nuevas muestras al grupo cuyo centroide esté más cercano.

Voy a tomar 3 registros aleatorios del dataset iris y asumiremos que estas son muestras nuevas.

```{python}
new_samples = iris.sample(n = 3, random_state = 42)
print(new_samples)
```

Para asignar las nuevas muestras a los grupos existentes, pasaremos el conjunto de nueva muestra al método de predicción del modelo kmeans.

```{python}
new_labels = model.predict(new_samples)
print(new_labels)
```

Como puede observar, esto devuelve las etiquetas de grupo para los datos nuevos o muestra de datos nueva.

> En la siguiente sección aprenderá cómo evaluar la calidad de su agrupación.

#### Scatter Plots

Por ahora, visualizaremos nuestra agrupación de las muestras de iris usando diagramas de dispersión. A continuación se muestra un diagrama de dispersión de la longitud del sépalo frente a la longitud del pétalo del dataset iris.

![](img/fig1.png){fig-align="center" width="300"}

Cada punto representa una muestra de iris y está coloreada según el grupo de la muestra. Para crear un diagrama de dispersión como este, usaremos `PyPlot`.

-   La longitud del sépalo está en la columna 0 de la matriz iris, mientras que la longitud de los pétalos está en la segunda columna.

-   Y labels o etiquetas que encontramos previamente lo usamos para colorear por etiqueta de clúster como un paramétro en `.scatter()`.

```{python}
import matplotlib.pyplot as plt 

xs = iris.iloc[:, 0]   # Longitud de los sépalos 
ys = iris.iloc[:, 2]   # Longitud de los pétalos

plt.scatter(xs, ys, c = labels)
plt.show()
```

Ahora, calculemos las coordenadas de los centroides utilizando el atributo `.cluster_centers_` de `model`. y asignaremos la columna 0 de `centroids` a `centroids_x`, y la columna 2 de `centroids` a `centroids_y`. Posterior a ello, realizaremos un diagrama de dispersión de centroids_x y centroids_y, utilizando `marker = 'D'` (un rombo) como marcador especificando el parámetro marker. El tamaño de los marcadores en 50 utilizando `s = 50`.

```{python}
xs = iris.iloc[:, 0]   # Longitud de los sépalos 
ys = iris.iloc[:, 2]   # Longitud de los pétalos

plt.scatter(xs, ys, c = labels, alpha = 0.5)
centroids = model.cluster_centers_

centroids_x = centroids[:,0]
centroids_y = centroids[:,2]

plt.scatter(centroids_x, centroids_y, marker = 'D', s = 50)
plt.show()
```

## Evaluar una agrupación 

En la sección anterior, utilizamos KMeans para agrupar el dataset de iris en tres grupos. Pero **¿cómo podemos evaluar la calidad de esta agrupación?**

Un enfoque directo es comparar los grupos con las especies de iris. Primero aprenderá sobre esto, antes de considerar el problema de cómo medir la calidad de una agrupación de una manera que no requiera que nuestras muestras vengan preagrupadas en especies. Esta medida de calidad puede utilizarse más adelante para tomar una decisión informada sobre el número de conglomerados a buscar.

### Iris: Clusters vs Especies

En primer lugar, comprobemos si los 3 grupos de muestras de iris tienen alguna correspondencia con la especie de iris.

La correspondencia se describe en esta tabla:

![](img/fig2.png){fig-align="center" width="400"}

Existe una columna para cada una de las tres especies de iris: setosa, versicolor y virginica, y una fila para cada una de las tres etiquetas de grupo: 0, 1 y 2.

La tabla muestra el número de muestras que tienen cada combinación posible de etiquetas de grupo/especie. Por ejemplo:

-   Vemos que el grupo 0 se corresponde perfectamente con la especie setosa.

-   Por otro lado, mientras que el cluster 1 contiene principalmente muestras de virginica en el grupo 2.

Tablass como esta, se denominan **tabulaciones cruzadas** o **Cross tabulation**

### Cross Tabulation con Pandas

Para construir uno, usareos la biblioteca `pandas.` Como podemos observar en el bloque de código siguiente, creamos un dataframe de dos columnas, donde la primera columna son las etiquetas del grupo y la segunda son las especies de iris, de modo que cada fila proporciona la etiqueta del grupo y la especie de una sola muestra.

```{python}
iris_raw = datasets.load_iris() 
iris = pd.DataFrame(iris_raw.data, columns = iris_raw.feature_names)
# Agregamos columna de especie verdadera al dataframe iiris
species = pd.Categorical.from_codes(iris_raw.target, iris_raw.target_names)

# 

df = pd.DataFrame({'labels':labels, 'species':species})
print(df)
```

Ahora usamos la función de tabla cruzuda de pandas para crear la tubulación cruzada, pasando las dos columnas del DataFrame.

```{python}
ct = pd.crosstab(df['labels'], df['species'])
print(ct)
```

Tabulaciones cruzadas como estás proporcionan información valiosa sobre qué tipo de muestras se encuentran en qué grupo. Pero la mayoría de los conjuntos de datos, las muestras no están etiquetadas por especie.

**¿Cómo se puede evaluar la calidad de un clustering en estos casos?**

### Medición de la calidad de la agrupación 

Una buena agrupación tiene grupos compactos, lo que significa que las muestras de cada grupo están agrupadas, no dispersas.

La **inercia** puede medir la distribución de las muestras dentro de cada grupo. Intuitivamente, la inercia mide qué tan lejos están las muestras de sus centroides. Puede encontrar la definición precisa en la documentación de scikit-learn. Queremos grupos que no estén dispersos, por lo que los valores más bajos de inercia son mejores.

La inercia de un modelo KMeans se mide automaticamente cuando se llama al metodo de ajuste .fit y luego están disponibles como atributos de `inertia_`.

De hecho, KMeans pretende colocar los clusters de forma que se minimice la inercia.

```{python}
model = KMeans(n_clusters = 3, random_state = 42)
model.fit(iris)
print(model.inertia_)
```

A continuación se muestra un gráfico de los valores de inercia de las agrupaciones del conjunto de datos de iris con diferentes números de agrupaciones.

![](img/fig3.png){fig-align="center" width="400"}

Nuestro modelo KMeans con 3 grupos tiene una inercia relativamente baja, lo cual es genial. Pero observe que la incercia continúa disminuyendo lentamente. Entonces, **¿cuál es la mejor cantidad de clústeres para elegir?** En última instacia se trata de una compensación.

Una buena agrupación tiene agrupaciones estrechas (lo que significa inercia baja). Pero tampoco tiene demasiados grupos.

Una buena regla general es elegir un codo en el gráfico de inercia, es decir, un punto donde la inercia comienza a disminuir más lentamente.

Por ejemplo, según este criterio, 3 es un buen número de grupos para el conjunto de datos del iris.

```{python}
ks = range(1, 11)
inertias = []

for k in ks:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(iris)
    inertias.append(model.inertia_)   # suma de distancias cuadradas intracluster

plt.plot(ks, inertias, '-o')
plt.xlabel('número de clusters')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()
```

## Transformación de Características para mejorar agrupaciones 

Veamos ahora otro conjunto de datos, el conjunto de datos de los vinos del Piamonte.

-   Disponemos de 178 muestras de vino tinto de la región de Piamonte de Italia.

-   Las entradas o features miden la composición química (como el contendio del alcohol) y propiedades visuales como la intensidad del color.

-   Las muestras proceden de 3 variedades distintas de vino.

```{python}

samples = pd.read_csv("wine.csv")
varieties = samples['class_name']
samples = samples.drop("class_name", axis = 1)
samples
```

### Clustering con Vinos 

Tomaremos la matriz de muestra de los vinos y usaremos KMeans para encontrar 3 grupos

```{python}
model = KMeans(n_clusters = 3, random_state = 42)
labels = model.fit_predict(samples)
```

#### Cluster vs Varieties 

Hay tres variedades de vinom así que usamos pandas para crear la tabla cruzada para comprobar las correspondencia entre la etiqueta del clúster y la variedad de vino.

```{python}
df = pd.DataFrame({'labels': labels, 'varieties':varieties})
ct = pd.crosstab(df['labels'], df['varieties'])
print(ct)
```

Como podemos ver, esta vez las cosas no han salido tan bien. Los clusters KMeans no se corresponden bien con las variedades de vino.

#### Variación de Características

El problema es que las características del conjunto de datos del vino tiene variaciones muy diferentes.

> La varianza de una característica mide la dispersión de sus valores

![](img/fig4.png){fig-align="center" width="400"}

Por ejemplo, la característica del ácido málico (`malic_acid`) tiene una mayor varianza que la característica `0d280`, y esto también se puede ver en su diagrama de dispersión.

Las diferencias en algunas de las variaciones de características son enormes, por ejmplo, en el diagrama de dispersión de características `od280` y `prolina`.

![](img/fig5.png){fig-align="center" width="300"}

### StandarScaler

En la agrupación en clústeres de KMeans, la varianza de una característica corresponde a su influencia en el algoritmo de agrupación en clústeres. Es decir:

-   <div>

    > Varianza de Carasterística = Influencia de Característica

    </div>

Para darle una oportunidad a cada característica, los datos deben transformarce para que las características tengan la misma varianza. Esto se puede lograr con el `StandarScaler` de scikit-learn. Transforma cada característica para que tenga media 0 y varianza 1.

Las características **estandarizadas** resultantes pueden ser muy informativas.

Si utilizamos, por ejemplo, los valores estandarizados de `od280` y `proline`, las tres variedades de vino son mucho más distintas.

![](img/fig6.png){fig-align="center" width="800"}

Veamos StandarScaler en acción:

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() # Creamos el objeto StandarScaler 
scaler.fit(samples)
StandardScaler(copy = True, with_mean = True, with_std = True)
samples_scaled = scaler.transform(samples)
```

El método de transformación ahora se puede utilizar para estandarizar cualquier muestra, ya sean las mismas o completamente nuevas.

#### Métodos similares

> Las API de StandarScaler y KMeans son similares, pero hay una diferencia importante:
>
> -   StandardScaler transforma datos y por eso tiene un método de transformación.
>
> -   KMeans por el contrario, asigna etiquetas de clúster a las muestras y esto se hace utilizando el método de predicción.

### StandardScaler luego KMeans

Volvamos al problema del agrupamiento de los vinos. Necesitamos realizar dos pasos:

1.  Estandarizar los datos utilizando StandardScaler y

2.  Tomar los datos estandarizados y agruparlos utilizando KMeans

Esto se hace fácilmente combinando los dos pasos mediante un pipeline de scikit-learn, luego, los datos fluyen de un paso al siguiente de forma automática.

#### Pipelines combinando multiples pasos

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline

scaler = StandardScaler()
kmeans = KMeans(n_clusters = 3, random_state = 42)

# Aplicamos los pasos que queremos combinar
pipeline = make_pipeline(scaler, kmeans)
pipeline.fit(samples)

# Utilizamos el método de predicción para obtener las etiquetas del clúster

labels = pipeline.predict(samples)


```

Ahora, realicemos la comprobación de la correspondencia entre las etiquetas de los cluster y las variedadesd de vino

```{python}
df = pd.DataFrame({'labels': labels, 'varieties':varieties})
ct = pd.crosstab(df['labels'], df['varieties'])
print(ct)
```

> Y como podemos observar, esto nos revela que al incorporar estandarización la nueva agrupación es fantástica. Sus tres clúster corresponden casi exactamente a las tres variedades de vino. Por tanto, esto es una gran mejora con respecto a la agrupación sin estandarización.

**StandardScaler** es un ejemplo de un paso de *preprocesamiento*. Existen varios de estos disponibles en Scikit-Learn, por ejemplo, MaxAbsScaler y Normalizer.

# **Visualización con agrupamiento Jerárquico y t-SNE**

En este capítulo, aprenderás dos técnicas de aprendizaje no suprvisado para la visualización de datos: la **agrupación jerárquica** y **t-SNE**.

**La agrupación jerárquica** fusiona las muestras de datos en grupos cada vez más amplios, lo que da como resultado una visualización en forma de árbol de la jerarquía de grupos resultantes.

**t-SNE** mapea las muestras de datos en un espacio bidimensional para que se pueda visualizar la proximidad antre ellas.

## Visualizar Jerarquías 

Una gran parte del trabajo de cualquier cientifico de datos es la comunicación de los resultados o conocimiento a otras personas. Las visualizaciones son una excelente manera de compartir sus hallazgos especialmente con una audiencia no técnica. En este nuevo capítulo, aprenderá sobre dos métodos de aprendizaje no supervisado correspondientes a visualización:

-   t-SNE

-   Agrupamiento Jerárquico

**t-SNE**, que lo veremos más adelante, crea un mapa 2D de cualquier conjunto de datos y transmite información útil sobre la proximidad de las muestras entre sí. Pero primero aprendamos sobre la agrupación jerárquica.

Ya has visto muchas agrupaciones jerárquicas en el mundo real. Por ejemplo, los seres vivos pueden organizarse en grupos pequeños y estrechos como los humanos, los simios, las serpientes y los lagartos, o en grupos más grandes y amplios, como los mamíferos y los reptiles, o incluso grupos más amplios, como los animales y las plantas. Como puede visualizar en la figura de abajo, estos grupos están contenidos unos dentro de otros y forman una jerarquía.

![](img/fig7.png){fig-align="center" width="450"}

De manera análoga, la agrupación jerárquica organiza las muestras en una jerarquía de grupos.

La **agrupación jerárquica** puede organizar cualquier tipo de datos en una jerarquía, no sólo muestras de plantas y animales.

### Dataset de scoring Eurovision

Consideremos un nuevo tipo de conjunto de datos que describe cómo los países calificaron sus actuaciones en el Festival de la Canción de Eurovisión 2016. Los datos se organizan en una matriz rectangular, donde las filas de la matriz muestra cuántos puntos le dio un país a cada canción.

![](img/fig8.png){fig-align="center" width="400"}

Las *muestras* en este caso son los países. El resultado de aplicar la agrupación jerárquica al Festival de Eurovisión:

![](img/fig9.png){fig-align="center" width="500"}

Las puntuaciones se pueden visualizar como u diagrama en forma de árbol llamado ***dendrograma***. Esta única imagen revela mucha información sobre el comportamiento electoral de los países en Eurovisión. El dendrograma agrupa a los países en grupos cada vez más grandes, y muchos de ellos los grupos se reconocen inmediatamente como compuestos por países cercanos entre si geográficamente, o que tienen estrechos vínculos culturales o político, o que pertenecen a un solo grupo linguístico. De esta manera, la agrupación jerárquica puede generar excelentes visualizaciones. **¿Pero cómo funciona?**

La agrupación jerárquica se realiza en pasos:

-   Al principio cada país es su propio clúster,por lo que hay tantos clústeres como países.

-   En cada paso se fusionan los dos clústeres más cercanos. Esto disminuye el número de clústeres, y

-   Al final, sólo queda un grupo, que contiene todos los países.

Este proceso es en realidad un tipo particular de agrupamiento jerárquico llamdo **agrupamiento aglomerativo,** también existe el **agrupamiento divisivo**, que funciona a la inversa.

Aún no hemos definido qué significa que dos clústeres estén cerca, pero volveremos a abordar este tema más adelante.

### Dendrograma de una Clusterización Jerárquica 

Todo el proceso de agrupamiento jerárquico está codificado en el dendrograma.

![](img/fig9.png){fig-align="center" width="500"}

En la parte inferior, cada país se encuentra en un grupo propio. Luego el agrupamiento continúa desde abajo hacia arriba.

Los grupos se representan como líneas verticales y una unión de líneas verticales indica una fusión de grupos.

Para comprender mejor, hagamos un acercamiento:

![](img/fig10.png){fig-align="center" width="500"}

Observemos sólo una parte de este dendrograma,

![](img/fig11.png){fig-align="center" width="174"}

Al principio hay seis grupos , cada uno de los cuales contiene sólo un país. La primera fusión se produce aquí:

![](img/fig12.png){fig-align="center" width="174"}

donde los grupos que contienen a Cyprus y Grecia se fusionan en un solo grupo. Posteriormente, este nuevo clúster se fusiona con el clúster que contiene Bulgaria:

![](img/fig13.png){fig-align="center" width="174"}

Poco después, los grupos que incluyen a Moldova y Rusia son fusionados,

![](img/fig14.png){fig-align="center" width="174"}

que más tarde a su vez se fusiona con el grupo que contiene Armenia.

![](img/fig15.png){fig-align="center" width="174"}

Más tarde aún, los dos grupos compuestos se fusionan, Este proceso continua hasta que sólo quede un grupo, y éste contenga todos los países.

![](img/fig16.png){fig-align="center" width="500"}

### Clusterización Jerárquica con SciPy

En el capítulo anterior, utilizamos la agrupación KMeans para agrupar empresas según los movimientos de sus cotizaciones bursátiles. Ahora, realizaremos la agrupación jerárquica de las empresas.

linkage() de SciPy realiza una agrupación jerárquica en una matriz de muestras.

```{python}
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram
from sklearn.preprocessing import normalize
import numpy as np

empresas = pd.read_csv("/Users/juanisaulamejia/Documents/webmathJI/posts/Unsupervised_Learning/empresas.csv")

companies = empresas.iloc[:, 0].tolist()

empresas = empresas.select_dtypes(include="number").copy()

arr = empresas.to_numpy()
normalized_movements = normalize(arr)

# Calculate the linkage: mergings
mergings = linkage(normalized_movements, method = 'complete')

# Plot the dendrogram
dendrogram(mergings, labels = companies,
leaf_rotation = 90,
leaf_font_size = 6)
plt.show()
plt.show()
```

Como podemos observar, podemos crear visualizaciones geniales como esta con el clustering jerárquico, pero se puede usar ara más que solo visualizaciones.

## Etiquetas de clústeres en la agrupación jerárquica 

En la sección anterior empleamos la agrupación jerárquica. Creamos una gran visualización de las empresas según sus movimientos de sus cotizaciones bursátiles. Pero la **agrupación jerárquica no es sólo una herramienta de visualización**.

En esta sección aprenderá cómo extraer los clústeres de las etapas intermedias de una agrupación jerárquica. Las etiquetas de clúster para estos agrupamientos intermedios se pueden utilizar posteriormente en otros cálculos, como tabulaciones cruzadas, al igual que las etiquetas de clúster de KMeans.

### Agrupaciones intermedias y altura en el dendrograma

Una etapa intermedia en la agrupación jerárquica se especifica eligiendo una altura en el dendrograma.

![](img/fig17.png){fig-align="center" width="200"}

Por ejemplo, elegir una altura de 15 define una agrupación en el que Bulgaria, Cyprus y Grecia están en un grupo, Rusia y Moldavia en otro, y Armenia en un grupo propio.

Pero **¿qué significa la altura?**

El eje `y` del dendrograma codifica la distancia entre los grupos fusionados. Por ejemplo, la distancia entre el grupo que contiene Cyprus y el número de países que contenían Grecia era de aproximadamente 6 cuando se fusionaron en un solo grupo. Cuando este nuevo cúmulo se fusiono con el cúmulo que contenía a Bulgaria, la distancia entre ellos era de 12. Por lo tanto, la altura que especifica una agrupación intermedia corresponde a una distancia. Esto especifica que la agrupación jerárquica debe detenerse a fusionar clústeres cuando todos los clústeres están al menos a esta distancia.

### Distancia entre Clusters

La distancia entre dos grupos se mide utilizando un **método de enlace (método `linkage`).** En nuestro ejemplo, utilizamos un enlace **completo**, donde la distancia entre dos clusteres es el máximo de las distancias entre sus muestras. Esto se especificó a través del parámetro "method" o en python `linkage(samples, method = "complete")`. Existen muchos otros métodos de vinculación y los veremos en algunos ejercicios que realizaremos en Python con Scikit-Learn.

> Los diferentes métodos de vinculación producen diferentes agrupaciones jerárquicas.

### Extracción de etiquetas de clúster mediante `fcluster.` 

La etiquetas de clúster para cualquier etapa intermedia se pueden extraer utilizando la función `fcluster`. Vamos a probarlo, especificando la altura de 15. Despues de realizar la agrupación jerárquica de los datos de empresas, importamos la función `fcluster`. Luego, pasamos el resultado de la función de vinculación a la función fcluster, especificando la altura como segundo argumento.

```{python}
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import fcluster

mergings = linkage(arr, method = 'complete')
labels   = fcluster(mergings, 15, criterion = 'distance')
print(labels)
```

Esto nos devolvio una matriz numpy que contiene las etiquetas de clúster para todos los países.

### Alineación de las etiquetes de los clústeres con los nombres de las empresas 

Para inspeccionar las etiquetas de los clústeres, usaremos un DataFrame para alinear las etiquetas con los nombres de los países.

```{python}
import pandas as pd 
pairs = pd.DataFrame({'labels': labels, 'companies': companies}) 
print(pairs.sort_values('labels'))

```

Como era de esperar, las etiquetas del clúster agrupan a Intel, Texas instruments en el mismo clúster. Pero tenga en cuenta que las etiquetas de clúster de scipy comienzan en 1, no en 0 como lo hacen en `scikit-learn.`

### t-SNE para mapas bidimensionales 

En esta sección, aprenderá un método de aprendizaje no supervisado para visualización llamado `t-SNE`.

> t-SNE significa "incrustación de vecinos estocásticos distribuidos en t"

Tiene un nombre complicado, pero tiene un propósito muy simple. Mapea muestras de su espacio de alta dimensión a un espacio de 2D o 3D dimensiones para que puedan visualizarse. Si bien es inevitable cierta distorsión, t-SNE hace un gran trabajo de representar aproximadamente las distancias entre las muestras. Por este motivo, t-SNE es una ayuda visual invaluable para comprender un conjunto de datos.

#### t-SNE en el dataset iris 

Para ver qué tipo de conocimientos son posibles con t-SNE, veamos cómo funciona en el conjunto de datos del iris. Las muestras de iris están en un espacio de cuatro dimensiones, donde cada dimensión corresponde a una de las cuatro medidas del iris, como la longitud y el ancho de los pétalos.

Ahora a t-SNE solo se le dieron las medidas de las muestras de iris. En particular, no se proporcionó ninguna información sobre las tres especies de iris. Pero si coloreamos las especies de manera diferente en el diagrama de dispersión vemos que t-SNE ha mantedio las especies separadas.

![](img/fig18.png){fig-align="center" width="500"}

Sin embargo, este diagrama de dispersión nos da una nueva perspectiva. Aprendemos que hay dos especies de iris, versicolor y virginica, cuyas muestras están muy juntas en el espacio. Por lo tanto, podría suceder que el conjunto de datos del iris parezca tener dos grupos en lugar de tres. Esto es compatible con nuestros ejemplos anteriores usando KMeans, donde vimos que la agrupación con 2 grupos también tenía una inercia relativamente baja, lo que significa grupos apretados.

#### t-SNE en Sklearn 

t-SNE está disponible en scikit-learn, pero funciona un poco de manera diferente a los componentes de ajuste/transformación que ya conoce.

Para comenzar, importamos `TSNE` y creamos un objeto TNSE. Aplicamos el método `fit_transform` a las muestras y luego hacemos un diagrama de dispersión del resultado, colerando los puntos según la especie.

```{python}
from sklearn.manifold import TSNE

model = TSNE(learning_rate = 100)
transformed = model.fit_transform(iris)
xs = transformed[:, 0]
ys = transformed[:, 1]
plt.scatter(xs, ys, c = species_)
plt.show()
```

Hay dos puntos que merecen especial atención:

-   El método `fit_transform` y la

-   tasa de aprendizaje (`learning_rate`)

t-SNE solo tiene un método `fit_transform()`. Como era de esperar, el método fit_transform ajusta el modelo y transforma los datos simultáneamente. Sin embargi, t-SNE no tiene métodos de ajuste o `fit()` y transformación o `transform()` separados. Esto significa que no se puede ampliar un mapa t-SNE para incluir nuevas muestras. En cambio, debes empezar de nuevo cada vez.

La segunda cosa a tener en cuenta es la `tasa de aprendizaje.` Está hace que el uso de t-SNE sea más complicado que otras técnicas. Es posible que deba probar diferentes tasas de aprendizaje para diferentes conjuntos de datos.

Está claro, sin embargo, cuando has hecho una mala elección, porque todas las muestras aparecen agrupadas en el diagrama de dispersión. Normalmente es suficiente probar algunos valores entre 50 y 200.

Una última cosa a tener en cuenta es que los ejes de un gráfico t-SNE no tienen ningún significado interpretable. De hecho son diferentes cada vez que se aplica t-SNE, incluso con los mismos datos. Por ejemplo, aquí hay tres gráficos t-SNE de muestras de vino de Piamonte escaladas, generados con el mismo código.

![](img/fig19.png){fig-align="center" width="800"}

Tenga en cuenta que si bien la orientación de la trama es diferente cada vez, los tres vinos, las variedades, representadas aquí mediante colores, tienen la misma posición entre si.

### Mapa t-SNE del mercado bursátil

t-SNE proporciona excelentes visualizaciones cuando se pueden etiquetar las muestras individuales. En el siguiente bloque de código, se aplica t-SNE a los datos del precio de las acciones de la empresa. Un gráfico de dispersión de las características t-SNE resultantes, etiquetadas con los nombres de las empresas, te ofrece un mapa del mercado de valores. Los movimientos de las cotizaciones bursátiles de cada empresa están disponibles en `normalized_movements` las cuales normalizamos anteriormente.

```{python}
# Import TSNE
from sklearn.manifold import TSNE

model = TSNE(learning_rate = 50)

tsne_features = model.fit_transform(normalized_movements)

xs = tsne_features[:, 0]

ys = tsne_features[:,1]

# Scatter plot
plt.scatter(xs, ys, alpha = 0.5)

for x, y, company in zip(xs, ys, companies):
    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
plt.show()

```

# Descorrelación de Datos y Reducción de Dimensiones 

La reducción de dimensiones resume un conjunto de datos utilizando sus patrones comunes. En esta sección, aprenderás la técnica más fundamental de reducción de dimensiones, el **análisis de componentes principales** (PCA, por sus siglas en inglés). El PCA se utiliza a menudo antes del aprendizaje supervisado para mejorar el rendimiento y la generalización del modelo. También puede ser útil para aprendizaje no supervisado. Por ejemplo, utilizaremos una variante PCA que te permitirá agrupar articulos de wikipedia según su contenido.

## Visualización de la transformación PCA 

La reducción de dimensiones encuentra patrones en los datos y utiliza estos patrones para reexpresarlos en una forma comprimida. Esto hace que el cálculo posterior con los datos sea mucho más sencillo, eficiente y, esto puede ser de gran importancia en un mundo de grandes conjuntos de datos.

Sin embargo, la función más importante de la reducción de dimensión es reducir un conjunto de datos a su ***"esqueleto básico"***, descartando características ruidosas que causan grandes problemas para las tareas de aprendizaje supervisado como regresión y clasificación.

> En muchas aplicaciones del mundo real, es la reducción de dimensiones la que hace posible la predicción.

### Análisis de componentes principales 

Esta es la técnica de reducción de dimensión más fundamental. Se llama *"Análisis de componentes principales"*, o *"PCA"* para abreviar. PCA realiza la reducción de dimensión en dos pasos, y el primero:

1.  Llamada **"descorrelación"**, no cambia la dimensión de los datos en absoluto.

Es justamente en este primer paso en el cual nos centraremos.

En este **primer paso**, PCA gira las muestras para que queden alineadas con los ejes de coordenadas. De hecho, hace más que eso: PCA también desplaza las muestras para que tengan media cero.

![](img/fig20.png){fig-align="center" width="600"}

Estos gráficos de dispersión muestran el efecto del PCA aplicado a dos características del conjunto de datos del vino. Tenga en cuenta que no se pierde información: esto es así sin importar cuántas características tenga su conjunto de datos. Veremos la visualización de esta transformación en ejercicios más adelante.

### PCA en Scikit - Learn

Scikit-learn tiene una implementación de PCA y tiene dos métodos de ajuste y transformación como StandardScaler.

-   `fit()` el método de ajuste aprende cómo desplazar y rotar las muestras, pero en realidad no las cambia.

-   `transform()` el método de transformación, por otro lado, aplica la transformación que se aprendio. En particular, el método de transformación se puede aplicar a muestras nuevas e invisibles.

Veamos PCA en acción en algunas características del conjunto de datos de vino, estas características son: `total_phenols` y `od280`.

```{python}
samples = pd.read_csv("wine.csv")
varieties = samples['class_name']
samples = samples.drop("class_name", axis = 1)
samples = samples[["total_phenols", "od280"]]
samples
```

Veamos:

1.  En primer lugar importamos el objeto PCA

2.  Luego creamos el objeto PCA y los ajustamos a las muestras o samples.

```{python}
from sklearn.decomposition import PCA

model = PCA()
model.fit(samples)

```

Ahora utilizamos el objeto PCA ajustado para transformar las muestras. Estos nos devuelve una matriz de muestras transformadas.

```{python}
transformed = model.transform(samples)
print(transformed)
```

Esta nueva matriz tiene el mismo número de filas y columnas que la matriz de muestra original. En particular, hay una fila para cada muestra transformada. Las columnas de la nueva matriz corresponden a *"características de PCA"*, tales como las características originales que correspondían a columnas de la matriz original.

### LAs características de PCA no están correlacionadas

A menudo ocurre que las características de un conjunto de datos están correlacionadas. Lo mismo ocurre con muchas de las características del conjunto de datos del vino, por ejemplo. Sin embargo, del PCA, debido a la rotación que realiza, *"descorrelaciona"* los datos, en el sentido de que las columnas de la matriz transformada no están correlacionadas linealmente.

Como sabemos la correlación lineal se puede medir con la `correlación de Pearson.` Toma valores entre -1 y 1, donde los valores mayores indican una correlación más fuerte y 0 indica que no hay correlación lineal. A continuación se muestran algunos ejemplos de características con distintos grados de correlación.

![](img/fig21.png){fig-align="center" width="600"}

### Componentes Principales

Finalmente, al PCA se le denomina *"análisis de componentes principales"* porque aprende los *"componentes principales"* de los datos. Estas son las direcciones en las que las muestran varían más, representadas aquí en rojo.

![](img/fig22.png){fig-align="center" width="300"}

Son los componentes principales que PCA alinea con los ejes de coordenadas. Una vez ajustado un modelo PCA, los componentes proncipales están disponibles como atributos de componentes.

```{python}
print(model.components_)
```

Esta es una matriz numpy con una fila para cada componente principal. \

## Dimensión Intrínseca

Consideremos un conjunto de datos con 2 características: **latitud** y **longitud**.

![](img/fig23.png){fig-align="center" width="300"}

Estas dos características podrían rastrear el vuelo de un avión, por ejemplo. Este conjunto de datos es bidimensional, pero resulta que se puede analizar observando el desplazamiento a lo largo de la trayectoria de vuelo.

![](img/fig24.png){fig-align="center" width="300"}

Este conjunto de datos es intrínsicamente unidimensional.

### Dimensión Intrínsica

La dimensión intrínsica de un conjunto de datos es el número de características necesarias para aproximarlo. La dimensión intrínsica informa la reducción de la dimensión, porque nos dice cuánto se puede comprimir un conjunto de datos.

Obtendremos una comprensión sólida de la dimensión intrínsica y seremos capaz de utilizar PCA para aidentificarlo en conjuntos de datos del mundo real que tienen miles de características.

Para ilustrar mejor la dimensión intrínseca, consideremos un conjunto de datos de ejemplo que contiene solo algunas de las muestras del conjunto de datos iris. En concreto, tomaremos tres medidas del iris.

-   sepal length (largo del sépalo)

-   sepal width (ancho del sépalo)

-   petal width (ancho del pétalo)

```{python}
iris_sample = iris[['sepal length (cm)', 'sepal width (cm)','petal width (cm)']]
iris_sample
```

Cada muestra se representa como un punto en el espacio tridimensional. Sin embargo, si hacemos un gráfico de dispersión 3D de las muestras, vemos que todas están muy cerca de una lámila plana bidimensional.

![](img/fig25.png){fig-align="center" width="300"}

Esto significa que los datos se pueden aproximar utilizando sólo dos coordenadas, sin perder mucha información. Entonces, este conjunto de datos tiene una dimensión intrínseca de 2. PEro los gráficos de dispersión solo son posibles si hay 3 características o menos.

### PCA identifica la dimensión intrínseca 

**Cómo se puede identificar la dimensión intrínseca, si hay muchas características?**

Aquí es donde el PCA resulta muy útil. La dimensión intrínseca se puede identificar contando las características de PCA que tienen alta varianza. Para ver cómo, veamos qué sucece cuando se aplica PCA al conjunto de datos de muestra versicolor.

![](img/fig26.png){fig-align="center" width="400"}

PCA gira y desplaza las muestras para alinearlas con los ejes de coordenadas. Esto expresa las muestras utilizando tres características PCA.

### Las carasterísticas de PCA se ordenan por varianza descendente

Las características de PCA están en un orden especial.

![](img/fig27.png){fig-align="center" width="500"}

Aquí se muestra un gráfico de barras que muestra la varianza de cada una de las características de PCA. Como se puede ver, cada característica de PCA tiene menos variación que la primera. Esto concuerda con el diagrama de dispersión de las características de PCA, donde las muestras no varían mucho en la dirección vertical. En las otras dos direcciones, sin embargo, la variación es evidente.

### Varianza y Dimensión Intrínseca 

-   La dimensión intrínseca es el número de características de PCA que tiene una varianza significativa.

    ![](img/fig28.png){width="200"}

En nuestro ejemplo, solo las dos primeras características de PCA tienen una varianza significativa. Por lo tanto, este conjunto de datos tiene una dimensión intrínseca de 2, lo que concuerda con lo que observamos al inspeccionar el diagrama de dispersión.

### Plotting de varianza de PCA 

Veamos como gráficar las varianzas de las características del PCA en la práctica.

```{python}
pca = PCA()
pca.fit(iris_sample)

```

Ahora creeamos un rango que enumere las características de PCA

```{python}
features = range(pca.n_components_)
```

Hacemos un diagrama de barras de la varianza; las varianzas están disponibles como el atributo `explained_variance` del modelo PCA.

```{python}
plt.bar(features, pca.explained_variance_)
plt.xticks(features)
plt.ylabel('variance')
plt.xlabel('PCA feature')
plt.show()
```

La dimensión intrínseca es una idea útil que ayuda a orientar la reducción de dimensión. Sin embargo, no siempre es inequívoco. Aquí se muestra u gráfico de las variaciones de las características de PCA para el conjunto de datos del vino.

![](img/fig29.png){fig-align="center" width="400"}

Podriamos argumentar a favor de una dimensión intrínseca de 2, de 3 o incluso más, dependiendo del umbral que elijamos.

En la siguiente sección, aprenderá a utilizar la dimensión intrínseca para la reducción de dimensión.

## Reducción de dimensiones con PCA

La reducción de dimensión representa los mismos datos utilizando menos funciones y es vital para construir pipeline de aprendizaje automático utilizando datos del mundo real. Finalmente aquí, aprenderá cómo realizar la reducción de dimensión utilizando PCA. Ya hemos visto que las características del PCA están en orden decreciente de varianza.

![](img/fig30.png){fig-align="center" width="200"}

PCA realiza un reducción de dimensión descartando las características de PCA con menor varianza, lo que asume que es ruido y conserva las características PCA de mayor varianza, que supone que son informativas.

> Para utilizar PCA para la reducción de dimensión, debe especificar cuántas características de PCA desea conservar.

Por ejemplo, especificar `PCA(n_components=2)` al crear un modelo PCA le indica que conserve solo las primeras dos características PCA. Una buena opción es la dimensión intrínseca del conjunto de datos, si la conoce. Consideremos inmediatamente un ejemplo:

El conjunto de datos del iris tiene 4 características que representan las 4 mediciones

```{python}
iris
```

Utilicemos un PCA para reducir la dimensión del conjunto de datos del iris a solo 2.

```{python}
pca = PCA(n_components=2)
pca.fit(iris)
transformed = pca.transform(iris)
print(transformed.shape)
```

Al imprimir la forma de las muestras transformadas, vemos que solo hay dos características, como se esperaba.

Aquí se muestra un diagrama de dispersión de las características de PCA, donde los colores representan las tres especies de iris.

```{python}
xs = transformed[:,0]
ys = transformed[:,1]
plt.scatter(xs, ys,c = species_)
plt.show()
```

Notablemente, a pesar de haberse reducido la dimensión de 4 a 2, aún es posible distinguir las especies. Recuerde que PCA ni siquiera sabía que existían especies distintas. PCA simplemente tomó las 2 características de PCA con mayor variación.

Como podemos ver estas dos características son muy informativas.

> El PCA descarta las características de baja varianza y supone que las características de mayor varianza son informativas.

Como sucede con todos los supuestos, existen casos donde esto no se cumple. Sin embargo, como vimos con el conjunto de datos del iris, en la práctica suele suceder esto. En algunos casos, es necesario utilizar una implementación alternativa de PCA.

### Matrices de frecuencia de palabras

Las matrices de frecuencia de palabras son un gran ejemplo de una implementación alternativa de PCA.

![](img/fig31.png){fig-align="center" width="400"}

En una matriz de frecuencia de palabras, cada fila corresponde a un documento y cada columna corresponde a una palabra de un vocabulario fijo. Las entradas de la matriz miden la frecuencia con la que aparece cada palabra en cada documento. Sólo algunas de las palabras del vocabulario aparecen en cualquier documento, por lo que la mayoría de las entradas de la matriz de frecuencia de palabras son cero.

Se dice que las matrices de palabras como esta son *"dispersas"* y, a menudo, representado utilizando un tipo especial de matriz llamada `"csr_matrix".`

-   `csr_matrix` ahorra espacio al recordar solo las entradas distintas de cero de la matriz.

### TruncatedSVD & csr_matrix

El PCA de Scikit-Learn no admite csr_matrix y necesitará utilizar TruncatedSVD en su lugar.

TruncatedSVD realiza la misma transformación que PCA, pero acepta matrices csr_matrix como entrada.

``` Python
from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD(n_components = 3)
model.fit(documents)
transformed = model.transform(documents)
```
