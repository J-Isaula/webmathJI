[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Juan Isaula",
    "section": "",
    "text": "Hola!\nSoy un profesional apasionado por transformar datos en soluciones estratégicas que impulsen el crecimiento y la innovación en el sector financiero. Actualmente, como Subgerente de Ciencia de Datos en Grupo Financiero Ficohsa, lidero proyectos de analitica avanzada y modelado predictivo, la gestión del riesgo y la toma de decisiones basadas en evidencia.\nMi sólida formación en Matemáticas y Estadística me permite abordar los desafíos complejos del entorno financiero con rigor técnico y visión analítica. Trabajo continuamente en el desarrollo de metodologías y herramientas innovadoras que integren el análisis de datos al corazón de la estrategia corporativa.\nEstoy comprometido con la excelencia, la innovación y la mejora continua, buscando siempre generar un impacto positivo en las organizaciones con la que colaboro."
  },
  {
    "objectID": "cursos_impartidos/index.html",
    "href": "cursos_impartidos/index.html",
    "title": "Cursos Impartidos en UNAH",
    "section": "",
    "text": "Saludos! Bienvenido a esta sección de mi website. Aquí podrá encontrar información sobre algnos cursos que he impartido.\n\n2023\n\n\nEstadística Aplicada - Maestría UNAH\n\nEste apartado está dedicado a los estudiantes de la maestría en Formulación, Gestión y Evaluación de Proyectos de la UNAH. Donde podrán encontrar el contenido sobre el taller de fundamentos estadísticos utilizando el Software de R. Mismo que tuve la oportunidad de impartirles.\n\nFundamentos estadísticos utilizando el Software de R\nLibro sobre fundamentos estadísticos con R - Juan Isaula\n\nBases de datos utilizadas\n\nco2.csv\nAAPL.csv\n\n\n\n\nMicroeconomía II - UNAH\n\n\nPrograma del curso\n\n\nModulo I - Teoria del Consumidor\n\n\nkaggle - Comandos Generales Python\nMaterial de repaso - Matemáticas de la optimización\nAula virtual Classroom\n\n\nModulo II - Teoria del Productor\n\n\nElasticidad Sustitución, Función de costos y matriz sustitución\n\n\nVideos\n\n\nComandos Generales Python\n\n\nModulo III - Estructuras de Mercado\n\n\nEjercicios equilibro a corto plazo - Competencia Perfecta\nCompetencia Perfecta: Largo y Corto Plazo\nPresentaciones sobre Monopolios\n\n\nAsignaciones\n\n\nTarea I\nTarea 3 - Teoría del Productor\nTarea Final - Estructuras de Mercado utilizando Python\n\n\nBibliografía Recomendada\n\n\nAdvanced Microeconomic Theory, Geoffrey A. Jehle Philip J.Reny\nTeoría Microeconómica (Principios básicos y ampliaciones-Walter Nikolso)\nMicroeconomia II - UNAH - Juan Isaula"
  },
  {
    "objectID": "Sobre_mi/index.html",
    "href": "Sobre_mi/index.html",
    "title": "Mi Bitacora Profesional",
    "section": "",
    "text": "En esta sección comparto algunos momentos clave de mi trayectoria académica , profesional y personal. Aquí encontrarás experiencias que me han enriquecido y personas con las que he tenido el privilegio de colaborar y compartir.\nInnovation Day ASICA 2025 - Honduras - Hotel Clarion\nParticipación en evento organizado por Grupo ASICA donde conocimos herramientas de análisis y soluciones avanzadas de SAS. Fue una excelente oportunidad para conectar con profesionales del sector y compartir ideas innovadoras.\n\nCompartiendo con amigos en la boda de uno de ellos\n\nCompartiendo con compañeros de la Maestría en Economía Matemática, México.\n\nCelebrando el cumpleaños de mi amigo Elmer, felicidades crack.\n\nLinda Experiencia, impartiendo el curso sobre fundamentos estadísticos utilizando el software de R a estudiantes de la maestría en Gestión de Proyectos de la UNAH.\n\n\n\n\n\nParticipación en conversatorio estudiantes egresados de la Lic. Matemáticas UNAH 2023\n\nNoche de Cine - 8/07/2023\n\nEquipo de Trabajo - IHSS\n\nFestejando Graduación (Lic. Matemáticas) de Amigos"
  },
  {
    "objectID": "Sobre_mi/sobre_mi.html",
    "href": "Sobre_mi/sobre_mi.html",
    "title": "Mi Bitacora Profesional",
    "section": "",
    "text": "En esta sección comparto algunos momentos clave de mi trayectoria académica y profesional. Aquí encontrarás experiencias que me han enriquecido y personas con las que he tenido el privilegio de colaborar\nInnovation Day ASICA 2025 - Honduras - Hotel Clarion\nParticipamos en un evento organizado por Grupo ASICA donde conocimos herramientas de análisis y soluciones avanzadas de SAS. Fue una excelente oportunidad para conectar con profesionales del sector y compartir ideas innovadoras.\n\nCompartiendo con compañeros de la Maestría en Economía Matemática, México.\n\nCelebrando el cumpleaños de mi amigo Elmer, felicidades crack.\n\nLinda Experiencia, impartiendo el curso sobre fundamentos estadísticos utilizando el software de R a estudiantes de la maestría en Gestión de Proyectos de la UNAH.\n\n\n\n\n\nParticipación en conversatorio estudiantes egresados de la Lic. Matemáticas UNAH 2023\n\nNoche de Cine - 8/07/2023\n\nEquipo de Trabajo - IHSS\n\nFestejando Graduación (Lic. Matemáticas) de Amigos"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Juan Isaula",
    "section": "",
    "text": "Artículos\n\n\n\n\nArtículos\n\nBienvenidos! En esta sección encontrará una selección de artículos que he escrito a lo largo de mi trayectoria profesional.\n\n\n\n\nImpacto Aranceles de EE.UU sobre el desempeño de las Tarjetas de Crédito y Préstamos en Honduras\nModelando Riesgo Crediticio con Python\nFundamentos Estadísticos con R\nMatemáticas de la Optimización - Microeconomía\nTest de Jarque-Bera con Python\nRedes Neuronales LSTM vs GRU\nPronósticos de Inflación Honduras utilizando Redes Neuronales\nCredit Scoring"
  },
  {
    "objectID": "Dasboard/index.html",
    "href": "Dasboard/index.html",
    "title": "App/Dashboards",
    "section": "",
    "text": "En esta sección encontrará algunas herramientas (App) que he desarrollado en lenguajes de programación R y Python, elaborados para distintas instituciones del país (Honduras) .\n\nGeoRGe\nAplicación creada R, utilizando principalmente los paquetes shiny | shinydashboard | DT | tidyverse | shinyWidgets | fresh. En la actualizad es utilizada en el Instituto Hondureño de Seguridad Social (IHSS). [Ver aplicación aquí]\nIA-JIM\nAplicación diseñada para facilitar el cálculo de gran parte de los indicadores actuariales primarios y secundarios del IHSS, en el marco del Régimen del Seguro de Previsión Social (RSPS). [Ver aplicación aquí]"
  },
  {
    "objectID": "posts/Neural Network/index.html",
    "href": "posts/Neural Network/index.html",
    "title": "Arquitectura de Redes Neuronales",
    "section": "",
    "text": "Las arquitecturas de redes neuronales se refieren a los diseños estructurales y organizativos de redes neuronales artificiales (RNA). Estas arquitecturas determinan cómo se organiza la red, incluida la cantidad de capas, la cantidad de neuronas en cada capa, las conexiones entre neuoronas y las funciones de activación utilizadas. Se forman diferentes arquitecturas de redes neuronales alterando estos componentes estructurales para adaptarse a tareas o desafíos específicos. Si desea conocer los tipos de arquitectura de redes neuronales que debe conocer, este artículo es para usted. En este artículo, le explicaré los tipos de arquitecturas de redes neuronales en Machine Learning y cuándo elegirlas."
  },
  {
    "objectID": "posts/Neural Network/index.html#fundamentos-previos-a-la-comprensión-de-redes-neuronales",
    "href": "posts/Neural Network/index.html#fundamentos-previos-a-la-comprensión-de-redes-neuronales",
    "title": "Arquitectura de Redes Neuronales",
    "section": "Fundamentos previos a la comprensión de Redes Neuronales",
    "text": "Fundamentos previos a la comprensión de Redes Neuronales\n\nFunción de Activación\nUna función de activación es una función que se agrega a una red neuronal para ayudar a la red a aprender dependencias no lineales complejas. Una función de activación típica debe ser diferenciable y continua en todas partes. A continuación proporcionaré algunos ejemplos de funciones de activación utilizando la biblioteca PyTorch.\n\nFunción ReLU\nReLU o la función ReLU realiza una operación simple: \\(y = \\max (0, x)\\). Aquí te proporcionó un ejemplo de uso de la función ReLU utilizando PyTorch.\n\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nx = torch.linspace(-10, 10,steps=100)\n\nrelu = torch.nn.ReLU()\n\ny = relu(x)\nplt.title(\"ReLU\")\nplt.plot(x.tolist(), y.tolist())\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFunción Sigmoidea\nEs una de las funciones de activación no lineal más comunes. La función sigmoidea se representa matemáticamente como:\n\\[\n\\sigma(x) = \\frac{1}{1 + e^x}\n\\]\nAl igual que ReLU, la función \\(\\sigma\\) se puede construir simplemente usando PyTorch.\n\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nx = torch.linspace(-10, 10,steps=100)\n\nsigmoid = torch.nn.Sigmoid()\n\ny = sigmoid(x)\nplt.title(\"Sigmoidea\")\nplt.plot(x.tolist(), y.tolist())\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFunción Tanh\nLa función tangente hiperbólica es similar a la función sigmoidea, pero devuelve valores en el rango \\((-1,1)\\). El beneficio de Tanh sobre \\(\\sigma\\) es que las entradas negativas se asignarán estrictamente a negativa, y las entradas positivas se asignarán estrictamente a positivas:\n\\[\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\n\nimport torch\nimport matplotlib.pyplot as plt\n\nx=torch.linspace(-10,10, steps = 100)\ntanh = torch.nn.Tanh()\ny = tanh(x)\n\nplt.title('Tanh')\nplt.plot(x.tolist(),y.tolist())\nplt.show()\n\n\n\n\n\n\n\n\nLas funciones de activación no lineales, como la \\(\\sigma\\) y \\(\\tanh\\) sufren de un gran problema computacional llamado problema de fuga de gradiente.\nLa fuga de gradiente hace que sea muy difícil entrenar y ajustar los parámetros de las capas iniciales en la red. Este problema empeora a medida que aumenta el número de capas en la red.\nLa fuga de gradiente es la causa principal que hace que las activaciones sigmoideas o Tanh no sean adecuadas para los modelos de Deep Learning (aprendizaje profundo). La función de activación ReLU no sufre de gradiente de fuga porque la derivada siempre es 1 para entradas positivas. Así que siempre considere usar ReLU como la función de activación en los primeros borradores del diseño de su modelo.\n\nLa creación de una arquitectura de red neuronal que se adapte más a un problema en particular es un arte. Existe una dirección de estudio separada en el aprendizaje profundo llamado Búsqueda de arquitectura neural, que automatiza la ingeniería de arquitectura de red: https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html. Pero incluso estos motores de búsqueda no pueden competir con las habilidades heurísticas humanas en el diseño todavía. Existen algunas técnicas que aumentan la probabilidad de mejorar el rendimiento de la red neuronal. Por supuesto, estas técnicas no garantizan la mejora en todos los casos. A veces incluso pueden empeorar el rendimiento de la red neuronal. Pero es probable que desarrolle una arquitectura de modelo robusta siguiendo estos enfoques.\n\n\n\nFunciones de Pérdida y Optimización\n\nFunciones de Pérdida\nLa función de pérdida calculará un error de red en cada iteración, mientras que la función de optimización determina “cómo y en qué dirección cambiar los parámetros de peso”.\nHay una cantidad diversa de funciones de pérdida, cada una de ellas está destinada a una tarea en particular. Para el análisis de series de tiempo, hay tres funciones de pérdida principales:\n\nPérdida absoluta (L1): La pérdida absoluta es la métrica más simple de la distancia entre dos vectores:\n\\[\nabsolute loss = \\frac{\\sum |y_{actual} - y_{predicción}|}{n}\n\\]\nEn PyTorch, la función de pérdida absoluta se implementa de la siguiente manera:\n\na = torch.tensor([1,2]).float()\nb = torch.tensor([1, 5]).float()\nabs_loss = torch.nn.L1Loss()\nabs_error = abs_loss(a,b)\nprint(f'abs: {abs_error.item()}')\n\nabs: 1.5\n\n\nError cuadrático medio (MSE) (L2): Es la función de pérdida más utilizada para los problemas de predicción de series de tiempo:\n\\[\nmean\\_squared\\_error =  \\frac{\\sum(y_{actual} - y_{predicted})^2}{n}\n\\]\nPérdida suave (L1): es algo intermedio entre las funciones de pérdida absoluta y MSE. La pérdida absoluto (L1) es menos sensible a los valores atípicos que MSE:\n\\[\nsmooth\\_loss(y^{\\prime},y) = \\frac{1}{n}\\sum z_i\n\\]\ndonde \\(y\\) es valor real, \\(y\\) se predice, \\(z_i\\) se define como:\n\\[ z =\n\\begin{equation}\n\\begin{matrix}\n  \\frac{0.5(y_{i}^{\\prime} - y_i)^2}{\\beta}, & |y_{i}^{\\prime} - y_i| &lt; \\beta\\\\  \n|y_{i}^{\\prime} - y_i| - 0.5\\beta, & otro\\_caso\n  \\end{matrix}\n\\end{equation}\n\\]\n\nLa función de pérdida de L1 suave tiene un parámetro \\(\\beta\\), es igual a 1 por defecto.\n\n\nOptimizador\nEl objetivo principal de un optimizador es cambiar los parámetros de pesos del modelo para minimizar la función de pérdida. La selección de un optimizador adecuado depende completamente de la arquitectura de la red neuronal y los datos sobre los que ocurre el entrenamiento.\n\nAdagrad: es un algoritmo de optimización basado en gradiente que adapta la tasa de aprendizaje a los parámetros. Realiza actualizaciones más pequeñas para los parámetros asociados con características frecuentes y actualizaciones más grandes para parámetros asociados con características raras.\nAdadelta es la versión avanzada del algoritmo de Adagrad. Adadelta busca minimizar su tasa de aprendizaje agresiva y monotónica que disminuye. En lugar de acumular todos los gradientes pasados.\nAdam es otro método de optimización que calcula las tasas de aprendizaje adaptativo para cada parámetro. Además de guardar un promedio exponencialmente en descomposición de gradientes cuadrados anteriores como Adadelta, Adam también mantiene un promedio exponencialmente de disminución de gradientes anteriores."
  },
  {
    "objectID": "posts/Neural Network/index.html#tipos-de-redes-neuronales",
    "href": "posts/Neural Network/index.html#tipos-de-redes-neuronales",
    "title": "Arquitectura de Redes Neuronales",
    "section": "Tipos de Redes Neuronales",
    "text": "Tipos de Redes Neuronales\nComenzaremos explorando algunas de las arquitecturas de redes neuronales más eficientes para el pronóstico de series de tiempo. Nos centraremos en la implementación de redes neuronales recurrentes (RNN), unidad recurrentes cerradas (GRU), redes de memoria a largo plazo (LSTM). Comprender los principios básicos de las RNN será una buena base para su aplicación directa y dominar otras arquitecturas similares. Trataremos de cubrir la lógica y el núcleo de cada arquitectura, su aplicación práctica y pros y contras.\nDiscutiremos los siguientes temas:\n\nRecurrent neural network (RNN)\nGated recurrent unit network (GRU)\nLong short-term memory network (LSTM)\n\n\nRecurrent Neural Network (RNN)\nRNN (Red Neuronal Recurrente Estándar) tiene un concepto de un estado oculto. Un estado oculto puede tratarse como memoria interna. El estado oculto no intenta recordar todos los valores pasados de la secuencia sino solo su efecto. Debido a la memoria interna, las RNN pueden recordar cosas importantes sobre su entrada, lo que les permite ser muy preciosos para predecir valores futuros.\nEstudiemos la teoría de RNN de una manera más formal. En RNN, la secuencia de entrada se representa a traves de un bucle. Cuando toma una decisión, considera la entrada actual y también lo que ha aprendido de las entradas que recibio anteriormente. Veamos el gráfico computacional de RNN para comprender esta lógica:\n\n\n\nGráfico Computacional de RNN\n\n\ndonde,\n\n\\(x_1, x_2, . . . , x_n\\) son la secuencia de entrada.\n\\(h_i\\) es el estado oculto. \\(h_i\\) es un vector de longitud \\(h\\).\nRNN Cell representa la capa de red neuronal que calcula la siguiente función: \\(h_t = \\tanh(W_{ih}x_t + b_{ih} + W_{hh}h_{(t-1)} + b_{hh})\\)\n\nPodemos ver a detalle la RNN Cell:\n\n\n\nGráfico computacional de RNN Cell\n\n\nLa RNN Cell combina información sobre el valor actual de la secuencia \\(x_i\\) y el estado previamente oculto \\(h_{i-1}\\). La RNN Cell, devuelve un estado oculto actualizado \\(h_i\\) después de aplicar la función de activación.\nLa RNN tiene los siguientes parámetros, que se ajustan durante el entrenamiento:\n\n\\(W_{ih}\\) pesos ocultos de entrada\n\\(b_{ih}\\) sesgos oculto de entrada\n\\(W_{hh}\\) pesos ocultos - ocultos\n\\(B_{hh}\\) sesgos oculto - oculto\n\nNota: Un error común ocurre cuando los subíndices en los parámetros RNN \\((W_{ih}, b_{ih}, W_{hh}, b_{hh})\\) se interpretan como una dimensión de índice o tensor. No, son solo la abreviatura de entrada-oculto \\((h_í)\\) y oculto-oculto \\((h)\\). El mismo principio aplica a los parámetros de otros modelos: GRU y LSTM.\nEn ocasiones, los cientificos de datos utilizan la siguiente representación de las RNN:\n\n\n\nVisualización alternativa de RNN\n\n\nEl gráfico que se muestra puede dar lugar a algunos malentendidos, y estoy tratando de evitar esto. Pero si este tipo de gráfico se adapta a tu intuición, entonces úsalo sin ninguna duda.\n\nAhora estamos listos para examinar una implementación de RNN utilizando PyTorch\n\nimport torch.nn as nn\n\nclass RNN(nn.Module):\n\n    def __init__(self,\n                 hidden_size,\n                 in_size = 1,\n                 out_size = 1):\n        super(RNN, self).__init__()\n        self.rnn = nn.RNN(\n            input_size = in_size,\n            hidden_size = hidden_size,\n            batch_first = True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, h = None):\n        out, _ = self.rnn(x, h)\n        last_hidden_states = out[:, -1]\n        out = self.fc(last_hidden_states)\n        return out, last_hidden_states\n\nNote que nuestro modelo devuelve dos salidas: predicción y estado oculto. Es crucial reutilizar los estados ocultos durante la evaluación RNN. Utilizaremos conjuntos de datos de consumo de energía por hora ( https://www.kaggle.com/robikscube/Hourly-energy-Consumed) para la implementación de RNN.\n\nimport pandas as pd\nimport torch\n\ndf = pd.read_csv('AEP_hourly.csv')\nts = df['AEP_MW'].astype(int).values.reshape(-1, 1)[-3000:]\n\nimport matplotlib.pyplot as plt\n\nplt.title('AEP Hourly')\nplt.plot(ts[:500])\nplt.show()\n\n\n\n\n\n\n\n\nPodemos ver en que esta es una serie de tiempo realmente complicada. Tiene varios factores de estacionalidad con picos apenas predecibles.\nA continuación, voy a mostrarte como se desempeña RNN en la serie de tiempo AEP Hourly:\n\nimport copy\nimport random\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.preprocessing import MinMaxScaler\n\nrandom.seed(1)\ntorch.manual_seed(1)\n\n# Parametros globales\n\n\nfeatures = 240\n# Longitud del conjunto de datos de prueba\ntest_ts_len = 300\n# tamaño del estado oculto\nrnn_hidden_size = 24\n# tasa de aprendizaje de optimizador\nlearning_rate = 0.02\n\ntraining_epochs = 500\n\ndef sliding_window(ts, features):\n    X = []\n    Y = []\n\n    for i in range(features + 1, len(ts) + 1):\n        X.append(ts[i - (features + 1):i - 1])\n        Y.append([ts[i - 1]])\n\n    return X, Y\n\ndef get_training_datasets(ts, features, test_len):\n    X, Y = sliding_window(ts, features)\n\n    X_train, Y_train, X_test, Y_test = X[0:-test_len],\\\n                                       Y[0:-test_len],\\\n                                       X[-test_len:],\\\n                                       Y[-test_len:]\n\n    train_len = round(len(ts) * 0.7)\n\n    X_train, X_val, Y_train, Y_val = X_train[0:train_len],\\\n                                     X_train[train_len:],\\\n                                     Y_train[0:train_len],\\\n                                     Y_train[train_len:]\n\n    x_train = torch.tensor(data = X_train).float()\n    y_train = torch.tensor(data = Y_train).float()\n\n    x_val = torch.tensor(data = X_val).float()\n    y_val = torch.tensor(data = Y_val).float()\n\n    x_test = torch.tensor(data = X_test).float()\n    y_test = torch.tensor(data = Y_test).float()\n\n    return x_train, x_val, x_test,\\\n           y_train.squeeze(1), y_val.squeeze(1), y_test.squeeze(1)\n           \n\n# Preparando datos para entrenamiento\nscaler = MinMaxScaler()\nscaled_ts = scaler.fit_transform(ts)\nx_train, x_val, x_test, y_train, y_val, y_test =\\\n    get_training_datasets(scaled_ts, features, test_ts_len)\n    \n\n# Inicialización del modelo \nmodel = RNN(hidden_size = rnn_hidden_size)\nmodel.train()\n\n/var/folders/sg/qphrr1zj1qn5sjtfpqllwx6m0000gn/T/ipykernel_66519/1156180628.py:50: UserWarning:\n\nCreating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n\n\n\nRNN(\n  (rnn): RNN(1, 24, batch_first=True)\n  (fc): Linear(in_features=24, out_features=1, bias=True)\n)\n\n\n\n# Entrenamiento\noptimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\nmse_loss = torch.nn.MSELoss()\n\nbest_model = None\nmin_val_loss = sys.maxsize\n\ntraining_loss = []\nvalidation_loss = []\n\nfor t in range(training_epochs):\n\n    prediction, _ = model(x_train)\n    loss = mse_loss(prediction, y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    val_prediction, _ = model(x_val)\n    val_loss = mse_loss(val_prediction, y_val)\n\n    training_loss.append(loss.item())\n    validation_loss.append(val_loss.item())\n\n    if val_loss.item() &lt; min_val_loss:\n        best_model = copy.deepcopy(model)\n        min_val_loss = val_loss.item()\n\n    if t % 50 == 0:\n        print(f'epoch {t}: train - {round(loss.item(), 4)}, '\n              f'val: - {round(val_loss.item(), 4)}')\n\nepoch 0: train - 0.377, val: - 0.0866\nepoch 50: train - 0.0061, val: - 0.0133\nepoch 100: train - 0.0021, val: - 0.0044\nepoch 150: train - 0.0018, val: - 0.0034\nepoch 200: train - 0.0015, val: - 0.003\nepoch 250: train - 0.0014, val: - 0.0027\nepoch 300: train - 0.0013, val: - 0.0026\nepoch 350: train - 0.0012, val: - 0.0025\nepoch 400: train - 0.0012, val: - 0.0024\nepoch 450: train - 0.0012, val: - 0.0024\n\n\nY aquí llegamos al punto más difícil. Debe pasar el estado oculto al modelo RNN cuando lo evalua. La forma más sencilla de calentar el estado oculto es ejecutar el modelo en los datos de validación una vez y pasar un estado oculto cálido a través de cada iteración y por último evaluamos el modelo que construimos en el conjunto de datos de prueba.\n\nbest_model.eval()\n_, h_list = best_model(x_val)\n\nh = (h_list[-1, :]).unsqueeze(-2)\n\n\npredicted = []\nfor test_seq in x_test.tolist():\n    x = torch.Tensor(data = [test_seq])\n \n    y, h = best_model(x, h.unsqueeze(-2))\n    unscaled = scaler.inverse_transform(np.array(y.item()).reshape(-1, 1))[0][0]\n    predicted.append(unscaled)\n\nreal = scaler.inverse_transform(y_test.tolist())\nplt.title(\"Conjunto de datos prueba - RNN\")\nplt.plot(real, label = 'real')\nplt.plot(predicted, label = 'predicción')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nRNN muestra un gran rendimiento en el conjunto de datos de prueba. El modelo que hemos entrenado predice picos estacionales con mucha precisión.\nY finalmente, examinamos el proceso de entrenamiento en sí.\n\nplt.title('Desempeño RNN')\nplt.yscale('log')\nplt.plot(training_loss, label = 'Entrenamiento')\nplt.plot(validation_loss, label = 'validación')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nEl proceso del entrenamiento es suave sin picos agudos e impredecibles.\nAhora, podemos establecer con confianza la promesa y la efectividad de la aplicación de RNN a los problemas de pronósticos de la serie temporal.\nA pesar de todas las ventajas de RNN, tiene desventajas significativas:\n\nDebido a la complejidad computacional, sufren problemas de gradiente de fuga. El proceso de entrenamiento se vuelve demasiado lento. El problema del gradiente de fuga es un problema común a todas las RNN.\nEl estado oculto se actualiza en cada iteración, lo que dificulta el almacenamiento de información a largo plazo en RNN. Las arquitecturas GRU y LSTM resuelven este problema. Tienen enfoques similares sobre cómo almacenar información a largo plazo.\n\n\n\nGated recurrent unit network (GRU)\nLa GRU es es una versión avanzada de la RNN clásica. El propósito principal de GRU es almacenar información a largo plazo. En breve exploraremos como GRU logra esto.\nLa forma más fácil de almacenar información a largo plazo en un estado oculto es restringir las actualizaciones ocultas sobre cada iteración. Este enfoque evitará sobrescribir información importante a largo plazo.\nPuede encontrar la siguiente definición de GRU en internet:\nSe comienza calculando la puerta de actualización \\(z_t\\) para el peso de tiempo \\(t\\) usando la fórmula:\n\\[\n\\begin{eqnarray*}\nz_{t} &=& \\sigma(W^{z}x_t + U^{z}h_{t-1}) \\hspace{1cm} \\mbox{Puerta de actualización}\\\\[0.2cm]\n\\end{eqnarray*}\n\\]\nlo que sucede aquí es que cuando \\(x_t\\) se conecta a la unidad de red, se multiplica por su propio peso \\(W^{z}\\). Lo mismo ocurre con \\(h_{t-1}\\), que contiene la información de las unidades \\(t-1\\) anteriores y se múltiplica por su propio peso \\(U^{z}\\). Ambos resultados se suman y se aplica una función de activación sigmoidea \\((\\sigma)\\) para acotar el resultado entre 0 y 1.\n\nLa puerta de actualización ayuda al modelo a determinar cuánta información pasada (de pasos de tiempo anteriores) debe transmitirse al futuro. Esto es muy poderoso porque el modelo puede decidir copiar toda la la información del pasado y eliminar el riesgo de que desaparezca el problema de fuga del gradiente.\nLuego continuamos con Restablecer puerta:\nBásicamente, esta puerta se utiliza desde el modelo para decidir cuánta información pasada se debe olvidar. Para calcularlo utilizamos:\n\\[\nr_t = \\sigma(W^{r}x_t + U^{r}h_{t-1})\\hspace{1cm} \\mbox{Restablecer puerta}\n\\]\nEsta fórmula es la misma que la de la puerta de actualización. La diferencia viene en los pesos y el uso de la puerta, que veremos en un momento.\n\nComo antes, conectamos \\(h_{t-1} - \\mbox{linea azul}\\) y \\(x_{t} - \\mbox{linea violeta}\\), los multiplicamos con sus pesos correspondientes, sumamos los resultados y aplicamos la función sigmoidea.\nContenido de la memoria actual:\nveamos como afectarán exactamente las puertaas al resultado final. Primero, comenzamos con el uso de la puerta de reinicio. Introducimos un nuevo contenido de memoria que utilizará la puerta de reinicio para almacenar la información del pasado. Se calcula de la siguiente manera:\n\\[\nh_{t}^{\\prime} = tanh(Wx_{t} + r_{t}\\odot U h_{t-1})\n\\]\n\nMultiplique la entrada \\(x_t\\) con un peso \\(W\\) y \\(h_{t-1}\\) con un peso \\(U\\).\nCalcule el producto de Hadamard (por elementos) entre la puerta de reinicio \\(r_t\\) y \\(Uh_{t-1}\\). Eso determinará qué eliminar de los pasos de tiempo anterior. Digamos que tenemos un problema de análisis de sentimientos para determinar la opinión de una persona sobre un libro a partir de una reseña que escribió. El texto comienza con “Este es un libro de fantasía que ilustra…” y después de un par de párrafos termina con “No disfruté mucho el libro porque creo que captura demasiados detalles”. Para determinar el nivel general de satisfacción con el libro sólo necesitamos la última parte de la reseña. En ese caso, a medida que la red neuronal se acerque al final del texto, aprenderá a asignar un vector \\(r_t\\) cercano a 0, eliminando el pasado y centrándose solo en las últimas oraciones.\nResuma los resultados de los pasos 1 y 2.\nAplicar la función de activación no lineal tanh.\n\nPuedes ver claramente los pasos aquí:\n\nHacemos una multiplicación por elementos de \\(h_{t-1} - \\mbox{línea azul}\\) y \\(r_t - \\mbox{línea naranja}\\) y luego sumamos el resultado - linea rosa con la entrada \\(x_t -\\) línea morada. Finalmente, tanh se usa para producir \\(h_{t}^{\\prime}:\\) línea verde brillante.\nMemoria final en el paso de tiempo actual\nComo último paso, la red necesita calcular \\(h_{t}\\), el vector que contiene información para la unidad actual y la transmite a la red. Para hacer eso, se necesita la puerta de actualización. Determina qué recopilar el contenido de la memoria actual \\((h_t^{\\prime})\\) y qué de los pasos anteriores \\((h_{(t-1)})\\). Eso se hace de la siguiente manera:\n\\[\nh_t = z_t\\odot h_{t-1} + (1 - z_t)\\odot h_{t}^{\\prime}\n\\]\n\nAplique la multiplicación por elementos a la puerta de actualización \\(z_t\\) y \\(h_{(t-1)}\\).\nAplique la multiplicación por elementos a \\((1- z_t)\\) y \\(h_{t}^{\\prime}\\).\nSume los resultados de los pasos 1 y 2.\n\nPongamos el ejemplo de la reseña del equilibrio. En esta ocasión, la información más relevante se situa al inicio del texto. El modelo puede aprender a establecer el vector \\(z_t\\) cerca de 1 y conservar la mayor parte de la información anterior. Dado que \\(z_t\\) estará cerca de 1 en este paso de tiempo, \\((1-z_t)\\) estará cerca de 0, lo que ignorará gran parte del contenido actual (en este caso, la última parte de la reseña que explica la trama del libro), lo cual es irrelevante para nuestra predicción.\nAquí hay una ilustración que enfatiza la ecuación anterior:\n\nA continuación, puede ver cómo \\(z_t\\) (línea verde) para calcular \\(1 - z_t\\) que combinado con \\(h_{t}^{\\prime}\\) (línea verde brillante), produce un resultado en la línea roja oscura. \\(z_t\\) también se usa con \\(h_{t-1} - \\mbox{línea azul}\\) en una multiplicación de elementos. Finalmente, \\(h_{t}:\\) la línea azul es el resultado de la suma de las salidas correspondientes a las líneas rojas brillantes y oscuras.\nAhora puede ver cómo las GRU pueden almacenar y filtrar la información utilizando sus puertas de actualización y reinicio. Eso elimina el problema del gradiente de fuga, ya que el modelo no elimina la nueva entrada cada vez, sino que mantiene la información relevante y la pasa a los siguientes pasos de la red. Si se les entrena cuidadosamente, pueden desempeñarse extremadamente bien incluso en escenarios complejos.\nEl modelo de predicción GRU es muy similar al RNN. Veamos su desempeño utilizando la misma data que el casa RNN.\n\nimport torch.nn as nn\n\nrandom.seed(1)\ntorch.manual_seed(1)\n\n\nfeatures = 240\ntest_ts_len = 300\ngru_hidden_size = 24\nlearning_rate = 0.02\ntraining_epochs = 500\n\nclass GRU(nn.Module):\n\n    def __init__(self,\n                 hidden_size,\n                 in_size = 1,\n                 out_size = 1):\n        super(GRU, self).__init__()\n        self.gru = nn.GRU(\n            input_size = in_size,\n            hidden_size = hidden_size,\n            batch_first = True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, h = None):\n        out, _ = self.gru(x, h)\n        last_hidden_states = out[:, -1]\n        out = self.fc(last_hidden_states)\n        return out, last_hidden_states\n\n# Inicializando el modelo GRU\nmodel = GRU(hidden_size = gru_hidden_size)\nmodel.train()\n\n# Entrenamiento\noptimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\nmse_loss = torch.nn.MSELoss()\n\nbest_model = None\nmin_val_loss = sys.maxsize\n\ntraining_loss = []\nvalidation_loss = []\n\n\nfor t in range(training_epochs):\n\n    prediction, _ = model(x_train)\n    loss = mse_loss(prediction, y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    val_prediction, _ = model(x_val)\n    val_loss = mse_loss(val_prediction, y_val)\n\n    training_loss.append(loss.item())\n    validation_loss.append(val_loss.item())\n\n    if val_loss.item() &lt; min_val_loss:\n        best_model = copy.deepcopy(model)\n        min_val_loss = val_loss.item()\n\n    if t % 50 == 0:\n        print(f'epoch {t}: train - {round(loss.item(), 4)}, '\n              f'val: - {round(val_loss.item(), 4)}')\n\nbest_model.eval()\n_, h_list = best_model(x_val)\nh = (h_list[-1, :]).unsqueeze(-2)\n\npredicted = []\nfor test_seq in x_test.tolist():\n    x = torch.Tensor(data = [test_seq])\n    y, h = best_model(x, h.unsqueeze(-2))\n    unscaled = scaler.inverse_transform(np.array(y.item()).reshape(-1, 1))[0][0]\n    predicted.append(unscaled)\n\nepoch 0: train - 0.0626, val: - 0.0326\nepoch 50: train - 0.0017, val: - 0.0026\nepoch 100: train - 0.0012, val: - 0.0024\nepoch 150: train - 0.0012, val: - 0.0023\nepoch 200: train - 0.0011, val: - 0.0022\nepoch 250: train - 0.0011, val: - 0.0022\nepoch 300: train - 0.0011, val: - 0.0023\nepoch 350: train - 0.0011, val: - 0.0023\nepoch 400: train - 0.0011, val: - 0.0022\nepoch 450: train - 0.0011, val: - 0.0022\n\n\n\nreal = scaler.inverse_transform(y_test.tolist())\nplt.title(\"Conjunto de datos prueba - GRU\")\nplt.plot(real, label = 'real')\nplt.plot(predicted, label = 'predicción')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nVemos que el modelo GRU imita el comportamiento original de la serie temporal con bastante precisión.\n\nplt.title('Desempeño GRU')\nplt.yscale('log')\nplt.plot(training_loss, label = 'Entrenamiengto')\nplt.plot(validation_loss, label = 'validación')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLas pérdidas de entrenamiento y validación tienen descenso asintótico con un brecha natural constante entre ellas. Podemos concluir que el modelo realmente aprende el comportamiento de la serie temporal.\n\n\nLong short-term memory network (LSTM)\nLa red LSTM se ha desarrollado para superar el problema de fuga de gradiente en RNN al mejorar el flujo de gradiente de la red. Debe mencionarse que la arquitectura apareció mucho antes que la GRU. La arquitectura LSTM se desarrolló en 1997, y el GRU se propueso en 2014. El diseño GRU es más simple y más comprensible que LSTM. Es por eso que comenzamos nuestro estudio examinando primero GRU.\nComo su nombre lo índica, LSTM aborda los mismos problemas de memoria a corto y largo plazo que GRU. A nivel global, el flujo computacional del LSTM se ve de la siguiente manera:\n\nLSTM funciona sobre los principios similares que GRU pero tiene más variables. RNN y GRU solo pasan un estado oculto \\(h_t\\) a través de cada iteración. Pero LSTM pasa dos vectores:\n\n\\(h_t\\) estado oculto (memoria a corto plazo)\n\\(c_t\\) estado de celda (memoria a largo plazo)\n\nLas salidas de LSTM Cell se calculan a través de las fórmulas:\n\\[\n\\begin{eqnarray*}\ni_t &=& \\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi})\\\\[0.2cm]\nf_t &=& \\sigma(W_{ii}x_{t} + b_{if} + W_{hf}h_{t-1} + b_{hf})\\\\[0.2cm]\ng_t &=& tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hn})\\\\[0.2cm]\no_t &=& \\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})\\\\[0.2cm]\nc_t &=& f_t \\circ c_{t-1} + i_t\\circ g_t\\\\[0.2cm]\nh_t &=& o_t \\circ tanh(c_t)\n\\end{eqnarray*}\n\\]\ndonde:\n\n\\(\\sigma\\) es la función sigmoidea\n\\(\\circ\\) es el producto de Hadamard\n\nEn cuanto a las variables:\n\n\\(i_t~(puerta de entrada)\\) es la variable que se utiliza para actualizar el estado \\(c_t\\). El estado previamente oculto \\(h_t\\) y la secuencia \\(x_t\\) se dan como entradas a una función sigmoidea \\((\\sigma)\\). Si la salida está cerca de 1, entonces la información es más importante.\n\\(f_t ~ (puerta~de~olvido)\\) es la variable que decide que información debe olvidarse en el estado \\(c_t\\). El estado \\(h_t\\) de estado previamente oculto y la secuencia \\(x_t\\) se dan como entradas a una función sigmoidea. Si la salida \\(f_t\\) está cerca de cero, la información se puede olvidar, mientras que si la salida está cerca de 1, la información debe almacenarse o recordarse.\n\\(g_t\\) representa información importante potencialmente nueva para el estado \\(c_t\\).\n\\(c_t ~ (estado~celda)\\) es una suma de:\n\nestado de celda anterior \\(c_{t-1}\\) con información olvidada \\(f_t\\).\nnueva información de \\(g_t\\) seleccionada por \\(i_t\\)\n\n\\(o_t ~ (puerta~de~salida)\\) es la variable para actualizar el estado oculto \\(h_t\\).\n\\(h_t ~(estado~oculto)\\) es el siguiente estado oculto que se calcula eligiendo la información importante del estado de celda o celular \\(c_t\\).\n\nA continuación te muestro el gráfico computacional de la celda LSTM:\n\nLSTM tiene los siguientes parámetros, que se ajustan durante el entrenamiento:\n\n\\(W_{ii}, W_{hi}, W_{if}, W_{hf}, W_{ig}, W_{hg}, W_{io}, W_{ho}\\) estos son los pesos.\n\\(b_{ii}, b_{hi}, b_{if}, b_{hf}, b_{ig}, b_{hg}, b_{io}, b_{ho}\\) estos son sesgos.\n\nAhora examinemos la implementación de Pytorch del modelo de predicción LSTM:\n\nimport torch.nn as nn\n\nclass LSTM(nn.Module):\n\n    def __init__(self,\n                 hidden_size,\n                 in_size = 1,\n                 out_size = 1):\n        super(LSTM, self).__init__()\n        self.lstm = nn.LSTM(\n            input_size = in_size,\n            hidden_size = hidden_size,\n            batch_first = True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, h = None):\n        out, h = self.lstm(x, h)\n        last_hidden_states = out[:, -1]\n        out = self.fc(last_hidden_states)\n        return out, h\n\nComo vemos, la implementación del modelo LSTM es bastante similar a las implementaciones de RNN y GRU.\nProbaremos el modelo LSTM con el siguiente conjunto de datos de la serie tiempo de consumo de energía por hora).\n\nimport pandas as pd\nimport torch\n\ndf = pd.read_csv('NI_hourly.csv')\nts = df['NI_MW'].astype(int).values.reshape(-1, 1)[-3000:]\n\nimport matplotlib.pyplot as plt\n\nplt.title('NI Hourly')\nplt.plot(ts[:500])\nplt.show()\n\n\n\n\n\n\n\n\nVeamos el modelo en acción:\n\nimport copy\nimport random\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.preprocessing import MinMaxScaler\n\nrandom.seed(1)\ntorch.manual_seed(1)\n\n\nfeatures = 240\ntest_ts_len = 300\nlstm_hidden_size = 24\nlearning_rate = 0.02\ntraining_epochs = 100\n\n# Preparar el conjunto de datos para el entrenamiento \nscaler = MinMaxScaler()\nscaled_ts = scaler.fit_transform(ts)\nx_train, x_val, x_test, y_train, y_val, y_test =\\\n    get_training_datasets(scaled_ts, features, test_ts_len)\n\n# Inicializando el modelo \nmodel = LSTM(hidden_size = lstm_hidden_size)\nmodel.train()\n\n# Entrenamiento \noptimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\nmse_loss = torch.nn.MSELoss()\n\nbest_model = None\nmin_val_loss = sys.maxsize\n\ntraining_loss = []\nvalidation_loss = []\n\n\nfor t in range(training_epochs):\n\n    prediction, _ = model(x_train)\n    loss = mse_loss(prediction, y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    val_prediction, _ = model(x_val)\n    val_loss = mse_loss(val_prediction, y_val)\n\n    training_loss.append(loss.item())\n    validation_loss.append(val_loss.item())\n\n    if val_loss.item() &lt; min_val_loss:\n        best_model = copy.deepcopy(model)\n        min_val_loss = val_loss.item()\n\n    if t % 10 == 0:\n        print(f'epoch {t}: train - {round(loss.item(), 4)}, '\n              f'val: - {round(val_loss.item(), 4)}')\n\nepoch 0: train - 0.0979, val: - 0.087\nepoch 10: train - 0.0253, val: - 0.0255\nepoch 20: train - 0.0131, val: - 0.0119\nepoch 30: train - 0.0056, val: - 0.0059\nepoch 40: train - 0.0032, val: - 0.0043\nepoch 50: train - 0.0026, val: - 0.0029\nepoch 60: train - 0.002, val: - 0.0025\nepoch 70: train - 0.0018, val: - 0.0023\nepoch 80: train - 0.0016, val: - 0.0021\nepoch 90: train - 0.0014, val: - 0.0019\n\n\nPara una evaluación del modelo LSTM, necesitamos pasar un estado celular y estado oculto.\n\nbest_model.eval()\nwith torch.no_grad():\n    _, h_list = best_model(x_val)\n\n    h = tuple([(h[-1, -1, :]).unsqueeze(-2).unsqueeze(-2)\n               for h in h_list])\n\n    predicted = []\n    for test_seq in x_test.tolist():\n        x = torch.Tensor(data = [test_seq])\n\n        y, h = best_model(x, h)\n        unscaled = scaler.inverse_transform(\n            np.array(y.item()).reshape(-1, 1))[0][0]\n        predicted.append(unscaled)\n        \nreal = scaler.inverse_transform(y_test.tolist())\nplt.title(\"Conjunto de prueba - LSTM\")\nplt.plot(real, label = 'real')\nplt.plot(predicted, label = 'predicción')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLSTM captura muy bien el comportamiento de las series temporales para hacer predicciones precisas.\n\nplt.title('Desempeño LSTM')\nplt.yscale('log')\nplt.plot(training_loss, label = 'Entrenamiento')\nplt.plot(validation_loss, label = 'validación')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nMirando, concluimos que detuvimos el proceso de entrenamiento demasiado temprano. Obtenemos modelos más precisos si establecemos más epocas (epoch) para el entrenamiento.\n\n\nCONCLUSIONES\nPudimos ver que las redes neuronales recurrentes muestran excelentes resultados y son adecuadas para problemas de pronósticos de series de tiempo.\nLas Redes Neuronales Recurrentes son la técnica muy popular de aprendizaje profundo (Deep Learning) para el pronóstico de series de tiempo, ya que permiten producir predicciones confiables en series de tiempo en diversos problemas. El principal problema con RNN es que sufre el problema de fuga de gradiente cuando se aplica a secuencia largas, y no tiene una herramienta de memoria a largo plazo. Se desarrollaron LSTM y GRU para evitar el problema de gradiente de RNN con el uso de puertas que regulan el flujo de información e implementan el almacenamiento de memoria a largo plazo. El uso de LSTM y GRU ofrece resultados notables, pero LSTM y GRU no siempre funcionan mejor que RNN.\n\nRNN tiene un estado oculto que puede tratarse como una memoria interna de la secuencia de entrada.\nRNN vuelve a calcular el estado oculto después de procesar cada nuevo valor de entrada de forma recurrente.\nRNN sufre un problema de fuga de gradiente.\nRNN actualiza un estado oculto en cada iteración. Por tanto, no tiene memoria a largo plazo.\nGRU implementa la puerta de reinicio, que rechaza algunas actualizaciones en un estado oculto.\nLSTM pasa dos vectores a través de cada iteración: estado oculto y estado de celda.\n\n\n\nREFERENCIAS\n\nTime Series Forecasting Using Deep Learning - Ivan Gridin\nUnderstanding GRU Networks"
  },
  {
    "objectID": "posts/estructuras_mercado/index.html",
    "href": "posts/estructuras_mercado/index.html",
    "title": "Estructuras de Mercado con Python",
    "section": "",
    "text": "Este post tiene como objetivo dar a conocer la importancia del software de Python en el ambito microeconomico, particularmente en este caso hablamos de las diferentes estructuras de mercado; competencia perfecta, monopolio y oligopolio."
  },
  {
    "objectID": "posts/estructuras_mercado/index.html#condiciones-necesaria-para-la-competencia-perfecta",
    "href": "posts/estructuras_mercado/index.html#condiciones-necesaria-para-la-competencia-perfecta",
    "title": "Estructuras de Mercado con Python",
    "section": "Condiciones necesaria para la competencia perfecta",
    "text": "Condiciones necesaria para la competencia perfecta\n\nMuchos productores, ninguno de los cuales tiene una gran cuota de mercado.\nUna industria puede ser perfectamente competitiva sólo si los consumidores consideran como equivalentes a los productos de todos los productores (producto homogéneo)"
  },
  {
    "objectID": "posts/estructuras_mercado/index.html#libre-entrada-y-salida",
    "href": "posts/estructuras_mercado/index.html#libre-entrada-y-salida",
    "title": "Estructuras de Mercado con Python",
    "section": "Libre entrada y salida",
    "text": "Libre entrada y salida\nExiste libre entrada y salida en una industria cuando nuevos productores pueden entrar facilmente en esa industria a los que ya estan en ella pueden abondonarla sin coste alguno.\n\nRegla de Producción Optima\nLa regla de producción optima dice que el beneficio se maximiza cuando se produce la cantidad de output para la cual el ingreso marginal de la última unidad de output producida es igual a su coste marginal.\n\\[\nIMg = CMg\n\\]\n\n\nFunción de Benenficios\nLa función de beneficios \\((\\pi)\\) representa las diferencias entre los costos totales, \\(C(Q)\\) e ingresos totales,\\(R(Q)\\) , de las empresas\n\\[\n\\pi = R(Q) - C(Q)\n\\]\n\n\nTomador de Precios\nPrecio igual al costo marginal\n\\[\n\\begin{eqnarray*}\nCMg = IMg = P\n\\end{eqnarray*}\n\\]\nPor tanto, se dice que el beneficio de una empresa precio-aceptante se maximiza produciendo la cantidad de output para la cual el costo marginal de la última unidad producida es igual al precio de mercado, tal como se aprecia en el siguiente gráfico\n\n\n\nCantidad de producto que maximiza el beneficio de una empresa precio-aceptante"
  },
  {
    "objectID": "posts/estructuras_mercado/index.html#costes-y-producción-en-el-corto-plazo",
    "href": "posts/estructuras_mercado/index.html#costes-y-producción-en-el-corto-plazo",
    "title": "Estructuras de Mercado con Python",
    "section": "Costes y Producción en el Corto Plazo",
    "text": "Costes y Producción en el Corto Plazo\nEn el corto plazo tenemos las siguientes condiciones de producción de empresas competitivas\n\n\n\n\n\n\n\nCondiciones\nResultados\n\n\n\n\nP &gt; CVMe mínimo\nLa empresa produce en el corto plazo. Si P &lt; CTMe mínimo, la empresa cubre sus costos variables y parte de sus costes fijos pero no todos. Si P &gt; CTMe mínimo, la empresa cubre todos sus costes variables y sus costes fijos.\n\n\nP = CVMe mínimo\nLa empresa es indiferente entre producir en el corto plazo o no producir. Cubre exactamente sus costes variables.\n\n\nP &lt; CVMe mínimo\nLa empresa cierra en el corto plazo. No cubre sus costes variables."
  },
  {
    "objectID": "posts/estructuras_mercado/index.html#ejemplo-1--corto-plazo",
    "href": "posts/estructuras_mercado/index.html#ejemplo-1--corto-plazo",
    "title": "Estructuras de Mercado con Python",
    "section": "Ejemplo # 1- Corto Plazo",
    "text": "Ejemplo # 1- Corto Plazo\nPrimero resolveremos el siguiente ejercicio de manera manual y posteriormente lo resolveremos en Python.\nSuponga que la empresa tiene una curva de costos de corto plazo dada por\n\\[\nC(Q) = 100 + 20Q + Q^2\n\\]\n\n¿Cuál es la ecuación para el costo variable Medio?\n¿Cuál es el valor mínimo para el costo variable promedio?\n¿Cuál es la curva de oferta de corto plazo?\n\nSolución\n\nDada la función de costo \\(C(Q) = 100 + 20Q + Q^2\\) es claro que el costo variable, CV, esta dado por \\[CV = 20Q + Q^2\\] por tanto su costo variable promedio es \\[CVMe = \\frac{CV}{Q} = 20 + Q\\]\nAhora bien, su costo marginal sabemos que unicamente requiere aplicar la regla de diferenciación, ya que \\[CMg = \\frac{\\partial C(Q)}{\\partial Q} = 20 + 2Q\\]\nSi queremos encontrar el costo variable promedio mínimo, \\[CVMe_{\\min}\\], se obtiene como \\[CMg = CVMe \\longrightarrow Q = \\fbox{0}\\]\nEntonces la función de oferta es: \\[\\begin{eqnarray*}CMg &=& p\\\\[0.2cm] 20 + 2Q &=& P\\\\[0.2cm] Q(P) &=& \\frac{P}{2} - 10 \\end{eqnarray*}\\]\n\nPor tanto, también podemos obtener el precio de equilibrio, ya que \\[0 = \\frac{P}{2} - 10 \\longrightarrow P = \\fbox{20}\\]\nAhora, encontremos estos resultados en Python:\n\n# Paquete previo \nfrom sympy import *\nQ = symbols(\"Q\")\n\n\n# función de costo de corto plazo \nCT = 100 + 20*Q + Q**2\n# costo variale promedio \nCV = 20 + Q \n# Encontrar el costo variable minimo \n# Primero: costo marginal\n\nCM = diff(CT,Q)\n\n\n# igualar costo marginal y costo variable promedio \nsolve(Eq(CM,CV))\n\n[0]\n\n\n\ncantidad = solve(Eq(CM,CV))\ncantidad[0]\n\n\\(\\displaystyle 0\\)\n\n\n\nP = CV.subs({Q:cantidad[0]})\nP\n\n\\(\\displaystyle 20\\)\n\n\n\nplot(CT, CT/Q, CV, CM, (Q,0,100), xlim = (0, 100), ylim = (0,100), xlabel = \"Q\", ylabel = \"P\")\n\n\n\n\n\n\n\n\nPuedes notar lo rápido y fácil que resulta realizar estos procedimientos con Python y la utilidad que puede brindarte en caso de que trabajes con volumnes de datos.\n\nEjemplo # 2 - Corto Plazo\nAhora suponga que la empresa tiene una curva costos en el corto plazo de la siguiente forma:\n\\[\nC(Q) = 1 + 10Q + Q^2\n\\]\nSi la empresa opera en un mercado perfectamente competitivo, donde \\(P = 12\\), ¿Cuál será los beneficios de la empresa en el corto plazo?\nSolución\nSabemos que la función de beneficios esta dada por\n\\[\n\\pi = R - C\n\\]\nentonces,\n\\[\n\\frac{\\partial \\pi}{\\partial Q} = IMg - CMg = 0\n\\]\nasí pues,\n\\[\nCMg = 10 + 2Q \\hspace{1cm}y\\hspace{1cm} IMg = P\n\\]\npor tanto,\n\\[\n\\begin{eqnarray*}\nCMg &=& IMg\\\\[0.2cm]\n10 + 2Q &=& P\\\\[0.2cm]\nQ &=& \\frac{P}{2} - 5\\\\[0.2cm]\nQ &=& \\frac{12}{2} - 5, \\hspace{2cm}\\mbox{ya que P = 12}\\\\[0.2cm]\nQ &=& \\fbox{1}\n\\end{eqnarray*}\n\\]\nentonces,\n\\[\n\\pi = 12 - (1 + 10 +1) = \\fbox{0}\n\\]\nAhora veamos esta solución en Python:\n\n# Función de costos a corto plazo \nQ = symbols(\"Q\")\nCT = Q**2 + 10*Q + 1\nP = 12\nR = P*Q\n# costo marginal\nCM = diff(CT,Q)\nCM\nIM = diff(R,Q)\nIM\ncantidad = solve(Eq(IM,CM))\nprint(\"El valor de la producción que garantiza un equilibrio será:\", cantidad[0])\n\nEl valor de la producción que garantiza un equilibrio será: 1\n\n\nEste resultado lo que nos dice es que la empresa oferta una unidad de producción \\(Q = 1\\).\n\n# Beneficio = IT - CT\ncosto = CT.subs({Q:cantidad[0]})\ncosto\n\n\\(\\displaystyle 12\\)\n\n\n\ningreso = R.subs({Q:cantidad[0]})\ningreso \n\n\\(\\displaystyle 12\\)\n\n\n\nBeneficios = R - CT\npi = Beneficios.subs({Q:cantidad[0]})\npi\n\n\\(\\displaystyle 0\\)\n\n\n\nplot(CT,CM,CT/Q,(Q,0,60), xlim=(0,5), ylim=(0,30), xlabel='Q', ylabel='CT,CM')\n\n\n\n\n\n\n\n\nRecuerde que todo este análisis se realizo para un mercado en competencia perfecta a corto plazo.\nPronto actualizare para el mercado en competencia perfecta a largo plazo, monopolio, e introducirnos un poco a la teoria de juegos."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Juan Isaula",
    "section": "",
    "text": "Teorema de Bayes Aplicado al Credit Scoring\n\n\nBad/Good\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nNov 7, 2025\n\n\nJuan Isaula\n\n\n\n\n\n\n\n\n\n\n\n\nAprendizaje No Supervisado\n\n\nPython\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nSep 7, 2025\n\n\nJuan Isaula\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning\n\n\nPyTorch\n\n\n\nPython\n\nPyTorch\n\n\n\n\n\n\n\n\n\nJul 1, 2025\n\n\nJuan Isaula\n\n\n\n\n\n\n\n\n\n\n\n\nArquitectura de Redes Neuronales\n\n\n\n\n\n\nRNN\n\nGRU\n\nLSTM\n\nPyTorch\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nJuan Isaula\n\n\n\n\n\n\n\n\n\n\n\n\nCredit Scoring and Segmentation using Python\n\n\nMachine Learning\n\n\n\nFICO\n\nCredit Scores\n\nPython\n\nsklearn\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nJuan Isaula\n\n\n\n\n\n\n\n\n\n\n\n\nEstructuras de Mercado con Python\n\n\nCompetencia Perfecta, Monopolio y Oligopolio\n\n\n\nCMg\n\nCVP\n\nCTP\n\nPython\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJuan Isaula\n\n\n\n\n\n\n\n\n\n\n\n\nMicroeconomía Intermedia con R\n\n\nUtilidad y sus Curvas de Indiferencia\n\n\n\nEconomía\n\nMicroeconomía\n\nR\n\nRStudio\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nJuan Isaula\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/microeconomia_R/index.html",
    "href": "posts/microeconomia_R/index.html",
    "title": "Microeconomía Intermedia con R",
    "section": "",
    "text": "Una forma de iniciar el análisis de los individuos es plantear un conjunto básico de postulados, o axiomas, que describen el comportamiento racional del mismo. Supondremos que dadas tres canastas de consumo cualesquiera \\((x_1,x_2)\\), \\((y_1,y_2)\\) y \\((z_1,z_2)\\). El consumidor puede ordenarlas según su atractivo. Es decir, puede decidir que una de ellas es estrictamente mejor que la otra o bien que le son indiferentes.\nUtilizaremos la notación:\n\n\\(\\succ\\) Para indicar que una canasta se prefiere estrictamente a otra, es decir, \\((x_1,x_2) \\succ (y_1,y_2)\\).\n\\(\\sim\\) Para indicar que al consumidor le resulta indiferente elegir una u otra de las dos canastas de bienes y lo representamos matemáticamente como \\((x_1,x_2)\\sim (y_1,y_2)\\).\n\\(\\succeq\\) Para indicar si el individuo prefiere una de las dos canastas o es indiferente entre ellas, decimos que prefiere debilmente la canasta \\((x_1,x_2)\\) a la \\((y_1,y_2)\\) y escribimos \\((x_1,x_2)\\succeq (y_1,y_2)\\).\n\n\n\nCon base en lo anterior, ya estamos preparados para conocer los tres axiomas de la teoría del consumidor. Decimos que las preferencias son:\n\nCompletas: suponemos que es posible comprar dos canastas cualesquiera, es decir, dada cualquier canasta \\(\\textbf{X}\\) y cualquier canasrta \\(\\textbf{Y}\\), suponemos que \\((x_1,x_2)\\succeq (y_1,y_2)\\) o \\((y_1,y_2) \\succeq (x_1,x_2)\\) o las dos cosas, en cuyo caso el consumidor es indiferente entre las dos canastas.\nReflexivas: suponemos que cualquier canasta es al menos tan buena como ella misma: \\((x_1,x_2)\\succeq (y_1,y_2)\\).\nTransitiva: si \\((x_1,x_2)\\succeq (y_1,y_2)\\) y \\((y_1,y_2)\\succeq (z_1,z_2) \\Longrightarrow (x_1,x_2)\\succeq (z_1,z_2)\\). Es decir, si el consumidor piensa que la canasta \\(\\textbf{X}\\) es al menos tan buena como la \\(\\textbf{Y}\\) y que la \\(\\textbf{Y}\\) es al menos tan buena como la \\(\\textbf{Z}\\), piensa que la \\(\\textbf{X}\\) es al menos tan buena como la \\(\\textbf{Z}\\).\n\nConsidere que cuando nos referimos a las canastas \\(\\textbf{X}, \\textbf{Y}\\) o \\(\\textbf{Z}\\) estamos haciendo referencia a:\n\n\\(\\textbf{X} = (x_1,x_2)\\)\n\\(\\textbf{Y} = (y_1.y_2)\\)\n\\(\\textbf{Z} = (z_1,z_2)\\)\n\nSi las preferencias no fueran transitivas, podría muy bien haber un conjunto de canastas tal que ninguna de las elecciones fuera la mejor. Sin embargo, en el curso de microeconomía II estamos trabajando bajo el modelo tradicional, donde asumimos que el individuo es razonal, tomando en cuenta que siempre va a preferir mas que menos.\n\n\n\nEl primer axioma, la completitud, es dificilmente criticable, al menos en el caso de los tipos de elecciones que suelen analizar los economistas. Decir que pueden compararse dos canastas cualesquiera es decir simplemente que el consumidor es capaz de elegir entre dos canasas cualesquiera.\nEl segundo axioma, la reflexividad, plantea más problemas. Una canasta cualquiera es ciertamente tan buena como una canasta idéntica.\nEl tercer axioma, la transitividad, plantea más problemas. No esta claro que las preferencias deban tener necesariamente esta propiedad. El supuesto de que son transitivas no parece evidente desde un punto de vista puramente lógico, y, de hecho, no lo es. La transitividad es una hipótesis sobre la conducta de los individuos en sus elecciones y no una afirmación lógica. Sin embargo, no importa que sea o no un hecho lógico básico; lo que importa es que sea o no una descripción razonablemente exacta del comportamiento de los individuos.\n¿Qué pensarías de una persona que dijera que prefiere la canasta \\(\\textbf{X}\\) a la \\(\\textbf{Y}\\) y la \\(\\textbf{Y}\\) a la \\(\\textbf{Z}\\), pero que también dijera que prefiere la \\(\\textbf{Z}\\) a la \\(\\textbf{X}\\)? Desde luego, lo consideraríamos como prueba de una conducta particular. Y lo que es más importante, ¿Cómo se comportaría este consumidor si tuviera que elegir entre las tres canastas \\(\\textbf{X}, \\textbf{Y}\\) y \\(\\textbf{Z}\\)?"
  },
  {
    "objectID": "posts/microeconomia_R/index.html#axiomas-de-la-teoría-del-consumidor",
    "href": "posts/microeconomia_R/index.html#axiomas-de-la-teoría-del-consumidor",
    "title": "Microeconomía Intermedia con R",
    "section": "",
    "text": "Con base en lo anterior, ya estamos preparados para conocer los tres axiomas de la teoría del consumidor. Decimos que las preferencias son:\n\nCompletas: suponemos que es posible comprar dos canastas cualesquiera, es decir, dada cualquier canasta \\(\\textbf{X}\\) y cualquier canasrta \\(\\textbf{Y}\\), suponemos que \\((x_1,x_2)\\succeq (y_1,y_2)\\) o \\((y_1,y_2) \\succeq (x_1,x_2)\\) o las dos cosas, en cuyo caso el consumidor es indiferente entre las dos canastas.\nReflexivas: suponemos que cualquier canasta es al menos tan buena como ella misma: \\((x_1,x_2)\\succeq (y_1,y_2)\\).\nTransitiva: si \\((x_1,x_2)\\succeq (y_1,y_2)\\) y \\((y_1,y_2)\\succeq (z_1,z_2) \\Longrightarrow (x_1,x_2)\\succeq (z_1,z_2)\\). Es decir, si el consumidor piensa que la canasta \\(\\textbf{X}\\) es al menos tan buena como la \\(\\textbf{Y}\\) y que la \\(\\textbf{Y}\\) es al menos tan buena como la \\(\\textbf{Z}\\), piensa que la \\(\\textbf{X}\\) es al menos tan buena como la \\(\\textbf{Z}\\).\n\nConsidere que cuando nos referimos a las canastas \\(\\textbf{X}, \\textbf{Y}\\) o \\(\\textbf{Z}\\) estamos haciendo referencia a:\n\n\\(\\textbf{X} = (x_1,x_2)\\)\n\\(\\textbf{Y} = (y_1.y_2)\\)\n\\(\\textbf{Z} = (z_1,z_2)\\)\n\nSi las preferencias no fueran transitivas, podría muy bien haber un conjunto de canastas tal que ninguna de las elecciones fuera la mejor. Sin embargo, en el curso de microeconomía II estamos trabajando bajo el modelo tradicional, donde asumimos que el individuo es razonal, tomando en cuenta que siempre va a preferir mas que menos."
  },
  {
    "objectID": "posts/microeconomia_R/index.html#explicación-de-los-axiomas",
    "href": "posts/microeconomia_R/index.html#explicación-de-los-axiomas",
    "title": "Microeconomía Intermedia con R",
    "section": "",
    "text": "El primer axioma, la completitud, es dificilmente criticable, al menos en el caso de los tipos de elecciones que suelen analizar los economistas. Decir que pueden compararse dos canastas cualesquiera es decir simplemente que el consumidor es capaz de elegir entre dos canasas cualesquiera.\nEl segundo axioma, la reflexividad, plantea más problemas. Una canasta cualquiera es ciertamente tan buena como una canasta idéntica.\nEl tercer axioma, la transitividad, plantea más problemas. No esta claro que las preferencias deban tener necesariamente esta propiedad. El supuesto de que son transitivas no parece evidente desde un punto de vista puramente lógico, y, de hecho, no lo es. La transitividad es una hipótesis sobre la conducta de los individuos en sus elecciones y no una afirmación lógica. Sin embargo, no importa que sea o no un hecho lógico básico; lo que importa es que sea o no una descripción razonablemente exacta del comportamiento de los individuos.\n¿Qué pensarías de una persona que dijera que prefiere la canasta \\(\\textbf{X}\\) a la \\(\\textbf{Y}\\) y la \\(\\textbf{Y}\\) a la \\(\\textbf{Z}\\), pero que también dijera que prefiere la \\(\\textbf{Z}\\) a la \\(\\textbf{X}\\)? Desde luego, lo consideraríamos como prueba de una conducta particular. Y lo que es más importante, ¿Cómo se comportaría este consumidor si tuviera que elegir entre las tres canastas \\(\\textbf{X}, \\textbf{Y}\\) y \\(\\textbf{Z}\\)?"
  },
  {
    "objectID": "posts/microeconomia_R/index.html#curvas-de-indiferencia",
    "href": "posts/microeconomia_R/index.html#curvas-de-indiferencia",
    "title": "Microeconomía Intermedia con R",
    "section": "Curvas de Indiferencia",
    "text": "Curvas de Indiferencia\nCon base en la definición previa de utilidad, podemos concluir, una función de utilidad es la que explica la cantidad de utilidad que posee un consumidor dado su consumo de dos bienes diferentes. \\(x, y\\). Una curva de indiferencia es solo una rebanada infenitesimal de esa función que describe todas las diferentes combinaciones entre dos bienes que producen la misma cantidad de utilidad (es decir, a la que una persona sería indiferente).\nSupongamos que una persona clasifica las hamburguesas \\((y)\\) y las bebidas \\((x)\\) de acuerdo con la función de utilidad\n\\[\nU(x,y) = \\sqrt{xy}\n\\]\nEn el caso de esta función, obtenemos la curva de indiferencia identificando un conjunto de combinaciones de \\(x,y\\) en el cual la utilidad tiene el mismo valor. Suponga que arbitrariamente decimos que la utilidad tiene un valor de 10. Entonces, la ecuación de esta curva sera:\n\\[\nU(x,y) = 10 = \\sqrt{xy}\n\\]Note que si elevamos esta función al cuadrado se mantiene el mismo orden, por lo cual también podemos representar esta curva de indiferencia como\n\\[\n100 = xy\n\\]\nEs importante siempre despejar este tipo de ecuaciones para \\(y\\) la importancia esta en que será mucho más facil posteriormente encontrar su tasa marginal de sustitución ( en otra sección de esta publicación estudiaremos a detalle esto), entonces, al despejar obtenemos:\n\\[\ny = \\frac{100}{x}\n\\]\nPara trazar su curva de indiferencia, lo haremos en R , a continuación les muestro como hacerlo. Puedes realizar este ejercicio en tu PC tu mismo.\n\n# 1. Primero cargamos las librerias que utilizaremos, en caso que nos las tengas \n#    instaladas sugiero lo hagas usando install.package(\"libreria\") en su consola\n#    de Rstudio.\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(tidyverse)\nlibrary(plotly)\n\n# 2. Creamos la función de utilidad del ejemplo \nutilidad &lt;- function(x,y){\n  sqrt(x*y) \n}\n\n# 3. Creamos una matriz para hacer un bucle en la función de utilidad\nvalores_matriz &lt;- matrix(0,nrow = 200, ncol = 200)\n\n# 3.1 Llenamos la matriz con usando la función de utilidad \nfor(fila in 1:nrow(valores_matriz)){\n  for(columna in 1:ncol(valores_matriz)){\n    valores_matriz[fila,columna] &lt;- utilidad(fila,columna)\n  }\n}\n\n# 4. Función que nos permitira graficar las curvas de indiferencia \n\nC_indiferencia &lt;- function(entrada_utilidad){\n  y &lt;- c()\n  \n  for(i in 1:50){\n    y_coord &lt;- entrada_utilidad^2/i\n    y       &lt;- c(y,y_coord)\n  }\n  \n  data &lt;- data.frame(\n    x = 1:50,\n    y = y,\n    z = rep(entrada_utilidad,50)\n  )\n  \n  return(data)\n}\n\n\n# 4.1 Resultado de utilidades obtenidas \nlista_utilidades &lt;- lapply(10, C_indiferencia)\n\nfull_df &lt;- do.call(rbind, lista_utilidades)\n\nAhora si ya estamos preparados para graficar nuestras curvas de indiferencia para \\(10 = \\sqrt{xy}\\)\n\n# 5. Gráfico\n\nggplot() + \n  geom_point(data = full_df, aes(x = x, y = y, color = z)) + \n  geom_path(data = full_df, aes(x = x, y = y, color = z)) +\n  theme_minimal()+\n  ylim(0,100) + \n  labs(x = \"Bebidad\", y = \"Hamburguesas\") + \n  scale_color_continuous(name = \"Utilidad\")\n\n\n\n\n\n\n\n\nNote que la curva previa representa una utilidad = 10.\nA continuación te muestro un grafico animado de la curva de indiferencia previa. Para generar el gráfico presiona el boton PLAY.\n\n\n\n\n\n\nVeamos que sucede cuando tenemos diferentes niveles de utilidad, en base al resultado usted puede deducir su propio análisis.\n\nutilidad &lt;- function(x,y){\n  sqrt(x*y) \n}\n\nvalores_matriz &lt;- matrix(0,nrow = 200, ncol = 200)\n\nfor(fila in 1:nrow(valores_matriz)){\n  for(columna in 1:ncol(valores_matriz)){\n    valores_matriz[fila,columna] &lt;- utilidad(fila,columna)\n  }\n}\n\nC_indiferencia &lt;- function(entrada_utilidad){\n  y &lt;- c()\n  \n  for(i in 1:100){\n    y_coord &lt;- entrada_utilidad^2/i\n    y       &lt;- c(y,y_coord)\n  }\n  \n  data &lt;- data.frame(\n    x = 1:100,\n    y = y,\n    z = rep(entrada_utilidad,100)\n  )\n  \n  return(data)\n}\n\nlista_utilidades &lt;- lapply(seq(from =10, to = 60, by = 10), C_indiferencia)\n\nfull_df &lt;- do.call(rbind, lista_utilidades)\n\n\nggplot() +\n  geom_point(data = full_df, aes(x = x, y = y, color = z)) +\n  geom_path(data = full_df, aes(x = x, y = y, color = z)) +\n  theme_minimal() +\n  ylim(0,200) +\n  labs(x = \"Bebidas\", y = \"Hamburguesas\") +\n  scale_color_continuous(name = \"Utilidad\")\n\n\n\n\n\n\n\n\nAquí podemos señalar lo siguiente:\n\nA medida que aumenta la utilidad, las curvas se desplazan hacia la derecha y hacia la izquierda a medida que disminuye la utilidad.\nObserve que las curvas se inclinan hacia abajo, esto debe ser necesariamente el caso; a medida que uno aumenta su consumo de bebidas renuncia al otro bien que les era indiferente, hamburguesa.\nTodo lo que está debajo de la curva representa paquetes con menos utilidad. La teoría de la utilidad asume que un consumidor siempre buscará maximizar la utilidad.\nComprende que la pendiente no es lineal. En genera, cuanto más se tiene de algo, menos utilidad se obtendrá de otra unidad y, por el contrario, más se renunciaría a adquirir el otro bien. Esta pendiente tiene un nombre oficial: Tasa Marginal de Sustitución o TMS hablaremos de esto en una sección posterior.\n\nPero esas son solo algunas rebanadas que ya he señalado como infinitesimalmente pequeñas.\nPara concluir esta sección te dejo un gráfico animado de los diferentes niveles de utilidad, por favor presiona el boton PLAY para que logres verlo."
  },
  {
    "objectID": "posts/microeconomia_R/index.html#tasa-marginal-de-sustitución-tms",
    "href": "posts/microeconomia_R/index.html#tasa-marginal-de-sustitución-tms",
    "title": "Microeconomía Intermedia con R",
    "section": "Tasa Marginal de Sustitución (TMS)",
    "text": "Tasa Marginal de Sustitución (TMS)\nOtro concepto importante en la teoría del consumidor es la Tasa Marginal de Sustitución (TMS). Matemáticamente esto es la pendiente de la curva de indiferencia, sin embargo, en términos microecnómicos esta pendiente se refiere a la relación de cambio entre un bien \\(x\\) y el bien \\(y\\), es decir, cuanto del bien x se tiene que sacrificar (aumentar) para aumentar (disminuir) el consumo del bien y para aumentarse en el mismo nivel de utilidad.\nEn términos matemáticos la TMS se define como:\n\\[\nTMS = -\\left.\\begin{array}{c}\\frac{dy}{dx} \\end{array}\\right|_{U = U_1}\n\\]\ndonde la notación indica que la pendiente se debe calcular a lo largo de la cuva de indiferencia \\(U_1\\).\nUn ejercicio interesante para el lector, seria intentar probar la identidad de \\(TMS\\) que acabamos de definir.\n\nMúltiples Curvas de Indiferencia\nHay una curva de indiferencia que pasa por cada punto del plano \\(xy\\). Cada una de estas curvas muestra combinaciones de \\(x\\) y \\(y\\) que proporcionan al individuo determinado nivel de satisfación como se vio en graficos anteriores. Los movimientos en dirección noreste representan movimientos hacia niveles más altos de satisfación.\n\n\n\nEl cambio de la pendiente a lo largo de U1 muestra que la canasta de consumo disponible afecta los intercambios que esta persona realizará libremente\n\n\nDado que ya conocemos en que consiste la TMS, y a este punto asumo que el lector ya tiene idea de como realizar el computo manual de la TMS para una función de utilidad dada. Entonces, procederemos a realizar el computo de la TMS en R, para el computo se procederá a crear una función que resuelva el problema más rapidamente, ya que como usted se ha podido dar cuenta se necesita y usar calculo diferencial (derivadas).\nLa función que crearemos para realizar el computo de la TMS la llamaremos TMS, veamos como hacerlo en R.\n\nTMS &lt;- function(fun.utilidad, bien_x) {\n  U  &lt;- parse(text = fun.utilidad)\n  v1 &lt;- D(U, \"x\")                       # D() función que realiza la derivada de U\n  print(paste(\"TMS = \", \n              eval(v1, envir = list(x = bien_x)), \"considerando\", \n              bien_x, \"unidades del bien x\"))\n}\n\nDel bloque de código previo:\n\nTMS: función que calcula la tasa marginal de sustitución.\nfun.utilidad: función de la curva de indiferencia (y en función de x). Tiene que especificarse en caracteres.\nbien_x: unidades del bien x en donde se evaluara la TMS.\n\n\n\nEjemplo\nconsideremos la siguiente función de utilidad con su respectivo nivel de utilidad\n\\[\nU(x,y) = 10 = \\sqrt{xy}\n\\]\nSi calculamos la TMS de esta curva de indiferencia de manera manual tendremos:\n\\[\nTMS = -\\frac{\\frac{\\partial U}{\\partial x}}{\\frac{\\partial U}{\\partial y}} = -\\frac{100}{x^2}\n\\]\nComo se puede dar cuenta la TMS depende de “x”, lo que indica que tenemos que variar “x” positiva o negativamente , con el fin de obtener menos o más de “y” y mantenernos en la misma utilidad de 10.\nSi evaluamos la TMS cuando el bien “x” es igual a 5, entonces, la TMS sera de\n\\[\nTMS = -\\frac{100}{5^2} = -4\n\\]\nEsto implica, que si aumentamos el consumo del bien “x” en 1 tendremos que disminuir el consumo del bien y en 4. Veamos este ejemplo en R.\n\n# bien_x = 5\n# Utilizamos la función TMS que creamos previamente \n\nTMS(fun.utilidad = \"100/x\", 5)\n\n[1] \"TMS =  -4 considerando 5 unidades del bien x\"\n\n\nNote que el resultado es el deseado. Pero si queremos ver como varía la TMS para distintas cantidades del bien “x”, podemos hacer pequeñas variaciones a la función (TMS) que definimos en el primer bloque de código de esta sección.\n\nVar_TMS &lt;- function(fun.utilidad, bien_x){\n  U  &lt;- parse(text = fun.utilidad)\n  v1 &lt;- D(U, \"x\")\n  eval(v1, envir = list(x = bien_x))\n}\n\n\n# Veamos el comportamiento de la TMS cuando variamos el bien x\n\nw &lt;- c()\nfor (i in seq(60, 10, -10)){\n  t &lt;- Var_TMS(fun.utilidad = \"100/x\",i)\n  w &lt;- c(w,t)\n}\n\nw\n\n[1] -0.02777778 -0.04000000 -0.06250000 -0.11111111 -0.25000000 -1.00000000\n\n\nVeamos que a medida que el bien x pasa de ser abundante a ser un bien escazo cada vez le resulta al consumidor más relevante y si desea obtener una unidad adicional del bien x tendrá que renunciar a más cantidad del bien y. Es así que si el consumidor tiene solo un bien, cambiará este bien siempre y cuando reciba cien unidades del bien y.\nEn la siguiente sección de este post verá el tema de maximización de la utilidad dado una restricción presupuestaria. Es decir, desarrollamaremos el cálculo óptimo de los bienes, que maximizan la función de utilidad."
  },
  {
    "objectID": "posts/Credit_Scoring/index.html",
    "href": "posts/Credit_Scoring/index.html",
    "title": "Credit Scoring and Segmentation using Python",
    "section": "",
    "text": "La calificación crediticia y la segmentación se refieren al proceso de evaluar la solvencia de personas o empresas y dividirlos en distintos grupos según sus perfiles crediticios. Su objetivo es evaluar la probabilidad de que los prestatarios pagen sus deudas y ayuda a las instituciones financieras a tomar decisiones informadas sobre préstamos y gestión del riesgo crediticio. Si desea aprender a calcular puntajes crediticios y segmentar clientes en función de sus puntajes crediticios, este artículo es para usted. En este artículo, lo guiaré a través de la tarea de segementación y calificación crediticia usando Python."
  },
  {
    "objectID": "posts/Credit_Scoring/index.html#calificación-crediticia-y-segmentación-usando-python",
    "href": "posts/Credit_Scoring/index.html#calificación-crediticia-y-segmentación-usando-python",
    "title": "Credit Scoring and Segmentation using Python",
    "section": "Calificación crediticia y segmentación usando Python",
    "text": "Calificación crediticia y segmentación usando Python\nAhora comencemos con la tarea de segmentación y calificación crediticia importando las bibliotecas de Python necesarias y el conjunto de datos\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\ndata = pd.read_csv(\"credit_scoring.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nAge\nGender\nMarital Status\nEducation Level\nEmployment Status\nCredit Utilization Ratio\nPayment History\nNumber of Credit Accounts\nLoan Amount\nInterest Rate\nLoan Term\nType of Loan\n\n\n\n\n0\n60\nMale\nMarried\nMaster\nEmployed\n0.22\n2685.0\n2\n4675000\n2.65\n48\nPersonal Loan\n\n\n1\n25\nMale\nMarried\nHigh School\nUnemployed\n0.20\n2371.0\n9\n3619000\n5.19\n60\nAuto Loan\n\n\n2\n30\nFemale\nSingle\nMaster\nEmployed\n0.22\n2771.0\n6\n957000\n2.76\n12\nAuto Loan\n\n\n3\n58\nFemale\nMarried\nPhD\nUnemployed\n0.12\n1371.0\n2\n4731000\n6.57\n60\nAuto Loan\n\n\n4\n32\nMale\nMarried\nBachelor\nSelf-Employed\n0.99\n828.0\n2\n3289000\n6.28\n36\nPersonal Loan\n\n\n\n\n\n\n\nA continuación se muestra la descripción de todas los campos de los datos:\n\nAge: representa la edad del individuo\nGender: identifica el género del individuo\nMarital Status: denota el estado civil del individuo\nEducation Level: representa en nivel más alto de educación alcanzado por el individuo.\nEmployment Status: indica el estado de empleo actual del individuo\nCredit Utilization: refleja la proporción de crédito utilizado por el individuo en comparación con su límite de crédito total disponible.\nInterest Rate: tasa de interés asociada con el préstamo.\nPayment History: representa el comportamiento de pago neto mensual de cada cliente, tomando en cuenta factores como pagos a tiempo, pagos atrasados, pagos atrasados e incumplimientos.\nNumber of Credit Accounts: representa el conteo de cuentas de crédito activas que posee la persona.\nLoan Amount: indica el valor monetario del préstamo.\nLoan Term: indica la duraciòn o plazo del préstamo.\nType of Loan: incluye categorías como “Préstamo personal”, “Préstamo para automovil” o potencialmente otro tipos de préstamos.\n\nAhora echemos un vistazo a las estadísticas de las columnas antes de seguir adelante:\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 12 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Age                        1000 non-null   int64  \n 1   Gender                     1000 non-null   object \n 2   Marital Status             1000 non-null   object \n 3   Education Level            1000 non-null   object \n 4   Employment Status          1000 non-null   object \n 5   Credit Utilization Ratio   1000 non-null   float64\n 6   Payment History            1000 non-null   float64\n 7   Number of Credit Accounts  1000 non-null   int64  \n 8   Loan Amount                1000 non-null   int64  \n 9   Interest Rate              1000 non-null   float64\n 10  Loan Term                  1000 non-null   int64  \n 11  Type of Loan               1000 non-null   object \ndtypes: float64(3), int64(4), object(5)\nmemory usage: 93.9+ KB\n\n\nAhora echemos un vistazo a las estadísticas descriptivas de los datos:\n\ndata.describe()\n\n\n\n\n\n\n\n\nAge\nCredit Utilization Ratio\nPayment History\nNumber of Credit Accounts\nLoan Amount\nInterest Rate\nLoan Term\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1.000000e+03\n1000.000000\n1000.000000\n\n\nmean\n42.702000\n0.509950\n1452.814000\n5.580000\n2.471401e+06\n10.686600\n37.128000\n\n\nstd\n13.266771\n0.291057\n827.934146\n2.933634\n1.387047e+06\n5.479058\n17.436274\n\n\nmin\n20.000000\n0.000000\n0.000000\n1.000000\n1.080000e+05\n1.010000\n12.000000\n\n\n25%\n31.000000\n0.250000\n763.750000\n3.000000\n1.298000e+06\n6.022500\n24.000000\n\n\n50%\n42.000000\n0.530000\n1428.000000\n6.000000\n2.437500e+06\n10.705000\n36.000000\n\n\n75%\n54.000000\n0.750000\n2142.000000\n8.000000\n3.653250e+06\n15.440000\n48.000000\n\n\nmax\n65.000000\n1.000000\n2857.000000\n10.000000\n4.996000e+06\n19.990000\n60.000000\n\n\n\n\n\n\n\nAhora echemos un vistazo a la distribución del índice de utilización del crédito en los datos:\n\ncredit_utilization_fig = px.box(data, y='Credit Utilization Ratio',\n                                title='Distribución del índice de utilización del crédito')\ncredit_utilization_fig.show()\n\n                                                \n\n\nAhora echemos un vistazo a la distribución del monto del préstamo en los datos:\n\nloan_amount_fig = px.histogram(data, x='Loan Amount', \n                               nbins=20, \n                               title='Distribución del monto del préstamo')\nloan_amount_fig.show()\n\n                                                \n\n\nLuego, echemos un vistazo a la correlación en los datos:\n\nnumeric_df = data[['Credit Utilization Ratio', \n                   'Payment History', \n                   'Number of Credit Accounts', \n                   'Loan Amount', 'Interest Rate', \n                   'Loan Term']]\ncorrelation_fig = px.imshow(numeric_df.corr(), \n                            title='Mapa de calor de correlación')\ncorrelation_fig.show()"
  },
  {
    "objectID": "posts/Credit_Scoring/index.html#calcular-puntajes-de-crédito",
    "href": "posts/Credit_Scoring/index.html#calcular-puntajes-de-crédito",
    "title": "Credit Scoring and Segmentation using Python",
    "section": "Calcular puntajes de crédito",
    "text": "Calcular puntajes de crédito\nEl conjunto de datos no tiene ninguna característica que represente los puntajes crediticios de las personas. Para calcular las puntuaciones de crédito, debemos utilizar una técnica adecuada. Existen varias técnicas ampliamente utilizadas para calcular puntajes credeticios, cada una con su propio proceso de cálculo. Un ejemplo es el puntaje FICO, es un modelo de calificación crediticia comúnmente utilizado en la industria.\nA continuación se muestra cómo podemos implementar el método de puntuación FICO para calcular las puntuaciones de crédito.\n\n# Definir el mapeo para características categóricas\neducation_level_mapping = {'High School': 1, 'Bachelor': 2, 'Master': 3, 'PhD': 4}\nemployment_status_mapping = {'Unemployed': 0, 'Employed': 1, 'Self-Employed': 2}\n\n# Aplicar mapeo  a características categóricas\ndata['Education Level'] = data['Education Level'].map(education_level_mapping)\ndata['Employment Status'] = data['Employment Status'].map(employment_status_mapping)\n\n# Calcule puntajes de crédito utilizando la fórmula FICO completa\ncredit_scores = []\n\nfor index, row in data.iterrows():\n    payment_history = row['Payment History']\n    credit_utilization_ratio = row['Credit Utilization Ratio']\n    number_of_credit_accounts = row['Number of Credit Accounts']\n    education_level = row['Education Level']\n    employment_status = row['Employment Status']\n    \n    # Apliaue la fórmula FICO para calcular el puntaje crediticio\n    credit_score = (payment_history * 0.35) + (credit_utilization_ratio * 0.30) + (number_of_credit_accounts * 0.15) + (education_level * 0.10) + (employment_status * 0.10)\n    credit_scores.append(credit_score)\n\n# Agregue los puntajes de crédito como una nueva columna al DataFrame\ndata['Credit Score'] = credit_scores\n\ndata.head()\n\n\n\n\n\n\n\n\nAge\nGender\nMarital Status\nEducation Level\nEmployment Status\nCredit Utilization Ratio\nPayment History\nNumber of Credit Accounts\nLoan Amount\nInterest Rate\nLoan Term\nType of Loan\nCredit Score\n\n\n\n\n0\n60\nMale\nMarried\n3\n1\n0.22\n2685.0\n2\n4675000\n2.65\n48\nPersonal Loan\n940.516\n\n\n1\n25\nMale\nMarried\n1\n0\n0.20\n2371.0\n9\n3619000\n5.19\n60\nAuto Loan\n831.360\n\n\n2\n30\nFemale\nSingle\n3\n1\n0.22\n2771.0\n6\n957000\n2.76\n12\nAuto Loan\n971.216\n\n\n3\n58\nFemale\nMarried\n4\n0\n0.12\n1371.0\n2\n4731000\n6.57\n60\nAuto Loan\n480.586\n\n\n4\n32\nMale\nMarried\n2\n2\n0.99\n828.0\n2\n3289000\n6.28\n36\nPersonal Loan\n290.797\n\n\n\n\n\n\n\nA continuación se muestra cómo funciona el código anterior:\n\nEn primer lugar, define asignaciones para dos características categóricas: “Nivel de educación” y “Estado laboral”. La asignación de “Nivel de educación” asigna valores numéricos a diferentes niveles de educación, como “Escuela secundaria” asignada a 1, “Licenciatura” a 2, “Maestría” a 3 y “Doctorado” a 4. El “Estado de empleo” el mapeo asigna valores numéricos a diferentes estados laborales, como “desempleado” asignado a 0, “empleado” asigna 1 y “autónomo” a 2.\nA continuación, el código aplica las asignaciones definidas a las columnas correspondientes en el DataFrame. Transforma los valores de las columnas “Nivel de educación” y “Estado de empleo” de su forma categórica original a las representaciones numéricas asignadas.\nDespués de eso, el código inicia una iteración sobre cada fila del DataFrame para calcular las puntuaciones de crédito de cada individuo. Recupera los valores de características relevantes, como “Historial de pagos”, “índice de utilización de crédito”, “Número de cuentas de crédito”, “Nivel de educación” y “Estado de empleo”, de cada fila.\n\nDentro de la iteración, se aplica la fórmula FICO para calcular el puntaje de crediticio de cada individuo. La fórmula incorpora los valores ponderados de las características mencionadas anteriormente:\n\nPeso del 35% para “Historial de pagos (Payment History)”\nPeso del 30% para el “índice de utilización de crédito (Credit Utilization Ratio)”\nPeso del 15% para “Número de cuentas de crédito (Number of Credit Accounts)”\n10% de peso para “Nivel de educación (Education Level)”\ny 10% de ponderación para “Estatus laboral (Employment Status)”\n\nLuego, el puntaje crediticio calculado se almacena en una lista llamada \"credit_scores\"."
  },
  {
    "objectID": "posts/Credit_Scoring/index.html#segmentación-basada-en-puntakes-crediticios",
    "href": "posts/Credit_Scoring/index.html#segmentación-basada-en-puntakes-crediticios",
    "title": "Credit Scoring and Segmentation using Python",
    "section": "Segmentación basada en puntakes crediticios",
    "text": "Segmentación basada en puntakes crediticios\nAhora, usemos el algoritmo de agrupamiento KMeans para segmentar a los clientes según sus puntajes crediticios.\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nX = data[['Credit Score']]\nX = np.nan_to_num(X)\nkmeans = KMeans(n_clusters=4, n_init=10, random_state=42)\nkmeans.fit(X)\ndata['Segment'] = kmeans.labels_\n\nAhora echemos un vistazo a los segmentos:\n\ndata.head()\n\n\n\n\n\n\n\n\nAge\nGender\nMarital Status\nEducation Level\nEmployment Status\nCredit Utilization Ratio\nPayment History\nNumber of Credit Accounts\nLoan Amount\nInterest Rate\nLoan Term\nType of Loan\nCredit Score\nSegment\n\n\n\n\n0\n60\nMale\nMarried\n3\n1\n0.22\n2685.0\n2\n4675000\n2.65\n48\nPersonal Loan\n940.516\n3\n\n\n1\n25\nMale\nMarried\n1\n0\n0.20\n2371.0\n9\n3619000\n5.19\n60\nAuto Loan\n831.360\n3\n\n\n2\n30\nFemale\nSingle\n3\n1\n0.22\n2771.0\n6\n957000\n2.76\n12\nAuto Loan\n971.216\n3\n\n\n3\n58\nFemale\nMarried\n4\n0\n0.12\n1371.0\n2\n4731000\n6.57\n60\nAuto Loan\n480.586\n0\n\n\n4\n32\nMale\nMarried\n2\n2\n0.99\n828.0\n2\n3289000\n6.28\n36\nPersonal Loan\n290.797\n0\n\n\n\n\n\n\n\n\n# Convertir la columna segmento al tipo de datos categoría\ndata['Segment'] = data['Segment'].astype('category')\n\n# Visualiza los segmentos usando Plotly\nfig = px.scatter(data, x=data.index, y='Credit Score', color='Segment',\n                 color_discrete_sequence=['green', 'blue', 'yellow', 'red'])\nfig.update_layout(\n    xaxis_title='Indice de clientes',\n    yaxis_title='Credit Score',\n    title='Customer Segmentation based on Credit Scores'\n)\nfig.show()\n\n                                                \n\n\nAhora nombremos los segmentos según los grupos anteriores y echemos un vistazo a los segmentos nuevamente:\n\ndata['Segment'] = data['Segment'].map({2: 'Muy baja', \n                                       0: 'Baja',\n                                       1: 'Buena',\n                                       3: \"Excelente\"})\n\n# Convertir la columna segmento al tipo de datos de categoria\ndata['Segment'] = data['Segment'].astype('category')\n\n# Visualiza los segmentos usando Plotly\nfig = px.scatter(data, x=data.index, y='Credit Score', color='Segment',\n                 color_discrete_sequence=['green', 'blue', 'yellow', 'red'])\nfig.update_layout(\n    xaxis_title='Customer Index',\n    yaxis_title='Credit Score',\n    title='Customer Segmentation based on Credit Scores'\n)\nfig.show()\n\n                                                \n\n\nAsí es como puede realizar la segmentación y la calificación crediticia utilizando Python."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html",
    "href": "posts/DL_PyTorch/index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "El Deep Learning está en todas partes, desde las cámaras de los smartphones hasta los asistentes de vos o los vehículos autónomos. En este curso, descubriras esta potente tecnología y aprenderás a aprovecharla con PyTorch, una de las bibliotecas de aprendizaje profundo más populares. Al finalizar tu recorrido por este documento, serás capaz de aprovechar PyTorch para resolver problemas de clasificación y regresión utilizando el aprendizaje profundo."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#qué-es-deep-learning",
    "href": "posts/DL_PyTorch/index.html#qué-es-deep-learning",
    "title": "Deep Learning",
    "section": "Qué es Deep Learning?",
    "text": "Qué es Deep Learning?\n\n\n\n\n\nDeep Learning (aprendizaje profundo) es un subconjunto del aprendizaje automático (machine learning). La estructura del modelo es una red de entradas (input), capas ocultas (hidden layers) y salidas (output), como se muestra en la siguiente imagen:\n\n\n\n\n\nComo apreciamos en la figura, una red puede tener una o muchas capas ocultas\n\n\n\n\n\nLa intuición original detrás del aprendizaje profundo era crear modelos inspirados en el cerebro humano, sobre todo por cómo aprende el cerebro humano: a través de células interconectadas llamadas neuronas. Es por esto que llamamos a los modelos de aprendizaje profundo Redes Neuronales.\n\n\n\n\n\nEstas estructuras de modelos en capas requieren muchos más datos en comparación con otros modelos de aprendizaje automático para derivar patrones. Generalmente hablamos de al menos cientos de miles de puntos de datos."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#pytorch-un-framework-del-deep-learning",
    "href": "posts/DL_PyTorch/index.html#pytorch-un-framework-del-deep-learning",
    "title": "Deep Learning",
    "section": "PyTorch: un framework del deep learning",
    "text": "PyTorch: un framework del deep learning\n\n\n\n\n\nSi bien existen varios framework y paquetes para implementar el aprendizaje profundo en cuanto a algoritmos, nos centraremos en PyTorch, uno de los frameworks más populares y mejor mantenidos. PyTorch fue desarrollado originalmente por Meta IA como parte del laboratorio de investigación de inteligencia artificial de Facebook antes de que pasara a depender de la fundación Linux.\nEstá diseñado para ser intuitivo y fácil de usar, compartiendo muchas similitudes con la biblioteca de Python NumPy.\n\nPyTorch Tensors\nPodemos importar el módulo PyTorch llamando a\n\nimport torch\n\n\nLa estructura de datos fundamental en PyTorch es un tensor, que es similar a una matriz.\nPuede soportar muchas operaciones matemáticas y constituye un componente básico para nuestras redes neuronales.\nSe pueden crear tensores a partir de listas de Python o matrices NumPy utilizando la clase torch.tensor() esta clase convierte los datos a un formato compatible para el aprendizaje profundo.\n\n\nmi_lista = [[1,2,3], [4,5,6]]\ntensor = torch.tensor(mi_lista)\nprint(tensor)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\n\nAtributos de los Tensores\nPodemos llamar a tensor.shape para mostrar la forma de nuestro objeto recién creado.\n\nprint(tensor.shape)\n\ntorch.Size([2, 3])\n\n\nY tensor.dtype() para mostrar su tipo de datos, aquí un entero de 64 bits.\n\nprint(tensor.dtype)\n\ntorch.int64\n\n\nVerificar la forma y el tipo de datos garantiza que los tensores se alineen correctamente con nuestro modelo y tarea, y puede ayudarnos en caso de depuración.\n\nOperaciones con Tensores\nSe pueden sumar o restar tensores de PyTorch, siempre que sus formas sean compatibles.\n\na = torch.tensor([[1,1], [2,2]])\nb = torch.tensor([[2,2],[3,3]])\nc = torch.tensor([[2,2,2], [3,3,5]])\n\n\nprint(a + b)\n\ntensor([[3, 3],\n        [5, 5]])\n\n\nCuando las dimensiones no son compatibles, obtendremos un error.\nTambién podemos realizar la multiplicación por elemento, lo que implica multiplicar cada elemento correspondiente.\n\nprint(a*b)\n\ntensor([[2, 2],\n        [6, 6]])\n\n\nTambién esta incluida la multiplicación de matrices, que no es más que uno forma de combinar dos matrices para crear una nueva.\n\nprint(a @ b)\n\ntensor([[ 5,  5],\n        [10, 10]])\n\n\nDetras de escena, los modelos de aprendizaje profundo realizan innumerables operaciones como la suma y multiplicación para procesar datos y aprender patrones."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#pesos-weights-y-sesgos-biases",
    "href": "posts/DL_PyTorch/index.html#pesos-weights-y-sesgos-biases",
    "title": "Deep Learning",
    "section": "Pesos (weights) y Sesgos (biases)",
    "text": "Pesos (weights) y Sesgos (biases)\nCada capa lineal tiene un conjunto de pesos y sesgos asociados. Estas son las cantidades clave que definen una neurona.\n\nprint(linear_layer.weight)\n\nParameter containing:\ntensor([[ 0.3821, -0.4537,  0.3866],\n        [-0.2932,  0.2641, -0.0264]], requires_grad=True)\n\n\n\nprint(linear_layer.bias)\n\nParameter containing:\ntensor([0.4748, 0.5690], requires_grad=True)\n\n\n\nLos pesos reflejan la importancia de diferentes características.\nEl sesgos es un término adicional que es independiente de los pesos, y proporciona a las neurona una salida de referencia.\n\nAl principio, la capa lineal asigna pesos y sesgos aleatorios; estos se ajustan posteriormente.\nImaginemos nuestra red totalmente conectada en acción.\nDigamos que tenemos un conjunto de datos meteorológicos con tres características: temperatura (temperature), humedad (humidity) y viento (wind). Y queremos predecir si lloverá (rain) o estará nublado (cloudy).\n\nLa característica humeda tendrá un peso más significativo en comparación a las demás características, ya que es un fuerte predictor de lluvia y nubes.\nLos datos meteorológicos corresponden a una región tropical con alta probabilidad de lluvia, por lo que agrega un sesgo para tener en cuenta esta información de referencia.\n\nCon esta información, nuestro modelo hace una predicción."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#capaz-y-parámetros-ocultos",
    "href": "posts/DL_PyTorch/index.html#capaz-y-parámetros-ocultos",
    "title": "Deep Learning",
    "section": "Capaz y Parámetros Ocultos",
    "text": "Capaz y Parámetros Ocultos\nHasta ahora, hemos utilizado una capa de entrada y una capa de lineal. Ahora, agregaremos más capas para ayudar a la red a aprender patrones complejos.\n\nApilamiento de capaz con nn.Sequential()\nApilaremos tres capas lineales usando nn.Sequential(), un contenedor de PyTorch para apilar capas en secuencia. Esta red toma la entrada, la pasa a cada capa lineal en secuencia y devuelve la salida.\nmodel = nn.Sequential(\n  nn.Linear(n_features, 8),\n  nn.Linear(8, 4),\n  nn.Linear(4, n_classes)\n)\n\nEn este caso, las capas dentro de nn.Sequential() son capas ocultas.\nn_features representa el número de características de entrada y n_classes representa el número de clases de salida, ambas definidas por el conjunto de datos.\n\n\n\nAdición de capas\nPodemos añadir tantas capas ocultas como queramos.\n\nLa dimensión de cada capa coincide con la dimensión de salida de la anterior.\n\nmodel = nn.Sequential(\n  nn.Linear(10, 18),\n  nn.Linear(18, 20),\n  nn.Linear(20, 5)\n)\n\nEn nuestro ejemplo de tres capas, la primera capa toma 10 características y genera 18. La segunda toda 18 y genera 20. Finalmente, la tercera toma 20 y genera 5.\n\n\nLas capas están hechas de neuronas\n\n\n\n\n\nUna capa está completamente conectada cuando cada neurona se vincula a todas las neuronas de la capa anterior, como se muestra en rojo en la figura.\nCada neurona es una capa lineal:\n\nrealiza una operación lineal utilizando todas las neuonras de la capa anterior.\nPor tanto, una sola neurona tiene \\(N+1\\) parámetros que se puede aprender, siendo la dimensión de salida la capa anterior, más 1 para el sesgo.\n\n\n\nParámetros y Capacidad del Modelo\nAumetar el número de capas ocultas aumenta el número total de parámetros en el modelo, también conocido como capacidad del modelo. Los modelos de mayor capacidad pueden manejar conjuntos de datos más complejos, pero su entrenamiento puede llevar más tiempo.\nUna forma eficaz de evaluar la capacidad de un modelo es calcular su número total de parámetros.\nVamos a desglosarlo con una red de dos capas,\n\nmodel = nn.Sequential(\n  nn.Linear(8, 4),\n  nn.Linear(4, 2)\n)\n\n\nLa primera capa tiene 4 neuronas, cada neurona tiene 8 pesos y un sesgo, lo que da como resultado 36 parámetros.\nLa segunda capa tiene 2 neuronas, cada neurona tiene 4 pesos y un sesgo, para un total de 10 parámetros.\nSumándolos todos, este modelo tiene 46 parámetros que se pueden aprender en total\n\nTambién podemos calcular esto en PyTorch usando el método .numel(). Este método devuelve el número de elementos de un tensor.\n\ntotal = 0\nfor parameter in model.parameters():\n  total += parameter.numel()\n  \nprint(total)\n\n46\n\n\n\n\nBalance entre complejidad y eficiencia\n\n\n\n\n\nComprender el recuento de parámetros nos ayuda a equilibrar la complejidad y la eficiencia del modelo. Demasiados parámetros pueden dar lugar a tiempos de entrenamiento largos o sobreajuste, mientras que muy pocos pueden limitar la capacidad de aprendizaje."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#redes-neuronales-y-capas",
    "href": "posts/DL_PyTorch/index.html#redes-neuronales-y-capas",
    "title": "Deep Learning",
    "section": "Redes Neuronales y Capas",
    "text": "Redes Neuronales y Capas\nVamos a contruir nuestra primer red neuronal usando tensores de PyTorch.\nUna red neuronal consta de capas de entrada, ocultas y de salida.\n\n\n\n\n\nLa capa de entrada contiene las características del conjunto de datos,\n\n\n\n\n\nLa capa de salida contiene las predicciones,\n\n\n\n\n\nY hay capas ocultas (hidden layers) en el medio\n\n\n\n\n\nSi bien una red puede tener cualquier cantidad de capas ocultas, comenzaremos construyendo una red sin capas ocultas donde la capa de salida es una capa lineal.\n\n\n\n\n\n\nAquí, cada neurona de entrada se conecta a cada neurona de salida, lo que se denomina una red “totalmente conectada”.\nEsta red es equivalente a un modelo lineal y nos ayuda a comprender los fundamentos antes de agregar complejidad.\n\nUsaremos el módulo torch.nn para construir nuestras redes. Esto hace que el código de la red sea más conciso y flexible y se importa convencionalmente como nn.\n\nimport torch.nn as nn\n\nAl diseñar una red neuronal, las dimensiones de las capas de entrada y salida están predefinidas.\n\nLa cantidad de neuronas en la capa de entrada es la cantidad de características en nuestro conjunto de datos.\nY el número de neuronas en la capa de salida es el número de clases que queremos predecir.\n\nDigamos que creamos un input_tensor con forma de \\(1\\times 3\\).\n\nimport torch\nimport torch.nn as nn\ninput_tensor = torch.tensor(\n  [[0.3471, 0.4547, -0.2356]]\n)\n\nPodemos pensar en esto como una fila con tres “carectísticas” o “neuronas” .\nA continuación, pasamos este input_tensor a una capa lineal, que aplica una función lineal para realizar predicciones.\n\n\n\n\n\nPara ello usaremos nn.Linear() toma dos argumentos: int_features es el número de características en nuestra entrada ( en este caso, tres) y out_features es el tamaño del tensor de salida (en este caso, dos).\n\n\n\n\n\n\nlinear_layer = nn.Linear(\n  in_features = 3,\n  out_features = 2\n)\n\nEspecificar correctamente in_features garantiza que nuestra capa lineal pueda recibir el input_tensor.\nPor último, pasamos input_tensor a linear_layer para generar una salida.\n\noutput = linear_layer(input_tensor)\nprint(output)\n\ntensor([[0.3101, 0.5936]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nTenga en cuenta que esta salida tiene dos características o neuronas debido a las out_features especificadas en nuestra capa lineal.\nCuando input_tensor se pasa a linear_layer, se realiza una operación lineal para incluir pesos y sesgos."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#funciones-de-activación",
    "href": "posts/DL_PyTorch/index.html#funciones-de-activación",
    "title": "Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\nHasta ahora hemos visto redes neuronales formadas únicamente por capas lineales.\n\n\n\n\n\nPodemos agregar no linealidad a nuestros modelos usando funciones de activación. Discutiremos dos funciones de activación:\n\nSigmoid para clasificación binaria y,\nSoftmax para clasificación multiclase.\n\nEsta no linealidad permite que las redes aprendan cosas más complejas, interacciones entre entradas y objetivos que son relaciones no linealeales.\nLlamaremos a la salida de la última capa lineal la “pre-activación”. Salida, que pasaremos a funciones de activación para obtener la salida transformada.\n\nFunción Sigmoid\nLa función de activación sigmoidea se utiliza ampliamente para problemas de clasificación binaria. Digamos que estamos tratando de clasificar un animal como mamífero o no?. Tenemos tres datos: el número de extremidades, si pone huevos y si tiene pelo. Las dos últimas son variables binarias: 1 si es si, 0 si no.\n\nPasar la entrada a un modelo con dos capas lineales devuelve una única salida: el número 6, tal como apreciamos en la siguiente figura:\n\n\n\n\n\nEste número aún no es interpretable. Tenemos que pasar el número 6 por la función sigmoide, transformandolo en un rango que represente la probabilidad entre cero y uno.\n\n\n\n\n\nSi el resultado está más cerca de uno (mayor que 0.5), lo etiquetamos como clase uno (mamífero). Si fuese menor que 0.5 la predección sería cero (no un mamifero).\n\n\n\n\n\nAhora, implementemos sigmoide en PyTorch.\n\nimport torch\nimport torch.nn as nn\n\ninput_tensor = torch.tensor([[6]])\nsigmoid = nn.Sigmoid()\noutput = sigmoid(input_tensor)\nprint(output)\n\ntensor([[0.9975]])\n\n\nNormalmente, nn.Sigmoid() se agrega como el último paso en nn.Sequential(), transformando automáticamente la salida de la capa lineal final.\n\nmodel = nn.Sequential(\n  nn.Linear(6, 4), # Primera capa lineal \n  nn.Linear(4, 1), # Segunda capa lineal\n  nn.Sigmoid()     # Función de activación\n)\n\nCuriosamente, una red neuronal con solo capas lineales y una activación sigmoidea se comporta como una Regresión Logística. Más adelante agregaremos más capas y activaciones para comprender realmente el verdadero potencial del Deep Learning.\n\n\nFunción Softmax\nUsamos softmax, otra función de activación popular, para clasificación multiclase que implica más de dos etiquetas de clase.\nDigamos que tenemos tres clases:\n\nPajaro o Bird (0)\nMamífero o Mammal (1)\nReptil o Reptile (2)\n\n\n\n\n\n\nEn esta red, Softmax toma una dimensión tridimensional, salida de preactivación y genera una salida de la misma forma, una por tres.\nLa salida es una distribución de probabilidad:\n\nPor cada elemento está entre cero y uno, y\nlos valores suman uno.\n\n\n\n\n\n\nAquí, la predicción es para la segunda clase, mamíferos, que tiene la probabilidad más alta 0.842.\n\n\n\n\n\nEn PyTorch, usamos nn.Softmax()\n\nimport torch\nimport torch.nn as nn\n\ninput_tensor = torch.tensor(\n  [[4.3, 6.1, 2.3]]\n)\n\nprobabilities = nn.Softmax(dim=-1)\n# dim = -1 indica que softmax se aplica a la última dimensión de input_tensor\noutput_tensor = probabilities(input_tensor)\nprint(output_tensor)\n\ntensor([[0.1392, 0.8420, 0.0188]])\n\n\nSimilar a sigmoide, softmax puede ser la última capa en nn.Sequential."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#paso-hacia-adelante",
    "href": "posts/DL_PyTorch/index.html#paso-hacia-adelante",
    "title": "Deep Learning",
    "section": "Paso hacia adelante",
    "text": "Paso hacia adelante\nHemos explorado tensores, redes pequeñas y funciones de activación. Ahora profundicemos en la generación de predicciones.\nEste proceso se llama “ejecutar un paso hacia adelante” a través de una red.\n\nQué es una paso hacia adelante (Forward Pass)?\nEs cuando los datos de entrada fluyen a través de una red neuronal en dirección hacia adelante, para producir resultados o predicciones, pasa a través de cada capa de red.\n\n\n\n\n\nLos calculos transforman los datos en nuevas representaciones en cada capa, que pasa a la siguiente capa hasta que se produce el resultado final.\nEl propósito del paso hacia adelante es pasar datos de entrada a través de la red y producir predicciones o resultados basados en los parámetros aprendidos del modelo, también conocidos como pesos y sesgos.\nEste proceso es esencial tanto para el entrenamiento como para realizar nuevas predicciones.\nEl resultado final puede ser clasificaciones binarias, clasificaciones multiclase o predicciones numéricas (regresiones).\n\n\n\n\n\nVeremos un ejemplo de cada uno.\nDigamos que tenemos datos de entrada de cinco animales, con seís características o neuronas por punto de datos.\n\ninput_data = torch.tensor(\n  [[-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n  [-0.9155, -0.0475, -1.3645, -0.6336, -1.9520, -0.3398],\n  [0.7406, 1.6763, -0.8511, 0.2432, 0.1123, -0.0633],\n  [-1.6630, -0.0718, -0.1285, 0.5396, -0.0288, -0.8622],\n  [-0.7413, 1.7920, -0.0883, -0.6685, 0.4745, -0.4245]]\n)\n\nCreamos una pequeña red con dos capas lineales y una función de activación sigmoidea en secuencia.\n\nimport torch\nimport torch.nn as nn\nmodel = nn.Sequential(\n  nn.Linear(6, 4),\n  nn.Linear(4, 1),\n  nn.Sigmoid()\n)\n\n\nLa primera capa toma seis características como entrada, genera cuatro.\nLa segunda capa procesa esto para obtener una probailidad final.\n\nEl resultado de nuestra clasificación binaria es una única probabilidad entre cero y uno para cada uno de nuestros cinco animales.\n\noutput = model(input_data)\nprint(output)\n\ntensor([[0.5332],\n        [0.4021],\n        [0.4487],\n        [0.4753],\n        [0.4996]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nRecuerde que comúnmente utilizamos un umbral de 0.5 para convertirlos en etiquetaas de 0 y 1, es decir:\n\nClass = 1 para \\(output \\geq 0.5\\)\nClass = 0 para \\(output \\leq 0.5\\)\n\nEsta salida no será significativa hasta que usemos retropropagación para actualizar los pesos y sesgos de las capas. Hablaremos más sobre esto más adelante.\n\n\nClasificación Multi-Class: Forward Pass\nEl modelo sería similar si quisiéramos ejecutar una clasificación de múltiples clases.\nDigamos que estamos prediciendo tres clases: mamíferos (Class 1), aves (Class 2) o reptiles (Class 3).\nEspecíficamos que nuestro modelo tiene tres clases, estableciendo este valor como la dimensión de salida de la última capa lineal.\n\nn_classes = 3\nmodel = nn.Sequential(\n  nn.Linear(6, 4),\n  nn.Linear(4, n_classes),\n  nn.Softmax(dim=-1)\n)\n\nUsamos Softmax en lugar de Sigmoid, con \\(dim = -1\\) para indicar los 5 animales. Los anímales tiene la misma última dimensión que la salida de la última capa lineal.\n\noutput = model(input_data)\nprint(output.shape)\n\ntorch.Size([5, 3])\n\n\nUtilizando la misma entrada que antes, la forma de salida es \\(5\\times 3\\).\n\nprint(output)\n\ntensor([[0.1850, 0.4169, 0.3981],\n        [0.3528, 0.5321, 0.1151],\n        [0.2644, 0.4431, 0.2925],\n        [0.2599, 0.4821, 0.2580],\n        [0.2463, 0.4823, 0.2714]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\nNote que cuando imprimimos la salida, cada fila representa las probabilidades de tres clases, que suman uno. La etiqueta prevista para cada fila se asigna a la clase con la mayor probabilidad.\nEn nuestro ejemplo, todas las filas son mamíferos.\n\n\nRegresión: Forward Pass\nEl último modelo que analizaremos es la regresión: predecir valores numéricos continuos.\nAhora usaremos las mismos datos para predecir el peso de los animales en función de sus propiedades.\n\nmodel = nn.Sequential(\n  nn.Linear(6, 4), \n  nn.Linear(4, 1)\n)\n\noutput = model(input_data)\nprint(output)\n\ntensor([[ 0.6664],\n        [-0.1888],\n        [-0.5721],\n        [ 0.3729],\n        [ 0.0682]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nEsta vez no hay función de activación al final, y la última dimensión de la última capa lineal devuelve una salida con una característica.\nLas dimensiones de salida son \\(5\\times 1\\): cinco valores continuos, uno para cada fila."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#funciones-de-pérdida-para-evaluar-las-predicciones-del-modelo",
    "href": "posts/DL_PyTorch/index.html#funciones-de-pérdida-para-evaluar-las-predicciones-del-modelo",
    "title": "Deep Learning",
    "section": "Funciones de Pérdida para Evaluar las Predicciones del Modelo",
    "text": "Funciones de Pérdida para Evaluar las Predicciones del Modelo\nHemos generado predicciones ejecutando un paso hacia adelante, el siguiente paso es ver qué tan buenas son nuestras predicciones en comparación con los valores reales.\n\nFunción de pérdida\nLa función de pérdida, otro componente de las redes neuronales, nos dicen qué tan bueno es nuestro modelo para hacer predicciones durante el entrenamiento.\nToma una predicción del modelo \\((\\hat{y})\\) y una etiqueta verdadera \\(y\\), o dato real, y genera un dato flotante, tal como se puede apreciar en el siguiente esquema\n\n\n\n\n\nUtilicemos nuestra multiclase\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHair\nFeathers\nEggs\nMilk\nFins\nLegs\nTail\nDomestic\nCatsize\nClass\n\n\n\n\n1\n0\n0\n1\n0\n4\n0\n0\n1\n0\n\n\n\nmodelo de clasificación que predice si un animal es un mamífero (0), ave (1) o reptil (2).\n\nSi nuestro modelo predice que la clase es igual a cero, es correcto y el valor de la pérdida será bajo.\nUna predicción incorrecta haría que el valor de la pérdida fuera alto.\nNuestro objetivo es minimizar las pérdidas.\n\n\n\nCalculo de la pérdida\nLa pérdida se calcula utilizando una función de pérdida, \\(F\\), que toma el dato real y el predicho, es decir,\n\\[\nLoss = F(y, \\hat{y})\n\\]\n\nEn nuestro ejemplo de los animales, los valores posibles para nuestra verdadera clase de \\(y\\) son los números enteros 0, 1 o 2, es decir, \\(y \\in \\{0, 1 , 2\\}\\).\n\\(\\hat{y}\\) es un tensor con las mismas dimensiones que el número de clases \\(N\\), es decir, \\(\\hat{y}\\in \\{-5.2, 4.6, 0.8\\}\\). Si \\(N=3\\) entonces la salidad softmax es un tensor de forma \\(1\\times 3\\).\n\n\n\nCodificación one-hot\nUsamos codificación one-hot para convertir un entero \\(y\\) en un tensor de ceros y unos para que podamos comparar para evaluar el rendimiento del modelo.\n\n\n\nFigura 1\n\n\nPor ejemplo, si \\(y=0\\) con tres clases, la forma codificada es 1, 0, 0 como se aprecia en Figura 1.\nPodemos importar torch.nn.functional como F para evitar la codificación one-hot manual.\n\nimport torch.nn.functional as F\n\nprint(F.one_hot(torch.tensor(0), num_classes = 3))\n\ntensor([1, 0, 0])\n\n\nEn el primer ejemplo, la verdad fundamental es cero (la primera clase). Tenemos 3 clases, por lo que la función genera un tensor de tres elementos con uno en la primera posición y ceros en el resto.\n\nprint(F.one_hot(torch.tensor(1), num_classes = 3))\n\ntensor([0, 1, 0])\n\n\nNotemos ahora que si \\(y=1\\) (la segunda clase), el tensor de salida tiene un uno en la segunda posición y ceros en caso contrario.\n\nprint(F.one_hot(torch.tensor(2), num_classes = 3))\n\ntensor([0, 0, 1])\n\n\nPor último, si \\(y=2\\) (tercera clase), el tensor de salida tiene un uno en la última posición y ceros en el resto de los casos.\n\n\nFunción de Pérdida Cross Entropy en PyTorch\nUna vez completada la codificación, podemos pasarla junto con nuestras predicciones a una función de pérdida. Lo que almacenaríamos será el tensor de “puntuaciones”.\n\nfrom torch.nn import CrossEntropyLoss\n\nscores = torch.tensor([-5.2, 4.6, 0.8])\none_hot_target = torch.tensor([1,0,0])\n\nLa función de pérdida más comunmente utilizada para la clasificaci´øn es la pérdida de entropía cruzada.\nComencemos definiendo nuestra función de pérdida como “criterio”. Luego le pasamos el método .double() del tensor de puntuaciones y del tensor one_hot_target.\n\ncriterion = CrossEntropyLoss()\nprint(criterion(scores.double(), one_hot_target.double()))\n\ntensor(9.8222, dtype=torch.float64)\n\n\nEsto garantiza que los tensores estén en el formato correcto para la función de pérdida. La salida es el valor de pérdida calculado.\nEn resumen, la función de pérdida toma como entrada el tensor de puntuaciones, que es el modelo, predice antes de la función softmax final y la etiqueta de verdad codificada one-hot. Emite un único flotante, la pérdida de esa muestra.\n\n\n\n\n\nRecordemos que nuestro objetivo es minimizar esa pérdida."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#utilizar-derivadas-para-actualizar-los-parámetros-del-modelo",
    "href": "posts/DL_PyTorch/index.html#utilizar-derivadas-para-actualizar-los-parámetros-del-modelo",
    "title": "Deep Learning",
    "section": "Utilizar derivadas para Actualizar los Parámetros del Modelo",
    "text": "Utilizar derivadas para Actualizar los Parámetros del Modelo\nVeamos ahora cómo podemos minímizar la pérdida. Sabemos que un modelo predice mal cuando la pérdida es alta. Podemos utilizar derivadas o gradientes para minimizar esta pérdida.\n\n\n\n\n\nImaginemos la función de pérdida como un valle. La derivada representa la pendiente, es decir qué tan pronunciada sube o baja la curva.\n\nLas pendientes pronunciadas, mostradas con flechas rojas, indican derivadas altas y pesos grandes.\nLas pendientes más suaves, representadas por flechas verdes, tienen derivadas pequeñas y pesos más pequeños.\nEn el fondo del valle, mostrado por la flecha azul, la pendiente es plana y la derivada es cero. Este punto es el mínimo de la función de pérdida que pretendemos alcanzar.\n\n\nFunciones Convexas y No-Convexas\nLas funciones convexas tienen un mínimo global.\n\n\n\n\n\nLas funciones no convexas, como las funciones de pérdida, tienen múltiples mínimos locales, donde el valor es inferior al de los puntos cercanos pero no el más bajo en general.\nAl minimizar las funciones de pérdida, nuestro objetivo es localizar el mínimo global cuando \\(x\\) es aproximadamente uno.\n\n\nConexión de derivadas y entrenamiento de modelos\nDurante el entrenamiento, ejecutamos un paso hacia adelante en las características y calculamos la pérdida comparando las predicciones con el valor objetivo.\n\n\n\n\n\nRecuerde que los pesos y sesgos de las capas se inicializan aleatoriamente cuando se crea un modelo. Los actualizamos durante el entrenamiento mediante un paso hacia atrás o retropropogación.\nEn el Deep Learning, las derivadas se conocen como gradientes.\n\n\n\n\n\nCalculamos los gradientes de la función de pérdida y los usamos para actualizar los parámetros del modelo. Incluyendo pesos y sesgos, con retropropagación, repitiendo hasta que las capas esten sintonizadas.\n\n\nBackpropagation (Retropropagación)\nDurante la retropropagación, si consideramos una red de tres capas lineales:\n\npodemos calcular gradientes de pérdida locales con respecto a \\(L_2\\)\nUsamos \\(L_2\\) para calcular el gradiente \\(L_1\\)\nY repetimos hasta llegar a la capa inicial \\(L_0\\).\n\n\n\n\n\n\nVeamos esto con PyTorch:\nmodel = nn.Sequential(nn.Linear(16, 8),\nnn.Linear(8, 4),\nnn.Linear(4, 2))\nprediction = model(sample)\n\ncriterion = CrossEntropyLoss()\nloss = criterion(prediction, target)\nloss.backward\nDespués de ejecutar un paso hacia adelante, definimos una función de pérdida, aquí CrossEntropyLoss() y úselo para comparar predicciones con valores objetivo.\nUsando .backward(), calculamos gradientes basados en esta pérdida, que se almacenan en los atributos .grad de los pesos y .bias de los sesgos de cada capa.\nmodel[0].weight.grad\nmodel[0].bias.grad\nmodel[1].weight.grad\nmodel[1].bias.grad\nmodel[2].weight.grad\nmodel[2].bias.grad\nCada capa del modelo se puede indexar comenzando desde cero para acceder a sus pesos, sesgos y gradientes.\n\n\nActualizar Manualmente los Parámetros del Modelo\nPara actualizar manualmente los parámetros del modelo, accedemos al gradiente de cada capa.\n# Tasa de aprendizaje tipicamente pequeña\nlr = 0.001\n\n# updater the pesos\nweight = model[0].weight\nweight_grad = model[0].weight.grad\n\n# update de sesgos \nbias = model[0].bias\nbias_grad = model[0].bias.grad \nluego multiplicamos por la tasa de aprendizaje y restamos este producto del peso.\nbias = bias - lr*bias_grad \nLa tasa de aprendizaje es otro parámetros ajustable. Discutiremos esto y el ciclo de entranamiento más adelante en este documento.\n\n\nGradiente Descendente\n\nUtilizamos un mecanismo llamado “gradiente desendiente” para encontrar el mínimo global de las funciones de pérdida.\nPyTorch simplifica esto con optimizadores, como el descenso de gradiente estocástico (SGD).\n\n\nimport torch.optim as optim\n\n# Creamos el optimizador\noptimizer = optim.SGD(model.parameters(), lr = 0.001)\n\n\nUsamos optim para instanciar SGD.\n.parameters() devuelve un iterable de todos los parámetros del modelo, que pasamos al optimizador.\nAquí utilizamos una tasa de aprendizaje estándar, “lr”.\n\nEl optimizador calcula automáticamente los gradientes y actualiza los parámetros del modelo con .step()\n\noptimizer.step()"
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#escribir-nuestro-primer-bucle-de-entrenamiento",
    "href": "posts/DL_PyTorch/index.html#escribir-nuestro-primer-bucle-de-entrenamiento",
    "title": "Deep Learning",
    "section": "Escribir nuestro primer bucle de entrenamiento",
    "text": "Escribir nuestro primer bucle de entrenamiento\nAhora que ya tenemos los componentes principales para entrenar un modelo de aprendizaje profundo con PyTorch.\n\nEntrenando una Red Neuronal\nUna vez que creamos un modelo, elegimos una función de pérdida, definimos un conjunto de datos y configuramos un optimizador, estamos listos para entrenar. Esto implica recorrer el conjunto de datos, calcular la pérdida, calcular gradientes y actualizar los parámetros del modelo. Este proceso, es llamado bucle de entrenamiento, se repite varias veces.\nUn bucle de entrenamiento permite una mayor flexibilidad y control, dándonos la opción de personalizar diferentes elementos.\nTrabajaremos con un conjunto de datos de salarios de científicos de datos para ver esto en acción.\n\n\n\n\n\n\n\n\n\n\nexperience_level\nemployment_type\nremote_ratio\ncompany_size\nsalary_in_usd\n\n\n\n\n0\n0\n0.5\n1\n0.036\n\n\n1\n0\n1.0\n2\n0.133\n\n\n2\n0\n0.0\n1\n0.234\n\n\n1\n0\n1.0\n0\n0.076\n\n\n2\n0\n1.0\n1\n0.170\n\n\n\n\nLas características son categóricas y el objetivo es el salario en dólares (salary_in_usd), ya normalizado. Dado que el objetivo es un valor continuo, este es un problema de regresión.\nPara la regresión, utilizaremos una capa lineal como salida final en lugar de softmax o sigomoide.\nAdemás, aplicaremos una función de pérdida específica de regresión, ya que la entropía cruzada solo se utiliza para tareas de clasificación.\n\n\nMean Squared Error Loss\nPodemos utilizar la pérdida de error cuadrático medio (MSE) para problemas de regresión. La pérdida de MSE es la media de la diferencia al cuadrado entre predicciones y el dato real o verdad fundamental, como se muestra en esta implementación en Python:\n\ndef mean_squared_loss(prediction, target):\n  return np.mean((prediction - target)**2)\n\nEn PyTorch, utilizamos la función nn.MSELoss como criterio.\ncriterion = nn.MSELoss()\nloss = criterion(prediction, target)\nTenga en cuenta que tanto las predicciones como los objetivos deben ser tensores flotantes.\nPongamos todo junto ahora, tenemos dos matrices NumPy, “características” y “objetivo”, que tienen nuestros datos y etiquetas.\n\ndata = pd.read_csv(\"salary_datascince.csv\", sep = \";\")\ndata\n\n\n\n\n\n\n\n\nexperience_level\nemployment_type\nremote_ratio\ncompany_size\nsalary_in_usd\n\n\n\n\n0\n0\n0\n0.5\n1\n0.036\n\n\n1\n1\n0\n1.0\n2\n0.133\n\n\n2\n2\n0\n0.0\n1\n0.234\n\n\n3\n1\n0\n1.0\n0\n0.076\n\n\n4\n2\n0\n1.0\n1\n0.170\n\n\n\n\n\n\n\n\nfeatures = data.iloc[:, :-1]\nX = features.to_numpy()\nprint(X)\n\n[[0.  0.  0.5 1. ]\n [1.  0.  1.  2. ]\n [2.  0.  0.  1. ]\n [1.  0.  1.  0. ]\n [2.  0.  1.  1. ]]\n\n\n\ntarget = data.iloc[:, -1]\ny = target.to_numpy()\nprint(y)\n\n[0.036 0.133 0.234 0.076 0.17 ]\n\n\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndataset = TensorDataset(torch.tensor(X),\ntorch.tensor(y))\n\ndataloader = DataLoader(dataset, batch_size = 4, shuffle = True)\n\nAhora podemos cargar nuestro conjunto de datos en la clase DataLoader() para habilitar el procesamiento por lotes.\n\nAquí utilizamos un tamaño de lote pequeño de cuatro, pero la selección del tamaño de lote es personalizable dependiendo del caso de uso.\n\nA continuación creamos nuestro modelo, este conjunto de datos tiene cuatro características de entrada y un destino (salida).\n\nmodel = nn.Sequential(nn.Linear(4, 2), \nnn.Linear(2, 1))\n\nNo necesitaremos codificación one-hot ya que se trata de un problema de regresión.\nFinalmente, creamos el criterio de pérdida MSE y el optimizador, con una taza de aprendizaje predeterminada para la mayoría de los problemas de deep learning.\n\ncriterio = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr = 0.001)\n\nAhora podemos recorrer el conjunto de datos varias veces:\n\nRecorrer todo el conjunto de datos una vez se denomina época y entrenamos en múltiples época, indicadas por num_epochs. Para cada época, recorremos el cargador de datos. Cada iteración del cargador de datos proporciona un lote de muestras, que vimos anteriormente. Antes del pase hacia adelante , establecemos los gradientes a cero usando optimizer.zero_grad(), porque el optimizador almacena gradientes de pasos anteriores de manera predeterminada, obtenemos características y objetivos de cada muestra del cargador de datos. Utilizamos las características para el paso hacia adelante del modelo y utilizamos el objetivo para el cálculo de la pérdida y finalmente, utilizamos el optimizador para actualizar los parámetros del modelo.\n\nfor epoch in range(num_epochs):\n  for data in dataloader: \n    optimizer.zero_grad()\n    feature, target = data \n    pred = model(feature)\n    loss = criterion(pred, target)\n    loss.backward()\n    optimizer.step()"
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#funciones-de-activación-relu",
    "href": "posts/DL_PyTorch/index.html#funciones-de-activación-relu",
    "title": "Deep Learning",
    "section": "Funciones de Activación ReLU",
    "text": "Funciones de Activación ReLU\nHemos visto como las funciones de activación introducen no linealidad para ayudar a las redes neuronales a aprender patrones complejos y hemos aprendido sobre los gradientes y su papel dentro del ciclo de entrenamiento.\nA veces, las funciones de activación pueden reducir demasiado los gradientes, lo que hace que el entrenamiento sea ineficiente.\nHasta ahora hemos trabajado con dos funciones de activación: sigmoidea y softmax, que normalmente se utilizan en la capa final de un modelo.\n\n\n\n\n\n\nLimitaciones de Sigmoid y Softmax\nComenzaremos por comprender algunas de las limitaciones de la función sigmoidea.\n\n\n\n\n\n\nLas salidas de sigmoid están limitadas entre 0 y 1, lo que significa que para cualquier entrada, la salida siempre estará dentro de este rango.\nSigmoid podría usarse en cualquier punto de una red. Sin embargo, los gradientes de la sigmoide, que se muestran en naranja, son muy pequeños para valores grandes y pequeños de \\(x\\). Este fenómeno se llama saturación. Durante la retropropagación, esto se vuelve problemático porque cada gradiente depende del anterior. Cuando los gradientes son extremadamente pequeños, no logran actualizar los pesos de manera efectiva.\nEste problema se conoce como el problema de los gradientes avanescentes y puede dificultar mucho el entrenamiento de redes profundas.\n\nLa función softmax, que también produce salidas acotadas entre 0 y 1, sufre saturación de manera similar.\nPor tanto, ambas funciones de activación no son ideales para capas ocultas y es mejor utilizarlas solo en la última capa.\n\n\nReLU\nDescubriremos dos funciones de activación ampliamente utilizadas, diseñadas para su uso entre capas lineales o en capas ocultas.\n\n\n\nFunción ReLU\n\n\n\n\\(f(x) = \\max(x,0)\\) aquí está la unidad lineal rectificada o ReLU. ReLU genera el valor máximo entre su entrada y cero. Como se muestra en el gráfico.\nPara entradas positivas, la salida es igual a la entrada.\nPara entradas negativas, la salida es cero.\n\nEsta función no tiene límite superior y sus gradientes no se aproximan a cero. Para valores grandes de \\(x\\), lo que ayuda a superar el problema de los gradientes que desaparecen.\nEn PyTorch, ReLU se puede utilizar a través del módulo torch.nn\n\nrelu = nn.ReLU()\n\n\n\nLeaky ReLU (ReLU con fugas)\nLa ReLU con fugas es una variación de la función ReLU. Para entradas positivas, se comporta exactamente como ReLU.\n\n\n\nLeaky ReLU\n\n\n\nPara entradas positivas, se comporta exactamente como ReLU.\nPara entradas negativas, multiplica la entrada por un coeficiente pequeño (predeterminado a 0.01 en PyTorch).\n\nEsto garantiza que los gradientes para entradas negativas permanezcan distintos de cero, lo que evita que las neuronas dejen de aprender por completo, lo que puede suceder con ReLU estándar.\nEn PyTorch, la función ReLU con fugas se implementa utilizando el módulo torch.nn.\n\nleaky_relu = nn.LeakyReLU(\n  negative_slope = 0.05\n)\n\nEl parámetro negative_slope controla el coeficiente aplicado a las entradas negativas."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#tasa-de-aprendizaje",
    "href": "posts/DL_PyTorch/index.html#tasa-de-aprendizaje",
    "title": "Deep Learning",
    "section": "Tasa de Aprendizaje",
    "text": "Tasa de Aprendizaje\nHemos hablado anteriormente sobre la tasa de aprendizaje, pero llego el momento de que profundicemos más.\n\nActualización de Pesos con SGD\n\nEntrenar una red neuronal significa = resolver una optimización. El problema planteado es minimizar la función de pérdida y ajustando los parámetros del modelo.\n\n\n\nPara ello utilizamos un algoritmo llamado descenso de gradiente estocástico, o SGD, implementado en PyTorch.\n\n\nsgd = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.95)\n\nRecuerde que este es el optimizador que usamos para encontrar el mínimo global de las funciones de pérdida.\nEl optimizador toma los parámetros del modelo junto con dos argumentos claves:\n\n\n\nTasa de aprendizaje: controla el tamaño del paso de las actualizaciones.\n\n\nmomentum: añade inercia para ayudar al optimizador a moverse con suavidad y evitar atascarse.\n\n\n\n\n\nImpaco de learning rate: tasa de aprendizaje óptima\nComprender su impacto nos ayuda a optimizar la eficiencia.\nIntentemos encontrar el mínimo de una función en forma de U.\n\n\n\n\n\nComenzamos en \\(x = -2\\) y ejecutamos el optimizador SGD durante diez pasos. Luego de estos pasos observamos que el optimizador está cerca del mínimo.\nTambién podemos observar que a medida que nos acercamos al mínimo, el tamaño del paso disminuye gradualmente. Esto sucede porque el tamaño del paso es el gradiente multiplicado por la tasa de aprendizaje. Como la función es menos pronunciada cerca de cero, el gradiente, y por tanto el tamaño del paso, se hace más pequeño.\n\n\nImpacto de learning rate tasa de aprendizaje pequeña\n\n\n\n\n\nSin embargo, si utilizamos el mismo algoritmo para un aprendizaje, si reducimos la velocidad diez veces, nos damos cuenta de que todavía estamos lejos del mínimo de la función después de diez pasos. El optimizador tardará mucho más tiempo en encontrar el mínimo de la función.\n\n\nImpacto learning rate: tasa de aprendizaje alto\nSi utilizamos un valor alto para la tasa de aprendizaje, observamos que el optimizador no puede encontrar el mínimo y rebota de un lado a otro en ambos lados de la función.\n\n\n\n\n\nRecuerde que las funciones de pérdida no son convexas.\n\n\n\n\n\nUno de los desafíos al intentar encontrar el mínimo de una función no convexa es quedarse atrapado en un mínimo local.\n\n\nSin momentum\n\n\\(lr = 0.01\\) \\(momentum = 0\\), ejecutemos nuestro optimizador durante 100 pasos con un momento nulo en esta función no convexa para \\(x = -1.23\\) y \\(y = -0.14\\).\n\n\n\nVemos que el optimizador se queda atascado en esta primera caída de la función, que no es su mínimo global.\n\nSin embargo, al utilizar lr = 0.01, momentum = 0.9 ejecutando nuestro optimizador durante 100 pasos para \\(x = 0.92\\) y \\(y = -2.04\\). Podemos encontrar el mínimo de la función.\n\n\n\nEste parámetro proporciona impulso al optimizador permitiendole superar caídas locales, como se muestra en la figura previa.\nEl impulso (momentum) mantiene el tamaño del paso grande cuando los pasos anteriores también fueron grandes, incluso si el gradiente actual es pequeño.\n\n\nSummary\nEnr esumen, dos parámetros optimizadores clave impactan el entrenamiento: la tasa de aprendizaje (learning rate) y el impulso (momentum):\n\n\n\n\n\n\n\nLearning Rate\nMomentum\n\n\n\n\nControla el tamaño del paso\nControla la inercia\n\n\nValor de tasa alta -&gt; bajo rendimiento\nAyuda a escapar de mínimos locales\n\n\nValor de tasa bajo -&gt; entrenamiento lento\nDemasiado pequeño -&gt; el optimizador se queda atascado\n\n\nValores tipicos varían de 0.01 a 0.0001.\nRango típico: 0.85 a 0.99\n\n\n\nEl momentum y learning rate son críticos par el entrenamiento de tu red neuronal. Una buena regla general es comenzar con una learning rate de 0.001 y un momentum de 0.95."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#evaluación-del-rendimiento-de-los-modelos",
    "href": "posts/DL_PyTorch/index.html#evaluación-del-rendimiento-de-los-modelos",
    "title": "Deep Learning",
    "section": "Evaluación del Rendimiento de los Modelos",
    "text": "Evaluación del Rendimiento de los Modelos\nHemos realizado mucho entrenamiento. Ahora vamos a evaluar nuestros modelos. En el Machine Learning, los datos se dividen en conjuntos de entrenamiento, validación y prueba.\n\n\n\nTraining\n80-90%\nAjusta los parámetros del modelo\n\n\nValidación\n10 - 20%\nAjusta los hiperparámetros del model\n\n\nTest\n5 - 10%\nEvalúa el rendimiento final del modelo\n\n\n\nLos datos de entrenamiento ajustan los parámetros del modelo, como pesos y sesgos, y los datos de validación ajustan hiperparámetros como la tasa de aprendizaje y el impulso, y el conjunto de pruebas evalúa el rendimiento final del modelo.\nRealizaremos un seguimiento de dos métricas clave: pérdida y precisión durante el entrenamiento y la validación.\n\nCalculando Pérdida de Entrenamiento\n\nLa pérdida de entrenamiento se calcula sumando la pérdida de todos los lotes en el cargador de datos de entrenamiento.\nAl final de cada época (epoch), calculamos la pérdida de entrenamiento media dividiendo la pérdida total por el número de epoch.\n\ntraining_loss = 0.0 \n\nfor inputs, labels in trainloader: \n  outputs = model(inputs)\n  loss = criterion(outputs, labels)\n  loss.backward()\n  optimizer.step()\n  optimizer.zero_grad()\n  \n  training_loss += loss.item()\n  \nepoch_loss = training_loss / len(trainloader)\nIteramos a través del cargador de entranamiento, ejecutamos un pase hacia adelante y calculamos la pérdida. Como es habitual, el modelo calcula gradiente y actualiza pesos mediante retropropagación. Y agregamos cada valor de pérdida al total usando .item(), que extrae el valor numérico de u tensor.\nDqdo que una epoch es un paso completo a través del cargado de datos de entrenamiento, calculamos la pérdida media dividiendo training_loss por el número de lotes en el cargador de train.\n\n\nCalculo de la pérdida de validación\nDespués de cada epoch de entrenamiento, ejecutamos un ciclo de validación.\n\nPrimero, establecemos el modelo en modo evaluación usando .eval(), como algunas capas se comportan de manera diferente durante el entrenamiento y la validación.\nPara mejorar la eficiencia, utilizamos torch.no_grad() , que deshabilita cálculos de gradiente ya que no actualizamos los pesos durante la validación.\nLuego, iteramos a través del cargador de datos de validación, ejecutamos un pase hacia adelante y calculamos la pérdida, sumándola en los lotes.\nAl final de la época, calculamos la pérdida de validación media.\nFinalmente, volvemos al modo de entrenamiento con .train(), preparándolo para la siguiente epoch de entrenamiento.\n\nvalidation_loss = 0.0\n\nmodel.eval()\n\nwith torch.no_grad(): \n  for inputs, labels in validationloader: \n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    validation_loss += loss.item()\n    \nepoch_loss = validation_loss / len(validationloader) \nmodel.train()\n\n\n\nSobreajuste o Overfitting\nRealizar un seguimiento de la pérdida de entrenamiento y validación nos ayuda a detectar el sobreajuste.\n\n\n\n\n\nCuando un modelo se sobreajusta, la pérdida de entrenamiento continúa disminuyendo, pero la pérdida de validación comienza a aumentar. Esto significa que el modelo está aprendiendo demasiado bien los datos de entrenamiento y no funcionará bien con datos nuevos.\n\n\nCalculando accuracy con torchmetrics\nLa pérdida nos indica qué tan bien está aprendiendo un modelo, pero no siempre refleja con qué precisión hace predicciones.\n\nHacemos un seguimiento de la precisión utilizando torchmetrics.\nPara tareas de clasificación de múltiples clases, creamos una métrica de precisión con torchmetrics.Accuracy()\nA medida que el modelo procesa cada lote, actualizamos esta métrica utilizando sus predicciones y las etiquetas reales.\nDado que el modelo genera probabilidades para múltiples clases, podemos utilizar argmax(dim=-1) para seleccionar la clase con mayor probabilidad. Esto convierte las predicciones codificadas one-hot en índices de clase antes de pasarlas a la métrica.\nAl final de cada epoch, calculamos la precisión general utilizando .compute().\nFinalmente, reiniciamos la métrica con .reset() para borrar su estado antes de la próxima epoch.\n\nimport torchmetrics \n\nmetrics = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 3)\n\nfor features, labels in dataloader: \n  outputs = model(features)\n  metrics.update(outputs, labels.argmax(dim=-1))\n  \naccuracy = metrics.compute()\n\nmetric.reset()"
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#evaluación-del-rendimiento-de-los-modelos-1",
    "href": "posts/DL_PyTorch/index.html#evaluación-del-rendimiento-de-los-modelos-1",
    "title": "Deep Learning",
    "section": "Evaluación del rendimiento de los modelos",
    "text": "Evaluación del rendimiento de los modelos\nHemos realizado mucho entrenamiento"
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#lucha-contra-el-sobreajuste",
    "href": "posts/DL_PyTorch/index.html#lucha-contra-el-sobreajuste",
    "title": "Deep Learning",
    "section": "Lucha contra el Sobreajuste",
    "text": "Lucha contra el Sobreajuste\nAnteriormente, aprendimos cómo detectar el sobreajuste observando las pérdidas de entrenamiento y validación. Ahora, descubriremos algunas formas de combatir el sobreajuste.\nRecuerde que el sobreajuste ocurre cuando el modelo no se generaliza a datos no vistos. Si no entrenamos correctamente el modelo, comenzará a memorizar los datos de entrenamiento, lo que conduce a un buen rendimiento en el conjunto de entrenamiento pero a un rendimiento deficiente en el conjunto de validación.\nPosibles causas:\n\n\n\nProblema\nSolución\n\n\n\n\nConjunto de datos pequeños\nMás data\n\n\nModelo con demasiada capacidad\nReducir el tamaño del modelo\n\n\nValores grandes de los pesos\nReducir los valores de los pesos\n\n\n\n\nPara contrarrestar el sobreajuste, podemos reducir el tamaño del modelo o agregar un nuevo tipo de capa llamada dropout.\nTambién podemos utilizar la descomposición del peso para forzar que los parámetros permanezcan pequeños.\nPara obtener más datos o utilizar la ampliación de datos\n\nExploremos estas estrategías.\n\nRegularización usando una capa dropout\nUna forma común de combatir el sobreajuste es agregar capas dropout a nuestra red neuronal.\nEl dropout es una técnica de regularización que desactiva aleatoriamente una fracción de las neuronas durante el entrenamiento, evitando que el modelo se vuelva demasiado dependiente de características específicas.\n\nLas capas dropout normalmente se agregan después de las funciones de activación.\n\n\nmodel = nn.Sequential(nn.Linear(8, 4),\nnn.ReLU(),\nnn.Dropout(p = 0.5))\n\nfeatures = torch.randn((1, 8))\nprint(model(features))\n\ntensor([[0., 0., 0., 0.]], grad_fn=&lt;MulBackward0&gt;)\n\n\nEl argumento p determina la probabilidad de que una neurona se establezca en cero. En este ejemplo se descartan el 75% de las neuronas.\nEl dropout se comporta de manera diferente durante el entrenamiento y la evaluación, durante el entrenamiento, desactiva neuronas aleatoriamente, mientras que durante la evaluación se deshabilita, lo que garantiza que todas las neuronas estén activas para obtener predicciones estables.\nPara cambiar entre estos modos, utilizamos model.train() y model.eval()\n\n\nRegularización con disminución de pesos\nLa siguiente estratefia para reducir el sobreajuste que descubriremos es la disminución de pesos, otra forma de regularización.\nEn PyTorch, la disminución del peso se agreha al optimizador mediante el parámetro weight_decay, normalmente establecido en un valor pequeño, por ejemplo 0.0001. Este parámetro agrega una penalización a la función de pérdida, formando pesos más pequeños y ayudando al modelo a generalizar mejor.\noptimizer = optim.SGD(model.parameters(), lr = 0.001, weight_decay = 0.0001)\n\nDurante la retropropagación, esta penalización se resta del gradiente, lo que evita un crecimiento excesivo del peso.\nCuanto más alto fijemos la caída del peso, más fuerte será la regularización y hará que el sobreajuste sea menos probable.\n\n\n\nAumentación de Data\n\n\n\n\n\n\nRecopilar más datos puede ser costoso, pero los investigadores han encontrado una forma de expandir conjuntos de datos artificialmente usando aumento de datos.\nEl aumento de datos se aplica comúnmente a los datos de imágenes, que se pueden rotar y escalados, de modo que diferentes vistas de la misma cara estén disponibles como puntos de datos nuevos.\nSi bien no analizaremos aquí como aumentar los datos, sigue siendo un método valioso para combatir el sobreajuste cuando no hay datos adicionales disponibles."
  },
  {
    "objectID": "posts/DL_PyTorch/index.html#mejorar-el-rendimiento-del-modelo",
    "href": "posts/DL_PyTorch/index.html#mejorar-el-rendimiento-del-modelo",
    "title": "Deep Learning",
    "section": "Mejorar el Rendimiento del Modelo",
    "text": "Mejorar el Rendimiento del Modelo\nEn esta sessión final, reuniremos todo y aprenderemos una receta para abordar cualquier problema de Deep Learning.\n\nPrimero, creamos un modelo que pueda sobreajustarse al conjunto de entrenamiento. Esto garantizará que el problema tenga solución. También establecemos un línea base de rendimiento a la que aspirar con el conjunto de validación.\nLuego, necesitamos reducir el sobreajuste para aumentar el rendimiento en el conjunto de validación.\nPor último, podemos ajustar ligeramente los diferentes hiperparámetros para garantizar que logremos el mejor rendimiento posible.\n\nEs útil comenzar con un solo punto de datos antes de sobreajustar todo el conjunto de entrenamiento.\nfeatures, labels = next(iter(dataloader))\n\nfor i in range(1000): \n  outputs = model(features)\n  loss = criterion(outputs, labels)\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n\nCuando el modelo está configurado correctamente, debería alcanzar rápidamente una pérdida a cero y una predicción del 100% en ese punto de datos. Una vez que este paso sea exitoso, escalamos al conjunto de entrenamiento completo.\nEn esta etapa, utilizamos una arquitectura de modelo existente lo suficientemente grande como para sobreajustar mientras mantiene los hiperparámetros, como la tasa de aprendizaje, en sus valores predeterminados.\n\nAhora necesitamos crear un modelo que se generalice bien para maximizar la precisión de la validación.\n\n\n\n\n\nReducir el sobreajuste a menudo tiene un costo, ya que aplicar la regularización puede afectar significativamente el rendimiento del modelo.\n\n\n\n\n\nEl modelo original se ajusta al conjunto de entrenamiento, logrando una alta precisión pero sin generalizarse bien a datos nuevos. Por el contrario, con demasiada regularización, el modelo actualizado muestra una caída en la precisión del entrenamiento y la validación, lo que limita su capacidad de aprender de manera efectiva. Esto resalta la importancia de equilibrar la reducción del sobreajuste.\nEstrategías mientras monitoreamos de cerca las métricas clave para encontrar el modelo con mejor rendimiento.\nUna vez que estemos satisfechos con el rendimiento, el paso final es ajustar los hiperparámetros. Esto se hace a menudo en configuraciones del optimizador, como la tasa de aprendizaje o el impulso.\nLa busqueda de cuadrícula prueba parámetros a intervalos fijos. Por ejemplo, valores de impulso de 0.85 a 0.99 y tasas de aprendizaje de diez elvado a menos seís\n\n# Grid Search\nfor factor in range(2, 6): \n  lr = 10 ** -factor\n\n\n\n\n\n\nLa búsqueda aleatoria adopta un enfoque diferente. En lugar de probar valores establecidos, los seleccionamos aleatoriamente dentro de un rango determinado. La función np.random.uniform(2, 6), por ejemplo, elige un número entre 2 y 6, lo que nos permite explorar una variedad más amplia de ritmos de aprendizaje.\n\nfactor = np.random.uniform(2, 6)\nlr = 10** - factor\n\n\n\n\n\n\nLa búsqueda aleatoria suele ser más eficiente, ya que evita pruebas innecesarias y aumenta la posibilidad de encontrar configuraciones óptimas."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html",
    "href": "posts/Unsupervised_Learning/index.html",
    "title": "Aprendizaje No Supervisado",
    "section": "",
    "text": "Supongamos que tiene una colección de clientes con diversas características, como edad, ubicación e historial financiero, y deseas descubrir patrones y clasificarlos en grupos. O quizás tengas un conjunto de textos, como páginas de Wikipedia, y quieras segmentarlos en categorías en función de su contenido. Este es el mundo del aprendizaje no supervizado, llamado así porque no estás guiando, o supervisando, el descubrimiento de patrones mediante alguna tarea de predicción, sino descubriendo la estructura oculta a partir de datos no etiquetados. El aprendizaje no supervizado engloba diversas técnicas de aprendizaje automático, desde la agrupación hasta la reducción de dimensiones y la factorización de matrices. En este artículo, aprenderás los fundamentos de aprendizaje no supervizado e implementarás los algoritmos esenciales utilizando scikit-learn y SciPy. Aprenderás a agrupar, transformar, visualizar y extraer información de conjuntos de datos no etiquetados."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#aprendizaje-no-supervizado",
    "href": "posts/Unsupervised_Learning/index.html#aprendizaje-no-supervizado",
    "title": "Aprendizaje No Supervisado",
    "section": "Aprendizaje no supervizado",
    "text": "Aprendizaje no supervizado\nEl aprendizaje no supervizado es una clase de técnica de aprendizaje automático para descrubrir patrones en los datos. Por ejemplo:\n\nEncontrar los grupos naturales de clientes en función de sus historiales de compras o buscar:\nPatrones y correlaciones entre estas compras, y utilizar estos patrones para expresar los datos en forma comprimida.\n\nEstos son ejemplos de técnicas de aprendizaje no supervizadas llamadas agrupación y reducción de dimensiones.\n\nEl aprendizaje no supervizado se define en oposición al aprendizaje supervizado.\n\n\nUn ejemplo de aprendizaje supervisado es utilizar las medidas de los tumores para clasificarlos como benignos o cancerosos. En este caso, el descubrimiento de patrones es guiado o supervizado, de modo que los patrones son lo más utilies posible para predecir la etiqueta: benigno o canceroso.\nEl aprendizaje no supervisado, por el contrario, es un aprendizaje sin etiquetas. Es puro descubrimiento de patrones, sin la guía de una tarea de predicción.\n\n\nIris dataset\nEl conjunto de datos iris consta de mediciones de muchas plantas iris de tres especies diferentes:\n\nSetosa\nVersicolor\nVirginica\n\nHay cuatro medidas:\n\nLargo de pétalo (Petal length)\nAncho de pétalo (Petal width)\nLargo de sépalo (Sepal length)\nAncho de sépalo (sepal width)\n\nEstas son las características del conjunto de datos.\n\nMatrices, características y muestras\nConjuntos de datos como este (iris) se escribiran como matrices numerosas bidimensionales.\n\nLas columnas de la matriz corresponderán a las características.\nLas medidas de plantas individuales son las muestras del conjunto de datos, estas corresponden a filas de la matriz.\n\n\n\nIris datases es 4-dimensional\nLas muestras del conjunto de datos iris tienen cuatro medidas y, por lo tanto, corresponden a puntos en un espacio de cuatro dimensiones. Es decir:\n\nDimensiones = número de características o features.\n\nNo podemos visualizar cuatro dimensiones directamente, pero utilizando técnicas de aprendizaje no supervizado aún podemos obtener información.\n\n\n\nK-Means Clustering\nAgruparemos estas muestras utilizando la agrupación de k-Means. K-Means encuentra un número específico de grupos en las muestras. Está implementado en la biblioteca de scikit-learn o sklearn.\nVeamos KMeans en acción con el conjunto de datos iris.\n\nimport pandas as pd         \nfrom sklearn import datasets    # Librería utilizada para importar iris dataset\niris = datasets.load_iris()     # importamos el conjunto de datos iris\n\niris = pd.DataFrame(iris.data, columns = iris.feature_names) # Convertimos iris a dataframe\n\nPara comenzar:\n\nImportamos KMeans de scikit-learn.\nLuego creamos un modelo KMeans, especificando la cantidad de clústeres que deseamos encontrar con n_clusters especificamos n_cluster = 3, ya que hay tres especies de iris.\nPosteriormente, llamamos el método de ajuste del modelo .fit(), pasando la matriz del iris dataset. Esto ajusta el modelo a los datos, localizando y recordando las regiones donde ocurren los diferentes grupos.\nPor último, podemos utilizar el método de predicción del modelo en este mismo conjunto de datos.\n\n\nfrom sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters = 3, random_state = 42)\nmodel.fit(iris)\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42) \n\n\n\nlabels = model.predict(iris)\nspecies_ = labels\nprint(labels)\n\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0\n 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0\n 0 2]\n\n\nEsto nos devuelve una etiqueta de grupo para cada muestra, que indica a qué grupo pertenece una muestra.\n\nEtiquetas de clúster para nuevas muestras\nSi alguien viene con un algún dataset iris nuevo, kMeans puede determinar a qué grupos pertenecen sin tener que empezar de nuevo.\nKMeans hace esto recordando la media (o promedio) de las muestras en cada grupo. Estos se llaman centroides se asignan nuevas muestras al grupo cuyo centroide esté más cercano.\nVoy a tomar 3 registros aleatorios del dataset iris y asumiremos que estas son muestras nuevas.\n\nnew_samples = iris.sample(n = 3, random_state = 42)\nprint(new_samples)\n\n     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n73                 6.1               2.8                4.7               1.2\n18                 5.7               3.8                1.7               0.3\n118                7.7               2.6                6.9               2.3\n\n\nPara asignar las nuevas muestras a los grupos existentes, pasaremos el conjunto de nueva muestra al método de predicción del modelo kmeans.\n\nnew_labels = model.predict(new_samples)\nprint(new_labels)\n\n[2 1 0]\n\n\nComo puede observar, esto devuelve las etiquetas de grupo para los datos nuevos o muestra de datos nueva.\n\nEn la siguiente sección aprenderá cómo evaluar la calidad de su agrupación.\n\n\n\nScatter Plots\nPor ahora, visualizaremos nuestra agrupación de las muestras de iris usando diagramas de dispersión. A continuación se muestra un diagrama de dispersión de la longitud del sépalo frente a la longitud del pétalo del dataset iris.\n\n\n\n\n\nCada punto representa una muestra de iris y está coloreada según el grupo de la muestra. Para crear un diagrama de dispersión como este, usaremos PyPlot.\n\nLa longitud del sépalo está en la columna 0 de la matriz iris, mientras que la longitud de los pétalos está en la segunda columna.\nY labels o etiquetas que encontramos previamente lo usamos para colorear por etiqueta de clúster como un paramétro en .scatter().\n\n\nimport matplotlib.pyplot as plt \n\nxs = iris.iloc[:, 0]   # Longitud de los sépalos \nys = iris.iloc[:, 2]   # Longitud de los pétalos\n\nplt.scatter(xs, ys, c = labels)\nplt.show()\n\n\n\n\n\n\n\n\nAhora, calculemos las coordenadas de los centroides utilizando el atributo .cluster_centers_ de model. y asignaremos la columna 0 de centroids a centroids_x, y la columna 2 de centroids a centroids_y. Posterior a ello, realizaremos un diagrama de dispersión de centroids_x y centroids_y, utilizando marker = 'D' (un rombo) como marcador especificando el parámetro marker. El tamaño de los marcadores en 50 utilizando s = 50.\n\nxs = iris.iloc[:, 0]   # Longitud de los sépalos \nys = iris.iloc[:, 2]   # Longitud de los pétalos\n\nplt.scatter(xs, ys, c = labels, alpha = 0.5)\ncentroids = model.cluster_centers_\n\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,2]\n\nplt.scatter(centroids_x, centroids_y, marker = 'D', s = 50)\nplt.show()"
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#evaluar-una-agrupación",
    "href": "posts/Unsupervised_Learning/index.html#evaluar-una-agrupación",
    "title": "Aprendizaje No Supervisado",
    "section": "Evaluar una agrupación",
    "text": "Evaluar una agrupación\nEn la sección anterior, utilizamos KMeans para agrupar el dataset de iris en tres grupos. Pero ¿cómo podemos evaluar la calidad de esta agrupación?\nUn enfoque directo es comparar los grupos con las especies de iris. Primero aprenderá sobre esto, antes de considerar el problema de cómo medir la calidad de una agrupación de una manera que no requiera que nuestras muestras vengan preagrupadas en especies. Esta medida de calidad puede utilizarse más adelante para tomar una decisión informada sobre el número de conglomerados a buscar.\n\nIris: Clusters vs Especies\nEn primer lugar, comprobemos si los 3 grupos de muestras de iris tienen alguna correspondencia con la especie de iris.\nLa correspondencia se describe en esta tabla:\n\n\n\n\n\nExiste una columna para cada una de las tres especies de iris: setosa, versicolor y virginica, y una fila para cada una de las tres etiquetas de grupo: 0, 1 y 2.\nLa tabla muestra el número de muestras que tienen cada combinación posible de etiquetas de grupo/especie. Por ejemplo:\n\nVemos que el grupo 0 se corresponde perfectamente con la especie setosa.\nPor otro lado, mientras que el cluster 1 contiene principalmente muestras de virginica en el grupo 2.\n\nTablass como esta, se denominan tabulaciones cruzadas o Cross tabulation\n\n\nCross Tabulation con Pandas\nPara construir uno, usareos la biblioteca pandas. Como podemos observar en el bloque de código siguiente, creamos un dataframe de dos columnas, donde la primera columna son las etiquetas del grupo y la segunda son las especies de iris, de modo que cada fila proporciona la etiqueta del grupo y la especie de una sola muestra.\n\niris_raw = datasets.load_iris() \niris = pd.DataFrame(iris_raw.data, columns = iris_raw.feature_names)\n# Agregamos columna de especie verdadera al dataframe iiris\nspecies = pd.Categorical.from_codes(iris_raw.target, iris_raw.target_names)\n\n# \n\ndf = pd.DataFrame({'labels':labels, 'species':species})\nprint(df)\n\n     labels    species\n0         1     setosa\n1         1     setosa\n2         1     setosa\n3         1     setosa\n4         1     setosa\n..      ...        ...\n145       0  virginica\n146       2  virginica\n147       0  virginica\n148       0  virginica\n149       2  virginica\n\n[150 rows x 2 columns]\n\n\nAhora usamos la función de tabla cruzuda de pandas para crear la tubulación cruzada, pasando las dos columnas del DataFrame.\n\nct = pd.crosstab(df['labels'], df['species'])\nprint(ct)\n\nspecies  setosa  versicolor  virginica\nlabels                                \n0             0           3         36\n1            50           0          0\n2             0          47         14\n\n\nTabulaciones cruzadas como estás proporcionan información valiosa sobre qué tipo de muestras se encuentran en qué grupo. Pero la mayoría de los conjuntos de datos, las muestras no están etiquetadas por especie.\n¿Cómo se puede evaluar la calidad de un clustering en estos casos?\n\n\nMedición de la calidad de la agrupación\nUna buena agrupación tiene grupos compactos, lo que significa que las muestras de cada grupo están agrupadas, no dispersas.\nLa inercia puede medir la distribución de las muestras dentro de cada grupo. Intuitivamente, la inercia mide qué tan lejos están las muestras de sus centroides. Puede encontrar la definición precisa en la documentación de scikit-learn. Queremos grupos que no estén dispersos, por lo que los valores más bajos de inercia son mejores.\nLa inercia de un modelo KMeans se mide automaticamente cuando se llama al metodo de ajuste .fit y luego están disponibles como atributos de inertia_.\nDe hecho, KMeans pretende colocar los clusters de forma que se minimice la inercia.\n\nmodel = KMeans(n_clusters = 3, random_state = 42)\nmodel.fit(iris)\nprint(model.inertia_)\n\n78.8556658259773\n\n\nA continuación se muestra un gráfico de los valores de inercia de las agrupaciones del conjunto de datos de iris con diferentes números de agrupaciones.\n\n\n\n\n\nNuestro modelo KMeans con 3 grupos tiene una inercia relativamente baja, lo cual es genial. Pero observe que la incercia continúa disminuyendo lentamente. Entonces, ¿cuál es la mejor cantidad de clústeres para elegir? En última instacia se trata de una compensación.\nUna buena agrupación tiene agrupaciones estrechas (lo que significa inercia baja). Pero tampoco tiene demasiados grupos.\nUna buena regla general es elegir un codo en el gráfico de inercia, es decir, un punto donde la inercia comienza a disminuir más lentamente.\nPor ejemplo, según este criterio, 3 es un buen número de grupos para el conjunto de datos del iris.\n\nks = range(1, 11)\ninertias = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k, random_state=42)\n    model.fit(iris)\n    inertias.append(model.inertia_)   # suma de distancias cuadradas intracluster\n\nplt.plot(ks, inertias, '-o')\nplt.xlabel('número de clusters')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()"
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#transformación-de-rasgos-para-mejorar-agrupaciones",
    "href": "posts/Unsupervised_Learning/index.html#transformación-de-rasgos-para-mejorar-agrupaciones",
    "title": "Aprendizaje No Supervisado",
    "section": "Transformación de rasgos para mejorar agrupaciones",
    "text": "Transformación de rasgos para mejorar agrupaciones"
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#transformación-de-características-para-mejorar-agrupaciones",
    "href": "posts/Unsupervised_Learning/index.html#transformación-de-características-para-mejorar-agrupaciones",
    "title": "Aprendizaje No Supervisado",
    "section": "Transformación de Características para mejorar agrupaciones",
    "text": "Transformación de Características para mejorar agrupaciones\nVeamos ahora otro conjunto de datos, el conjunto de datos de los vinos del Piamonte.\n\nDisponemos de 178 muestras de vino tinto de la región de Piamonte de Italia.\nLas entradas o features miden la composición química (como el contendio del alcohol) y propiedades visuales como la intensidad del color.\nLas muestras proceden de 3 variedades distintas de vino.\n\n\nsamples = pd.read_csv(\"wine.csv\")\nvarieties = samples['class_name']\nsamples = samples.drop(\"class_name\", axis = 1)\nsamples\n\n\n\n\n\n\n\n\nclass_label\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280\nproline\n\n\n\n\n0\n1\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n\n\n1\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n\n\n2\n1\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n\n\n3\n1\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n\n\n4\n1\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n3\n13.71\n5.65\n2.45\n20.5\n95\n1.68\n0.61\n0.52\n1.06\n7.70\n0.64\n1.74\n740\n\n\n174\n3\n13.40\n3.91\n2.48\n23.0\n102\n1.80\n0.75\n0.43\n1.41\n7.30\n0.70\n1.56\n750\n\n\n175\n3\n13.27\n4.28\n2.26\n20.0\n120\n1.59\n0.69\n0.43\n1.35\n10.20\n0.59\n1.56\n835\n\n\n176\n3\n13.17\n2.59\n2.37\n20.0\n120\n1.65\n0.68\n0.53\n1.46\n9.30\n0.60\n1.62\n840\n\n\n177\n3\n14.13\n4.10\n2.74\n24.5\n96\n2.05\n0.76\n0.56\n1.35\n9.20\n0.61\n1.60\n560\n\n\n\n\n178 rows × 14 columns\n\n\n\n\nClustering con Vinos\nTomaremos la matriz de muestra de los vinos y usaremos KMeans para encontrar 3 grupos\n\nmodel = KMeans(n_clusters = 3, random_state = 42)\nlabels = model.fit_predict(samples)\n\n\nCluster vs Varieties\nHay tres variedades de vinom así que usamos pandas para crear la tabla cruzada para comprobar las correspondencia entre la etiqueta del clúster y la variedad de vino.\n\ndf = pd.DataFrame({'labels': labels, 'varieties':varieties})\nct = pd.crosstab(df['labels'], df['varieties'])\nprint(ct)\n\nvarieties  Barbera  Barolo  Grignolino\nlabels                                \n0               37       1          64\n1               11      31           7\n2                0      27           0\n\n\nComo podemos ver, esta vez las cosas no han salido tan bien. Los clusters KMeans no se corresponden bien con las variedades de vino.\n\n\nVariación de Características\nEl problema es que las características del conjunto de datos del vino tiene variaciones muy diferentes.\n\nLa varianza de una característica mide la dispersión de sus valores\n\n\n\n\n\n\nPor ejemplo, la característica del ácido málico (malic_acid) tiene una mayor varianza que la característica 0d280, y esto también se puede ver en su diagrama de dispersión.\nLas diferencias en algunas de las variaciones de características son enormes, por ejmplo, en el diagrama de dispersión de características od280 y prolina.\n\n\n\n\n\n\n\n\nStandarScaler\nEn la agrupación en clústeres de KMeans, la varianza de una característica corresponde a su influencia en el algoritmo de agrupación en clústeres. Es decir:\n\n\n\nVarianza de Carasterística = Influencia de Característica\n\n\n\nPara darle una oportunidad a cada característica, los datos deben transformarce para que las características tengan la misma varianza. Esto se puede lograr con el StandarScaler de scikit-learn. Transforma cada característica para que tenga media 0 y varianza 1.\nLas características estandarizadas resultantes pueden ser muy informativas.\nSi utilizamos, por ejemplo, los valores estandarizados de od280 y proline, las tres variedades de vino son mucho más distintas.\n\n\n\n\n\nVeamos StandarScaler en acción:\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() # Creamos el objeto StandarScaler \nscaler.fit(samples)\nStandardScaler(copy = True, with_mean = True, with_std = True)\nsamples_scaled = scaler.transform(samples)\n\nEl método de transformación ahora se puede utilizar para estandarizar cualquier muestra, ya sean las mismas o completamente nuevas.\n\nMétodos similares\n\nLas API de StandarScaler y KMeans son similares, pero hay una diferencia importante:\n\nStandardScaler transforma datos y por eso tiene un método de transformación.\nKMeans por el contrario, asigna etiquetas de clúster a las muestras y esto se hace utilizando el método de predicción.\n\n\n\n\n\nStandardScaler luego KMeans\nVolvamos al problema del agrupamiento de los vinos. Necesitamos realizar dos pasos:\n\nEstandarizar los datos utilizando StandardScaler y\nTomar los datos estandarizados y agruparlos utilizando KMeans\n\nEsto se hace fácilmente combinando los dos pasos mediante un pipeline de scikit-learn, luego, los datos fluyen de un paso al siguiente de forma automática.\n\nPipelines combinando multiples pasos\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\n\nscaler = StandardScaler()\nkmeans = KMeans(n_clusters = 3, random_state = 42)\n\n# Aplicamos los pasos que queremos combinar\npipeline = make_pipeline(scaler, kmeans)\npipeline.fit(samples)\n\n# Utilizamos el método de predicción para obtener las etiquetas del clúster\n\nlabels = pipeline.predict(samples)\n\nAhora, realicemos la comprobación de la correspondencia entre las etiquetas de los cluster y las variedadesd de vino\n\ndf = pd.DataFrame({'labels': labels, 'varieties':varieties})\nct = pd.crosstab(df['labels'], df['varieties'])\nprint(ct)\n\nvarieties  Barbera  Barolo  Grignolino\nlabels                                \n0                0       0          67\n1               48       0           1\n2                0      59           3\n\n\n\nY como podemos observar, esto nos revela que al incorporar estandarización la nueva agrupación es fantástica. Sus tres clúster corresponden casi exactamente a las tres variedades de vino. Por tanto, esto es una gran mejora con respecto a la agrupación sin estandarización.\n\nStandardScaler es un ejemplo de un paso de preprocesamiento. Existen varios de estos disponibles en Scikit-Learn, por ejemplo, MaxAbsScaler y Normalizer."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#visualizar-jerarquías",
    "href": "posts/Unsupervised_Learning/index.html#visualizar-jerarquías",
    "title": "Aprendizaje No Supervisado",
    "section": "Visualizar Jerarquías",
    "text": "Visualizar Jerarquías\nUna gran parte del trabajo de cualquier cientifico de datos es la comunicación de los resultados o conocimiento a otras personas. Las visualizaciones son una excelente manera de compartir sus hallazgos especialmente con una audiencia no técnica. En este nuevo capítulo, aprenderá sobre dos métodos de aprendizaje no supervisado correspondientes a visualización:\n\nt-SNE\nAgrupamiento Jerárquico\n\nt-SNE, que lo veremos más adelante, crea un mapa 2D de cualquier conjunto de datos y transmite información útil sobre la proximidad de las muestras entre sí. Pero primero aprendamos sobre la agrupación jerárquica.\nYa has visto muchas agrupaciones jerárquicas en el mundo real. Por ejemplo, los seres vivos pueden organizarse en grupos pequeños y estrechos como los humanos, los simios, las serpientes y los lagartos, o en grupos más grandes y amplios, como los mamíferos y los reptiles, o incluso grupos más amplios, como los animales y las plantas. Como puede visualizar en la figura de abajo, estos grupos están contenidos unos dentro de otros y forman una jerarquía.\n\n\n\n\n\nDe manera análoga, la agrupación jerárquica organiza las muestras en una jerarquía de grupos.\nLa agrupación jerárquica puede organizar cualquier tipo de datos en una jerarquía, no sólo muestras de plantas y animales.\n\nDataset de scoring Eurovision\nConsideremos un nuevo tipo de conjunto de datos que describe cómo los países calificaron sus actuaciones en el Festival de la Canción de Eurovisión 2016. Los datos se organizan en una matriz rectangular, donde las filas de la matriz muestra cuántos puntos le dio un país a cada canción.\n\n\n\n\n\nLas muestras en este caso son los países. El resultado de aplicar la agrupación jerárquica al Festival de Eurovisión:\n\n\n\n\n\nLas puntuaciones se pueden visualizar como u diagrama en forma de árbol llamado dendrograma. Esta única imagen revela mucha información sobre el comportamiento electoral de los países en Eurovisión. El dendrograma agrupa a los países en grupos cada vez más grandes, y muchos de ellos los grupos se reconocen inmediatamente como compuestos por países cercanos entre si geográficamente, o que tienen estrechos vínculos culturales o político, o que pertenecen a un solo grupo linguístico. De esta manera, la agrupación jerárquica puede generar excelentes visualizaciones. ¿Pero cómo funciona?\nLa agrupación jerárquica se realiza en pasos:\n\nAl principio cada país es su propio clúster,por lo que hay tantos clústeres como países.\nEn cada paso se fusionan los dos clústeres más cercanos. Esto disminuye el número de clústeres, y\nAl final, sólo queda un grupo, que contiene todos los países.\n\nEste proceso es en realidad un tipo particular de agrupamiento jerárquico llamdo agrupamiento aglomerativo, también existe el agrupamiento divisivo, que funciona a la inversa.\nAún no hemos definido qué significa que dos clústeres estén cerca, pero volveremos a abordar este tema más adelante.\n\n\nDendrograma de una Clusterización Jerárquica\nTodo el proceso de agrupamiento jerárquico está codificado en el dendrograma.\n\n\n\n\n\nEn la parte inferior, cada país se encuentra en un grupo propio. Luego el agrupamiento continúa desde abajo hacia arriba.\nLos grupos se representan como líneas verticales y una unión de líneas verticales indica una fusión de grupos.\nPara comprender mejor, hagamos un acercamiento:\n\n\n\n\n\nObservemos sólo una parte de este dendrograma,\n\n\n\n\n\nAl principio hay seis grupos , cada uno de los cuales contiene sólo un país. La primera fusión se produce aquí:\n\n\n\n\n\ndonde los grupos que contienen a Cyprus y Grecia se fusionan en un solo grupo. Posteriormente, este nuevo clúster se fusiona con el clúster que contiene Bulgaria:\n\n\n\n\n\nPoco después, los grupos que incluyen a Moldova y Rusia son fusionados,\n\n\n\n\n\nque más tarde a su vez se fusiona con el grupo que contiene Armenia.\n\n\n\n\n\nMás tarde aún, los dos grupos compuestos se fusionan, Este proceso continua hasta que sólo quede un grupo, y éste contenga todos los países.\n\n\n\n\n\n\n\nClusterización Jerárquica con SciPy\nEn el capítulo anterior, utilizamos la agrupación KMeans para agrupar empresas según los movimientos de sus cotizaciones bursátiles. Ahora, realizaremos la agrupación jerárquica de las empresas.\nlinkage() de SciPy realiza una agrupación jerárquica en una matriz de muestras.\n\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.preprocessing import normalize\nimport numpy as np\n\nempresas = pd.read_csv(\"/Users/juanisaulamejia/Documents/webmathJI/posts/Unsupervised_Learning/empresas.csv\")\n\ncompanies = empresas.iloc[:, 0].tolist()\n\nempresas = empresas.select_dtypes(include=\"number\").copy()\n\narr = empresas.to_numpy()\nnormalized_movements = normalize(arr)\n\n# Calculate the linkage: mergings\nmergings = linkage(normalized_movements, method = 'complete')\n\n# Plot the dendrogram\ndendrogram(mergings, labels = companies,\nleaf_rotation = 90,\nleaf_font_size = 6)\nplt.show()\nplt.show()\n\n\n\n\n\n\n\n\nComo podemos observar, podemos crear visualizaciones geniales como esta con el clustering jerárquico, pero se puede usar ara más que solo visualizaciones."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#etiquetas-de-clústeres-en-la-agrupación-jerárquica",
    "href": "posts/Unsupervised_Learning/index.html#etiquetas-de-clústeres-en-la-agrupación-jerárquica",
    "title": "Aprendizaje No Supervisado",
    "section": "Etiquetas de clústeres en la agrupación jerárquica",
    "text": "Etiquetas de clústeres en la agrupación jerárquica\nEn la sección anterior empleamos la agrupación jerárquica. Creamos una gran visualización de las empresas según sus movimientos de sus cotizaciones bursátiles. Pero la agrupación jerárquica no es sólo una herramienta de visualización.\nEn esta sección aprenderá cómo extraer los clústeres de las etapas intermedias de una agrupación jerárquica. Las etiquetas de clúster para estos agrupamientos intermedios se pueden utilizar posteriormente en otros cálculos, como tabulaciones cruzadas, al igual que las etiquetas de clúster de KMeans.\n\nAgrupaciones intermedias y altura en el dendrograma\nUna etapa intermedia en la agrupación jerárquica se especifica eligiendo una altura en el dendrograma.\n\n\n\n\n\nPor ejemplo, elegir una altura de 15 define una agrupación en el que Bulgaria, Cyprus y Grecia están en un grupo, Rusia y Moldavia en otro, y Armenia en un grupo propio.\nPero ¿qué significa la altura?\nEl eje y del dendrograma codifica la distancia entre los grupos fusionados. Por ejemplo, la distancia entre el grupo que contiene Cyprus y el número de países que contenían Grecia era de aproximadamente 6 cuando se fusionaron en un solo grupo. Cuando este nuevo cúmulo se fusiono con el cúmulo que contenía a Bulgaria, la distancia entre ellos era de 12. Por lo tanto, la altura que especifica una agrupación intermedia corresponde a una distancia. Esto especifica que la agrupación jerárquica debe detenerse a fusionar clústeres cuando todos los clústeres están al menos a esta distancia.\n\n\nDistancia entre Clusters\nLa distancia entre dos grupos se mide utilizando un método de enlace (método linkage). En nuestro ejemplo, utilizamos un enlace completo, donde la distancia entre dos clusteres es el máximo de las distancias entre sus muestras. Esto se especificó a través del parámetro “method” o en python linkage(samples, method = \"complete\"). Existen muchos otros métodos de vinculación y los veremos en algunos ejercicios que realizaremos en Python con Scikit-Learn.\n\nLos diferentes métodos de vinculación producen diferentes agrupaciones jerárquicas.\n\n\n\nExtracción de etiquetas de clúster mediante fcluster.\nLa etiquetas de clúster para cualquier etapa intermedia se pueden extraer utilizando la función fcluster. Vamos a probarlo, especificando la altura de 15. Despues de realizar la agrupación jerárquica de los datos de empresas, importamos la función fcluster. Luego, pasamos el resultado de la función de vinculación a la función fcluster, especificando la altura como segundo argumento.\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import fcluster\n\nmergings = linkage(arr, method = 'complete')\nlabels   = fcluster(mergings, 15, criterion = 'distance')\nprint(labels)\n\n[35 29 33 23  9 18 25 18  6 26  1 18  3 21 18 18 18 36 32 18 14 18 19 31\n 18 11 20 27 10  7 34 15  4 18 18 30  8 18 10 18 11 28 16 17  5 18 18 18\n 24 16 18 18 18 22 12 20 13  2 18 18]\n\n\nEsto nos devolvio una matriz numpy que contiene las etiquetas de clúster para todos los países.\n\n\nAlineación de las etiquetes de los clústeres con los nombres de las empresas\nPara inspeccionar las etiquetas de los clústeres, usaremos un DataFrame para alinear las etiquetas con los nombres de los países.\n\nimport pandas as pd \npairs = pd.DataFrame({'labels': labels, 'companies': companies}) \nprint(pairs.sort_values('labels'))\n\n    labels                           companies\n10       1                      ConocoPhillips\n57       2                               Exxon\n12       3                             Chevron\n32       4                                  3M\n44       5                        Schlumberger\n8        6                         Caterpillar\n29       7                     Lookheed Martin\n36       8                    Northrop Grumman\n4        9                              Boeing\n38      10                               Pepsi\n28      10                           Coca Cola\n25      11                   Johnson & Johnson\n40      11                      Procter Gamble\n54      12                            Walgreen\n56      13                            Wal-Mart\n20      14                          Home Depot\n31      15                           McDonalds\n42      16                   Royal Dutch Shell\n49      16                               Total\n43      17                                 SAP\n46      18                      Sanofi-Aventis\n39      18                              Pfizer\n47      18                            Symantec\n37      18                            Novartis\n50      18  Taiwan Semiconductor Manufacturing\n34      18                          Mitsubishi\n51      18                   Texas instruments\n33      18                           Microsoft\n52      18                            Unilever\n45      18                                Sony\n59      18                               Yahoo\n58      18                               Xerox\n5       18                     Bank of America\n7       18                               Canon\n11      18                               Cisco\n15      18                                Ford\n16      18                   General Electrics\n19      18                     GlaxoSmithKline\n14      18                                Dell\n21      18                               Honda\n24      18                               Intel\n22      19                                  HP\n55      20                         Wells Fargo\n26      20                      JPMorgan Chase\n13      21                   DuPont de Nemours\n53      22                       Valero Energy\n3       23                    American express\n48      24                              Toyota\n6       25            British American Tobacco\n9       26                   Colgate-Palmolive\n27      27                      Kimberly-Clark\n41      28                       Philip Morris\n1       29                                 AIG\n35      30                            Navistar\n23      31                                 IBM\n18      32                       Goldman Sachs\n2       33                              Amazon\n30      34                          MasterCard\n0       35                               Apple\n17      36                     Google/Alphabet\n\n\nComo era de esperar, las etiquetas del clúster agrupan a Intel, Texas instruments en el mismo clúster. Pero tenga en cuenta que las etiquetas de clúster de scipy comienzan en 1, no en 0 como lo hacen en scikit-learn.\n\n\nt-SNE para mapas bidimensionales\nEn esta sección, aprenderá un método de aprendizaje no supervisado para visualización llamado t-SNE.\n\nt-SNE significa “incrustación de vecinos estocásticos distribuidos en t”\n\nTiene un nombre complicado, pero tiene un propósito muy simple. Mapea muestras de su espacio de alta dimensión a un espacio de 2D o 3D dimensiones para que puedan visualizarse. Si bien es inevitable cierta distorsión, t-SNE hace un gran trabajo de representar aproximadamente las distancias entre las muestras. Por este motivo, t-SNE es una ayuda visual invaluable para comprender un conjunto de datos.\n\nt-SNE en el dataset iris\nPara ver qué tipo de conocimientos son posibles con t-SNE, veamos cómo funciona en el conjunto de datos del iris. Las muestras de iris están en un espacio de cuatro dimensiones, donde cada dimensión corresponde a una de las cuatro medidas del iris, como la longitud y el ancho de los pétalos.\nAhora a t-SNE solo se le dieron las medidas de las muestras de iris. En particular, no se proporcionó ninguna información sobre las tres especies de iris. Pero si coloreamos las especies de manera diferente en el diagrama de dispersión vemos que t-SNE ha mantedio las especies separadas.\n\n\n\n\n\nSin embargo, este diagrama de dispersión nos da una nueva perspectiva. Aprendemos que hay dos especies de iris, versicolor y virginica, cuyas muestras están muy juntas en el espacio. Por lo tanto, podría suceder que el conjunto de datos del iris parezca tener dos grupos en lugar de tres. Esto es compatible con nuestros ejemplos anteriores usando KMeans, donde vimos que la agrupación con 2 grupos también tenía una inercia relativamente baja, lo que significa grupos apretados.\n\n\nt-SNE en Sklearn\nt-SNE está disponible en scikit-learn, pero funciona un poco de manera diferente a los componentes de ajuste/transformación que ya conoce.\nPara comenzar, importamos TSNE y creamos un objeto TNSE. Aplicamos el método fit_transform a las muestras y luego hacemos un diagrama de dispersión del resultado, colerando los puntos según la especie.\n\nfrom sklearn.manifold import TSNE\n\nmodel = TSNE(learning_rate = 100)\ntransformed = model.fit_transform(iris)\nxs = transformed[:, 0]\nys = transformed[:, 1]\nplt.scatter(xs, ys, c = species_)\nplt.show()\n\n\n\n\n\n\n\n\nHay dos puntos que merecen especial atención:\n\nEl método fit_transform y la\ntasa de aprendizaje (learning_rate)\n\nt-SNE solo tiene un método fit_transform(). Como era de esperar, el método fit_transform ajusta el modelo y transforma los datos simultáneamente. Sin embargi, t-SNE no tiene métodos de ajuste o fit() y transformación o transform() separados. Esto significa que no se puede ampliar un mapa t-SNE para incluir nuevas muestras. En cambio, debes empezar de nuevo cada vez.\nLa segunda cosa a tener en cuenta es la tasa de aprendizaje. Está hace que el uso de t-SNE sea más complicado que otras técnicas. Es posible que deba probar diferentes tasas de aprendizaje para diferentes conjuntos de datos.\nEstá claro, sin embargo, cuando has hecho una mala elección, porque todas las muestras aparecen agrupadas en el diagrama de dispersión. Normalmente es suficiente probar algunos valores entre 50 y 200.\nUna última cosa a tener en cuenta es que los ejes de un gráfico t-SNE no tienen ningún significado interpretable. De hecho son diferentes cada vez que se aplica t-SNE, incluso con los mismos datos. Por ejemplo, aquí hay tres gráficos t-SNE de muestras de vino de Piamonte escaladas, generados con el mismo código.\n\n\n\n\n\nTenga en cuenta que si bien la orientación de la trama es diferente cada vez, los tres vinos, las variedades, representadas aquí mediante colores, tienen la misma posición entre si.\n\n\n\nMapa t-SNE del mercado bursátil\nt-SNE proporciona excelentes visualizaciones cuando se pueden etiquetar las muestras individuales. En el siguiente bloque de código, se aplica t-SNE a los datos del precio de las acciones de la empresa. Un gráfico de dispersión de las características t-SNE resultantes, etiquetadas con los nombres de las empresas, te ofrece un mapa del mercado de valores. Los movimientos de las cotizaciones bursátiles de cada empresa están disponibles en normalized_movements las cuales normalizamos anteriormente.\n\n# Import TSNE\nfrom sklearn.manifold import TSNE\n\nmodel = TSNE(learning_rate = 50)\n\ntsne_features = model.fit_transform(normalized_movements)\n\nxs = tsne_features[:, 0]\n\nys = tsne_features[:,1]\n\n# Scatter plot\nplt.scatter(xs, ys, alpha = 0.5)\n\nfor x, y, company in zip(xs, ys, companies):\n    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\nplt.show()"
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#t-sne-para-mapas-bidimensionales",
    "href": "posts/Unsupervised_Learning/index.html#t-sne-para-mapas-bidimensionales",
    "title": "Aprendizaje No Supervisado",
    "section": "t-SNE para mapas bidimensionales",
    "text": "t-SNE para mapas bidimensionales\nEn esta sección, aprenderá un método de aprendizaje no supervisado para visualización llamado t-SNE."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#correlación-de-datos-y-reducción-de-dimensiones",
    "href": "posts/Unsupervised_Learning/index.html#correlación-de-datos-y-reducción-de-dimensiones",
    "title": "Aprendizaje No Supervisado",
    "section": "Correlación de Datos y Reducción de Dimensiones",
    "text": "Correlación de Datos y Reducción de Dimensiones\nLa reducción de dimensiones resume un conjunto de datos utilizando sus patrones comunes. En esta sección, aprenderás la técnica más fundamental de reducción de dimensiones, el análisis de componentes principales (PCA, por sus siglas en inglés). El PCA se utiliza a menudo antes del aprendizaje supervisado para mejorar el rendimiento y la generalización del modelo. También puede ser útil para aprendizaje no supervisado. Por ejemplo, utilizarás una variante PCA que te permitirá agrupar articulos de wikipedia según su contenido."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#visualización-de-la-transformación-pca",
    "href": "posts/Unsupervised_Learning/index.html#visualización-de-la-transformación-pca",
    "title": "Aprendizaje No Supervisado",
    "section": "Visualización de la transformación PCA",
    "text": "Visualización de la transformación PCA\nLa reducción de dimensiones encuentra patrones en los datos y utiliza estos patrones para reexpresarlos en una forma comprimida. Esto hace que el cálculo posterior con los datos sea mucho más sencillo, eficiente y, esto puede ser de gran importancia en un mundo de grandes conjuntos de datos.\nSin embargo, la función más importante de la reducción de dimensión es reducir un conjunto de datos a su “esqueleto básico”, descartando características ruidosas que causan grandes problemas para las tareas de aprendizaje supervisado como regresión y clasificación.\n\nEn muchas aplicaciones del mundo real, es la reducción de dimensiones la que hace posible la predicción.\n\n\nAnálisis de componentes principales\nEsta es la técnica de reducción de dimensión más fundamental. Se llama “Análisis de componentes principales”, o “PCA” para abreviar. PCA realiza la reducción de dimensión en dos pasos, y el primero:\n\nLlamada “descorrelación”, no cambia la dimensión de los datos en absoluto.\n\nEs justamente en este primer paso en el cual nos centraremos.\nEn este primer paso, PCA gira las muestras para que queden alineadas con los ejes de coordenadas. De hecho, hace más que eso: PCA también desplaza las muestras para que tengan media cero.\n\n\n\n\n\nEstos gráficos de dispersión muestran el efecto del PCA aplicado a dos características del conjunto de datos del vino. Tenga en cuenta que no se pierde información: esto es así sin importar cuántas características tenga su conjunto de datos. Veremos la visualización de esta transformación en ejercicios más adelante.\n\n\nPCA en Scikit - Learn\nScikit-learn tiene una implementación de PCA y tiene dos métodos de ajuste y transformación como StandardScaler.\n\nfit() el método de ajuste aprende cómo desplazar y rotar las muestras, pero en realidad no las cambia.\ntransform() el método de transformación, por otro lado, aplica la transformación que se aprendio. En particular, el método de transformación se puede aplicar a muestras nuevas e invisibles.\n\nVeamos PCA en acción en algunas características del conjunto de datos de vino, estas características son: total_phenols y od280.\n\nsamples = pd.read_csv(\"wine.csv\")\nvarieties = samples['class_name']\nsamples = samples.drop(\"class_name\", axis = 1)\nsamples = samples[[\"total_phenols\", \"od280\"]]\nsamples\n\n\n\n\n\n\n\n\ntotal_phenols\nod280\n\n\n\n\n0\n2.80\n3.92\n\n\n1\n2.65\n3.40\n\n\n2\n2.80\n3.17\n\n\n3\n3.85\n3.45\n\n\n4\n2.80\n2.93\n\n\n...\n...\n...\n\n\n173\n1.68\n1.74\n\n\n174\n1.80\n1.56\n\n\n175\n1.59\n1.56\n\n\n176\n1.65\n1.62\n\n\n177\n2.05\n1.60\n\n\n\n\n178 rows × 2 columns\n\n\n\nVeamos:\n\nEn primer lugar importamos el objeto PCA\nLuego creamos el objeto PCA y los ajustamos a las muestras o samples.\n\n\nfrom sklearn.decomposition import PCA\n\nmodel = PCA()\nmodel.fit(samples)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA() \n\n\nAhora utilizamos el objeto PCA ajustado para transformar las muestras. Estos nos devuelve una matriz de muestras transformadas.\n\ntransformed = model.transform(samples)\nprint(transformed)\n\n[[ 1.32771994e+00 -4.51396070e-01]\n [ 8.32496068e-01 -2.33099664e-01]\n [ 7.52168680e-01  2.94789161e-02]\n [ 1.64026613e+00  6.55724013e-01]\n [ 5.67992278e-01  1.83358911e-01]\n [ 8.07948468e-01  5.95331030e-01]\n [ 8.74453372e-01 -4.63619912e-01]\n [ 9.38570037e-01 -3.86879744e-01]\n [ 5.06600144e-01  2.34652243e-01]\n [ 1.15919131e+00 -7.60321086e-02]\n [ 8.48343677e-01  1.44589167e-01]\n [ 9.88781053e-02 -2.06553762e-01]\n [ 4.16736898e-01  4.91135760e-02]\n [ 6.06861937e-01  5.41812743e-01]\n [ 9.42293719e-01  5.22178083e-01]\n [ 5.61680527e-01  2.53787328e-01]\n [ 3.53119809e-01  3.62885573e-01]\n [ 3.87902672e-01  5.29289156e-01]\n [ 8.04161418e-01  6.37588080e-01]\n [ 8.33858333e-01 -1.69082914e-01]\n [ 1.29479891e+00 -1.63270739e-01]\n [ 7.70704274e-01 -4.94216064e-01]\n [ 1.26729041e+00 -6.48495720e-01]\n [ 9.00000123e-01 -5.11026278e-01]\n [ 1.07786477e+00 -5.94477857e-01]\n [ 6.66192400e-01 -1.20214368e-01]\n [ 8.22597096e-01  3.57906674e-02]\n [ 1.88741351e-01 -2.10150947e-02]\n [ 1.02484606e+00 -2.87916168e-03]\n [ 9.78302386e-01 -3.54921327e-01]\n [ 5.27397239e-01  4.77895909e-01]\n [ 5.68092193e-01  2.61461344e-01]\n [ 2.78304852e-01 -6.97837260e-02]\n [ 7.17885392e-01  2.53587497e-01]\n [ 2.33423186e-01 -1.23501843e-01]\n [ 9.18272518e-01 -2.39611246e-01]\n [ 3.24648697e-01  1.26053574e-01]\n [ 2.12752481e-02  1.84058317e-01]\n [ 1.27349217e-01  3.02782371e-02]\n [ 1.15666661e+00 -4.78607421e-02]\n [ 1.13773136e+00  1.63424506e-01]\n [ 3.97302069e-01 -1.30113340e-01]\n [ 1.33998032e+00  1.24754677e-01]\n [ 5.19123732e-01  1.56929782e-02]\n [ 1.01853431e+00  6.75492544e-02]\n [ 9.07011280e-01 -3.47376638e-02]\n [ 1.24789212e+00  2.01694675e-01]\n [ 1.06730294e+00  1.57112755e-01]\n [ 3.97801644e-01  2.60398824e-01]\n [ 7.49743895e-01  1.35752715e-01]\n [ 5.01350913e-01  1.34790110e-01]\n [ 6.81240688e-01 -3.67345000e-01]\n [ 1.51369481e+00  8.00567726e-01]\n [ 6.96225608e-01  3.36839246e-01]\n [ 6.46957401e-01 -1.43236418e-01]\n [ 7.47319109e-01  2.42026515e-01]\n [ 1.11607157e+00  2.46676255e-01]\n [ 6.27159457e-01  3.94544245e-01]\n [ 9.06648166e-01  6.82269915e-01]\n [-8.09580232e-01  2.65784517e-01]\n [-8.79808818e-01  4.15677631e-01]\n [-9.60435951e-01  4.43948913e-01]\n [-2.41503162e-01 -5.24739364e-02]\n [ 9.70764831e-01  7.59010083e-01]\n [-5.52650543e-01 -6.61599591e-02]\n [-1.59114103e-01  2.95681263e-01]\n [ 8.75252693e-01  1.61199551e-01]\n [ 5.47658212e-01 -6.98789900e-01]\n [-3.72524391e-01  6.17327107e-01]\n [ 6.63201973e-02 -6.35436010e-01]\n [-1.37380688e+00 -4.09528957e-01]\n [ 8.40669660e-01  1.51000834e-01]\n [-1.36991289e-01 -4.26475632e-01]\n [ 1.32599456e+00  2.01594760e-01]\n [ 1.09334927e+00  5.00218553e-01]\n [-8.01243355e-01 -2.23327430e-01]\n [-3.22330126e-01 -1.80407521e-01]\n [-4.39102388e-01 -3.82556572e-01]\n [-4.84846743e-01 -1.09779274e-01]\n [ 7.40707612e-01  7.81476322e-02]\n [ 4.70155270e-01 -2.30075388e-01]\n [ 3.44446641e-01 -4.11727089e-01]\n [-1.06095392e-01 -2.95917432e-01]\n [-8.75358907e-01 -1.09279699e-01]\n [ 2.98402541e-01 -3.73257090e-01]\n [ 3.59794675e-01 -4.24550422e-01]\n [-6.00156824e-01 -1.69809143e-01]\n [ 2.18638097e-01 -6.71481223e-01]\n [-1.15131674e-01 -3.53522515e-01]\n [ 3.98164758e-01 -4.56608755e-01]\n [-7.07892804e-01 -3.14353111e-01]\n [-5.12455165e-01 -6.73106688e-01]\n [-1.01010382e+00 -3.48536483e-01]\n [ 6.27522571e-01 -3.22463334e-01]\n [ 7.32070991e-01  3.32952280e-01]\n [ 1.45421781e-01  1.45488403e-01]\n [-7.15566821e-01 -3.07941444e-01]\n [ 2.61894298e-01  1.13330156e-01]\n [ 9.06847996e-01  8.38474781e-01]\n [ 5.23310443e-01  2.85845660e-01]\n [ 2.25549339e-01 -2.73295042e-01]\n [-4.20366964e-01 -7.50046686e-01]\n [ 7.59443036e-01 -2.89342482e-01]\n [-3.84536736e-04  2.67310066e-01]\n [ 6.74429361e-01 -6.87428748e-01]\n [ 1.33824252e-01 -9.13362624e-01]\n [ 1.48270353e-02 -8.53033010e-01]\n [-7.33839215e-01 -5.79356476e-01]\n [ 3.54945104e-01 -2.12002824e-01]\n [ 7.82764832e-01 -7.42701827e-02]\n [ 7.19547403e-01  5.51911546e-01]\n [ 2.92590365e-01  8.76834900e-02]\n [-4.35215422e-01 -3.46711188e-01]\n [-1.12970087e-01  3.35313697e-01]\n [ 6.13636718e-01 -1.67520819e-01]\n [ 3.03951517e-01 -3.90876590e-02]\n [ 3.49196297e-01 -7.02377121e-01]\n [ 7.80810103e-02 -4.49797428e-01]\n [-8.03768056e-01 -1.95156063e-01]\n [ 1.47147161e-01 -5.07502426e-01]\n [ 9.85113713e-01 -3.48375789e-02]\n [ 1.39486088e+00 -1.23151043e-02]\n [ 3.29098608e-01 -3.98903756e-01]\n [ 5.83040566e-01 -6.37717200e-02]\n [ 1.15131747e+00 -2.25825308e-01]\n [ 7.08349535e-01 -1.94529750e-01]\n [ 4.60456128e-01  1.95019809e-01]\n [-2.37616196e-01 -1.66285532e-02]\n [ 8.10053713e-02 -1.65559063e-01]\n [-1.57088978e-01 -1.23002268e-01]\n [-1.51765144e+00  2.44924053e-01]\n [-1.55253422e+00  4.18037417e-04]\n [-1.69475332e+00 -7.62222149e-02]\n [-1.39582978e+00  3.90730371e-01]\n [-1.03465142e+00  4.79894211e-01]\n [-1.22457663e+00  1.43399910e-01]\n [-1.61635114e+00  1.57985169e-01]\n [-1.03116411e+00  2.03329863e-01]\n [-1.04040022e+00 -1.04800857e-02]\n [-3.38341019e-01  3.15116093e-01]\n [-7.15666736e-01 -3.86043877e-01]\n [-6.82645799e-01 -5.96066775e-01]\n [-9.01105488e-01 -2.18078198e-01]\n [-6.20254514e-01  1.33664222e-01]\n [-1.05614792e+00 -3.10066484e-01]\n [-1.22477646e+00 -1.28049560e-02]\n [-1.82677370e+00 -1.87445500e-01]\n [-9.58410826e-01  2.52653823e-02]\n [-9.95118899e-01  3.55647763e-01]\n [-1.55107204e+00  1.42537220e-01]\n [-1.58050576e+00  1.54098203e-01]\n [-1.39875414e+00  1.06492006e-01]\n [-1.04455039e+00  7.48784543e-01]\n [-1.16288475e+00  3.26413876e-01]\n [-1.33736201e+00  5.51986747e-02]\n [-1.22437680e+00  2.99604775e-01]\n [-1.06312253e+00  2.43062212e-01]\n [-9.29939714e-01  2.62097381e-01]\n [-1.76387346e-01  8.05290560e-01]\n [-4.42752977e-01  7.67220221e-01]\n [-7.88583306e-01  6.65233048e-01]\n [-9.05755229e-01  1.50674266e-01]\n [-7.98618740e-01 -1.73396363e-01]\n [-1.24158667e+00 -1.42100806e-01]\n [-1.32095145e+00 -1.27915207e-01]\n [-1.31211500e+00 -2.26514990e-01]\n [-1.18863133e+00  2.17615377e-01]\n [-1.18388167e+00 -7.30346548e-02]\n [-1.10062992e+00 -5.13748700e-02]\n [-7.32840064e-01  2.01667852e-01]\n [-1.26995787e+00 -3.00830372e-01]\n [-1.33367487e+00 -6.51608079e-02]\n [-1.08634441e+00  1.06092346e-01]\n [-1.06332236e+00  8.68573465e-02]\n [-1.12451466e+00  2.94355544e-01]\n [-1.25915966e+00  1.33201192e-01]\n [-1.17464556e+00  1.40775294e-01]\n [-9.33526935e-01  4.60559297e-01]]\n\n\nEsta nueva matriz tiene el mismo número de filas y columnas que la matriz de muestra original. En particular, hay una fila para cada muestra transformada. Las columnas de la nueva matriz corresponden a “características de PCA”, tales como las características originales que correspondían a columnas de la matriz original.\n\n\nLAs características de PCA no están correlacionadas\nA menudo ocurre que las características de un conjunto de datos están correlacionadas. Lo mismo ocurre con muchas de las características del conjunto de datos del vino, por ejemplo. Sin embargo, del PCA, debido a la rotación que realiza, “descorrelaciona” los datos, en el sentido de que las columnas de la matriz transformada no están correlacionadas linealmente.\nComo sabemos la correlación lineal se puede medir con la correlación de Pearson. Toma valores entre -1 y 1, donde los valores mayores indican una correlación más fuerte y 0 indica que no hay correlación lineal. A continuación se muestran algunos ejemplos de características con distintos grados de correlación.\n\n\n\n\n\n\n\nComponentes Principales\nFinalmente, al PCA se le denomina “análisis de componentes principales” porque aprende los “componentes principales” de los datos. Estas son las direcciones en las que las muestran varían más, representadas aquí en rojo.\n\n\n\n\n\nSon los componentes principales que PCA alinea con los ejes de coordenadas. Una vez ajustado un modelo PCA, los componentes proncipales están disponibles como atributos de componentes.\n\nprint(model.components_)\n\n[[ 0.64116665  0.76740167]\n [ 0.76740167 -0.64116665]]\n\n\nEsta es una matriz numpy con una fila para cada componente principal."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#dimensión-intrínseca",
    "href": "posts/Unsupervised_Learning/index.html#dimensión-intrínseca",
    "title": "Aprendizaje No Supervisado",
    "section": "Dimensión Intrínseca",
    "text": "Dimensión Intrínseca\nConsideremos un conjunto de datos con 2 características: latitud y longitud.\n\n\n\n\n\nEstas dos características podrían rastrear el vuelo de un avión, por ejemplo. Este conjunto de datos es bidimensional, pero resulta que se puede analizar observando el desplazamiento a lo largo de la trayectoria de vuelo.\n\n\n\n\n\nEste conjunto de datos es intrínsicamente unidimensional.\n\nDimensión Intrínsica\nLa dimensión intrínsica de un conjunto de datos es el número de características necesarias para aproximarlo. La dimensión intrínsica informa la reducción de la dimensión, porque nos dice cuánto se puede comprimir un conjunto de datos.\nObtendremos una comprensión sólida de la dimensión intrínsica y seremos capaz de utilizar PCA para aidentificarlo en conjuntos de datos del mundo real que tienen miles de características.\nPara ilustrar mejor la dimensión intrínseca, consideremos un conjunto de datos de ejemplo que contiene solo algunas de las muestras del conjunto de datos iris. En concreto, tomaremos tres medidas del iris.\n\nsepal length (largo del sépalo)\nsepal width (ancho del sépalo)\npetal width (ancho del pétalo)\n\n\niris_sample = iris[['sepal length (cm)', 'sepal width (cm)','petal width (cm)']]\niris_sample\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n0.2\n\n\n1\n4.9\n3.0\n0.2\n\n\n2\n4.7\n3.2\n0.2\n\n\n3\n4.6\n3.1\n0.2\n\n\n4\n5.0\n3.6\n0.2\n\n\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n2.3\n\n\n146\n6.3\n2.5\n1.9\n\n\n147\n6.5\n3.0\n2.0\n\n\n148\n6.2\n3.4\n2.3\n\n\n149\n5.9\n3.0\n1.8\n\n\n\n\n150 rows × 3 columns\n\n\n\nCada muestra se representa como un punto en el espacio tridimensional. Sin embargo, si hacemos un gráfico de dispersión 3D de las muestras, vemos que todas están muy cerca de una lámila plana bidimensional.\n\n\n\n\n\nEsto significa que los datos se pueden aproximar utilizando sólo dos coordenadas, sin perder mucha información. Entonces, este conjunto de datos tiene una dimensión intrínseca de 2. PEro los gráficos de dispersión solo son posibles si hay 3 características o menos.\n\n\nPCA identifica la dimensión intrínseca\nCómo se puede identificar la dimensión intrínseca, si hay muchas características?\nAquí es donde el PCA resulta muy útil. La dimensión intrínseca se puede identificar contando las características de PCA que tienen alta varianza. Para ver cómo, veamos qué sucece cuando se aplica PCA al conjunto de datos de muestra versicolor.\n\n\n\n\n\nPCA gira y desplaza las muestras para alinearlas con los ejes de coordenadas. Esto expresa las muestras utilizando tres características PCA.\n\n\nLas carasterísticas de PCA se ordenan por varianza descendente\nLas características de PCA están en un orden especial.\n\n\n\n\n\nAquí se muestra un gráfico de barras que muestra la varianza de cada una de las características de PCA. Como se puede ver, cada característica de PCA tiene menos variación que la primera. Esto concuerda con el diagrama de dispersión de las características de PCA, donde las muestras no varían mucho en la dirección vertical. En las otras dos direcciones, sin embargo, la variación es evidente.\n\n\nVarianza y Dimensión Intrínseca\n\nLa dimensión intrínseca es el número de características de PCA que tiene una varianza significativa.\n\n\nEn nuestro ejemplo, solo las dos primeras características de PCA tienen una varianza significativa. Por lo tanto, este conjunto de datos tiene una dimensión intrínseca de 2, lo que concuerda con lo que observamos al inspeccionar el diagrama de dispersión.\n\n\nPlotting de varianza de PCA\nVeamos como gráficar las varianzas de las características del PCA en la práctica.\n\npca = PCA()\npca.fit(iris_sample)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA() \n\n\nAhora creeamos un rango que enumere las características de PCA\n\nfeatures = range(pca.n_components_)\n\nHacemos un diagrama de barras de la varianza; las varianzas están disponibles como el atributo explained_variance del modelo PCA.\n\nplt.bar(features, pca.explained_variance_)\nplt.xticks(features)\nplt.ylabel('variance')\nplt.xlabel('PCA feature')\nplt.show()\n\n\n\n\n\n\n\n\nLa dimensión intrínseca es una idea útil que ayuda a orientar la reducción de dimensión. Sin embargo, no siempre es inequívoco. Aquí se muestra u gráfico de las variaciones de las características de PCA para el conjunto de datos del vino.\n\n\n\n\n\nPodriamos argumentar a favor de una dimensión intrínseca de 2, de 3 o incluso más, dependiendo del umbral que elijamos.\nEn la siguiente sección, aprenderá a utilizar la dimensión intrínseca para la reducción de dimensión."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#reducción-de-dimensiones-con-pca",
    "href": "posts/Unsupervised_Learning/index.html#reducción-de-dimensiones-con-pca",
    "title": "Aprendizaje No Supervisado",
    "section": "Reducción de dimensiones con PCA",
    "text": "Reducción de dimensiones con PCA\nLa reducción de dimensión representa los mismos datos utilizando menos funciones y es vital para construir pipeline de aprendizaje automático utilizando datos del mundo real. Finalmente aquí, aprenderá cómo realizar la reducción de dimensión utilizando PCA. Ya hemos visto que las características del PCA están en orden decreciente de varianza.\n\n\n\n\n\nPCA realiza un reducción de dimensión descartando las características de PCA con menor varianza, lo que asume que es ruido y conserva las características PCA de mayor varianza, que supone que son informativas.\n\nPara utilizar PCA para la reducción de dimensión, debe especificar cuántas características de PCA desea conservar.\n\nPor ejemplo, especificar PCA(n_components=2) al crear un modelo PCA le indica que conserve solo las primeras dos características PCA. Una buena opción es la dimensión intrínseca del conjunto de datos, si la conoce. Consideremos inmediatamente un ejemplo:\nEl conjunto de datos del iris tiene 4 características que representan las 4 mediciones\n\niris\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\nUtilicemos un PCA para reducir la dimensión del conjunto de datos del iris a solo 2.\n\npca = PCA(n_components=2)\npca.fit(iris)\ntransformed = pca.transform(iris)\nprint(transformed.shape)\n\n(150, 2)\n\n\nAl imprimir la forma de las muestras transformadas, vemos que solo hay dos características, como se esperaba.\nAquí se muestra un diagrama de dispersión de las características de PCA, donde los colores representan las tres especies de iris.\n\nxs = transformed[:,0]\nys = transformed[:,1]\nplt.scatter(xs, ys,c = species_)\nplt.show()\n\n\n\n\n\n\n\n\nNotablemente, a pesar de haberse reducido la dimensión de 4 a 2, aún es posible distinguir las especies. Recuerde que PCA ni siquiera sabía que existían especies distintas. PCA simplemente tomó las 2 características de PCA con mayor variación.\nComo podemos ver estas dos características son muy informativas.\n\nEl PCA descarta las características de baja varianza y supone que las características de mayor varianza son informativas.\n\nComo sucede con todos los supuestos, existen casos donde esto no se cumple. Sin embargo, como vimos con el conjunto de datos del iris, en la práctica suele suceder esto. En algunos casos, es necesario utilizar una implementación alternativa de PCA.\n\nMatrices de frecuencia de palabras\nLas matrices de frecuencia de palabras son un gran ejemplo de una implementación alternativa de PCA.\n\n\n\n\n\nEn una matriz de frecuencia de palabras, cada fila corresponde a un documento y cada columna corresponde a una palabra de un vocabulario fijo. Las entradas de la matriz miden la frecuencia con la que aparece cada palabra en cada documento. Sólo algunas de las palabras del vocabulario aparecen en cualquier documento, por lo que la mayoría de las entradas de la matriz de frecuencia de palabras son cero.\nSe dice que las matrices de palabras como esta son “dispersas” y, a menudo, representado utilizando un tipo especial de matriz llamada \"csr_matrix\".\n\ncsr_matrix ahorra espacio al recordar solo las entradas distintas de cero de la matriz.\n\n\n\nTruncatedSVD & csr_matrix\nEl PCA de Scikit-Learn no admite csr_matrix y necesitará utilizar TruncatedSVD en su lugar.\nTruncatedSVD realiza la misma transformación que PCA, pero acepta matrices csr_matrix como entrada.\nfrom sklearn.decomposition import TruncatedSVD\nmodel = TruncatedSVD(n_components = 3)\nmodel.fit(documents)\ntransformed = model.transform(documents)"
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#factorización-de-matrices-no-negativas-nmf",
    "href": "posts/Unsupervised_Learning/index.html#factorización-de-matrices-no-negativas-nmf",
    "title": "Aprendizaje No Supervisado",
    "section": "Factorización de matrices no negativas (NMF)",
    "text": "Factorización de matrices no negativas (NMF)\nNMF significa “factorización matricial no negativa” . NMF, como PCA, es una técnica de reducción de dimensión. Sin embargo, a diferencia del PCA, los modelos NMF son interpretables. Esto significa que los modelos NMF son más fáciles de entender por uno mismo y mucho más fáciles de explicar a los demás. Sin embargo, el NMF no se puede aplicar a todos los conjuntos de datos. Se requiere que las características de la muestra sean “no negativas”, es decir mayor o iguales a 0.\nNMF logra su interpretabilidad al descomponer las muestras como sumas de sus partes. Por ejemplo, NMF descompone los documentos como combinaciones de temas comunes e e imagenes como combinaciones de patrones comunes.\n\n\n\n\n\nAprenderemos más sobre este ejemplo en detalle más adelante.\n\nNMF en Scikit - Learn\nNMF esta disponible en scikit learn y sigue el mismo patrón de ajuste fit()/transformación transform() que PCA. Sin embargo, a diferencia del PCA, siempre se debe especificar el número deseado de componentes NMF(n_components=2).\nNMF funciona tanto con matrices numpy con matrices dispersas en el formato csr_matrix.\nVeamos una aplicación de NMF a un ejemplo de juguete de una matriz de frecuencia de palabras.\nEn este conjunto de datos de juguetes, solo hay 4 palabras en el vocabulario, y estos corresponden a las cuatro columnas de la matriz de frecuencia de palabras.\n\n\n\n\n\nCada fila representa un documento y las entradas de la matriz miden la frecuencia de cada palabra en el documento utilizando lo que se conoce como “tf-idf”.\n\n“tf” es la frecuencia de la palabra en el documento. Entonces, si el 10% de las palabras del documento son “datacamp”, entonces el tf de “datacamp” para ese documento es el punto 1.\n“idf” es un esquema de ponderación que reduce la influencia de palabras frecuentes como “the”.\n\nfrom sklearn.decomposition import NMF\nmodel = NMF(n_components = 2)\nmodel.fit()\n\nnmf_features = model.transform(samples)\n\nprint(model.components_)\n\n\n\n\n\nAsí como el PCA tiene componentes principales, el NMF tiene componentes que aprenden de las muestras, y al igual que con PCA, la dimensión de los componentes es la misma que la dimensión de las muestras. En nuestros ejemplo, hay 2 componentes y viven en un espacio de 4 dimensiones, correspondientes a las 4 palabras del vocabulario.\nLas entradas de los componentes NMF son siempre no negativas.\nprint(nmf_features)\n\n\n\n\n\nLos valores de las características NMF también son no negativos. Como vimo con PCA, nuestros datos transformados en este ejemplo tendrán dos columnas, correspondientes a nuestras dos nuevas características.\nLas características y los componentes de un modelo NMF se pueden combinar para reconstruir aproximadamente las muestras de datos originales.\n\n\nReconstrucción de una muestra\nVeamos cómo funciona esto con una única muestra de datos. Aquí hay una muestra que representa un documento de nuestro conjunto de datos de juguetes:\nprint(samples[i,:])\n\n\n\n\n\ny podemos visualizar sus valores de características NMF:\nprint(nmf_features[i,:])\n\n\n\n\n\nAhora, si multiplicamos cada componente NMF por la característica NMF correspondiente y sumamos cada columna, obtenemos algo muy cercano a la muestra original.\n\n\n\n\n\nDe esta manera, una muestra se puede reconstruir multiplicando el componenten NMF por los valores característicos NMF de la muestra y sumando. Este cálculo también se puede expresar como lo que se conoce como un producto de matrices. No utilizaremos ese punto de vista, pero de ahí proviene la “factorización matricial”, o “MF”, en NMF.\n\nPor último, recuerde que NMF solo se puede aplicar a matrices de datos no negativos, como matrices de frecuencia de palabras.\n\nEn la siguiente sección, construiremos otro ejemplo codificando colecciones de imagenes como matrices no negativas. También hay muchos otros grandes ejemplos, como las matrices que codifican audio, espectrogramas y matrices que representan los historiales de compras en sitios de comercio electrónico. Por los momentos, en esta sección aprendimos los conceptos básicos de NMF."
  },
  {
    "objectID": "posts/Unsupervised_Learning/index.html#nmf-aprende-partes-interpretables",
    "href": "posts/Unsupervised_Learning/index.html#nmf-aprende-partes-interpretables",
    "title": "Aprendizaje No Supervisado",
    "section": "NMF Aprende partes interpretables",
    "text": "NMF Aprende partes interpretables\nAprendamos ahora que los componentes de NMF representan patrones que ocurren con frecuencia en las muestras.\nConsideremos un ejemplo concreto, donde los artículops científicos están representados por sus frecuencias de palabras.\n\n\n\n\n\nHay 20,000 artículos y 800 palabras. Entonces la matriz tiene 800 columnas.\nAjustemos un modelo NMF con 10 componentes a los artículos.\nprint(articles.shape)\n\n\n\n\n\nnmf = NMF(n_components=10)\nnmf.fit(articles)\n\n\n\n\n\nprint(nmf.components=10)\n\n\n\n\n\nLos 10 componentes se almancenan como las 10 filas de una matriz numpy bidimensional.\nLas filas, o componentes, viven en un espacio de 800 dimensiones: hay una dimensión para cada una de las palabras.\n\n\n\n\n\nAlinear las palabras de nuestro vocabulario con las columnas de los componentes del NMF permite interpretarlas.\n\n\n\n\n\nAl elejir un componente, como este, y observar qué palabras tienen los valores más altos, que se ajustan a un tema: las palabras son “species”, “plant”, “genetic”, “evolution”, “life”\n\n\n\n\n\nLo mismo ocurre si se considera cualquier otro componente:\n\n\n\n\n\n\nComponentes NMF\nSi se aplica NMF a los documentos, los componentes corresponden a temas, y las funciones de NMF reconstruyen los documentos a partir de los temas.\nPor otro lado, si se aplica NMF a una colección de imágenes, entonces\n\n\n\n\n\nLos componentes NMF representan patrones que ocurren con frecuencia en las imágenes. En este ejemplo, por ejemplo, NMF descompone las imagénes de una pantalla LCD en las celdas individuales de la pantalla.\nNecesitaremos saber cómo representar una coleccíon de imágenes como una matriz no negativas.\nUna imagen en la que todos los píxeles son tonos de gris que van del negro al blanco se denomina “imagen en escala de grises” . Como solo hay tonos de gris, una imagen en escala de grises se puede codificar por el brilo de cada píxel. Representando el brillo como un número entre 0 y 1, donde 0 es totalmente negro y 1 es totalmente blanco, la imagen se puede representar como una matriz bidimensional de números.\n\n\n\n\n\nAquí, por ejemplo, hay una fotografía en escala de grises de la luna!\n\n\n\n\n\nEstas matrices bidimensionales de números se pueden aplanar luego enumerando las entradas. Por ejemplo, podríamos leer los valores por fila, de izquierda a derecha y de arriba a abajo. La imagen en escala de grises ahora está representada por una matriz plana de números no negativos.\n\n\n\n\n\nDe este modo, se puede crear una colección de imágenes en escala de grises del mismo tamaño, codificado como una matriz bidimensional, en la que cada fila representa una imagen como una matriz aplanada y cada columna representa un píxel.\n\n\n\n\n\nAl observar las imágenes como muestras y los píxeles como características, podemos observar que los datos están organizados de manera similar a la matriz de frecuencia de palabras. De hecho, las entradas de esta matriz no son negativas, por lo que NMF se puede utilizar para aprender las partes de las imágenes."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html",
    "href": "posts/PDs_Bayesianas/index.html",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "",
    "text": "Este artículo explora en profundidad el Teorema de Bayes y su aplicación práctica en el Scoring Crediticio moderno. A través de un análisis matemático riguroso y ejemplos financieros concretos, examinaremos cómo esta potente herramienta estadística permite a los analistas de crédito actualizar sus evaluaciones de riesgo cuando disponen de nueva información sobre los clientes. El contenido abarca desde los fundamentos teóricos del teorema hasta su implementación en modelos de decisión crediticia, incluyendo casos prácticos y desarrollos matemáticos detallados."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#interpretación-financiera-del-teorema-de-bayes",
    "href": "posts/PDs_Bayesianas/index.html#interpretación-financiera-del-teorema-de-bayes",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Interpretación Financiera del Teorema de Bayes",
    "text": "Interpretación Financiera del Teorema de Bayes\nEn el contexto del anáisis crediticio, el Teorema de Bayes proporciona un marco matemático riguroso para actualizar nuesra evaluación del riesgo asociado a un cliente a medida que obtenemos nueva información sobre él. Esta metodología permite pasar de estimaciones generales basadas en datos históricos agregados a evaluaciones personalizadas que reflejan las características específicas de cada solicitante.\nLa interpretación financiera de los componentes del teorema en el contexto del scoring crediticio es la siguiente:\n\n\nLa probabilidad a priori: representa nuestra estimación inicial del riesgo de incumplimiento de un cliente basado en datos históricos generales antes de considerar sus características específicas.\nLa verosimilitud cuantifica la relación entre una características específica del cliente y su comportamiento de pago histórico.\nLa probabilidad a posteriori: es la estimación actualizada del riesgo de incumplimiento después de considerar las características específicas del cliente.\n\nEste enfoque permite a las instituciones financieras actualizar continuamente sus evaluaciones de riesgo a medida que recopilan más información sobre sus clientes, ya sea durante el proceso de solicitud inicial o a lo largo de la relación comercial. La ventaja principal es que cada nueva pieza de información se incorpora de manera matemáticamente coherente, lo que resulta en estimaciones de riesgo cada vez más precisas y personalizadas."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#componentes-matemáticos-en-el-scoring-crediticio",
    "href": "posts/PDs_Bayesianas/index.html#componentes-matemáticos-en-el-scoring-crediticio",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Componentes Matemáticos en el Scoring Crediticio",
    "text": "Componentes Matemáticos en el Scoring Crediticio\n\nDefinición de eventos\nIdentificamos los eventos clave en nuestro análisis:\n\nA = “cliente es buen pagador”\nno-A = “cliente es mal pagador (default)”\nTambién definimos B como la nueva evidencia o característica observada.\n\n\n\nProbabilidad a priori\nEstablecemos \\(P(A)\\) y \\(P(no-A)\\) basándonos en datos históricos de nuestra cartera. Por ejemplo, si históricamente el 95% de los clientes pagan a tiempo, entonces \\(P(A) = 0.95\\) y \\(P(no-A) = 0.05\\).\n\n\nActualización bayesiana\nAplicamos el Teorema de Bayes para calcular \\(P(A|B)\\) y \\(P(no-A|B)\\), actualizando nuestra estimación del riesgo de incumplimiento a la luz de la nueva evidencia.\n\n\nAnálisis de verosimilitud\nCalculamos \\(P(B|A)\\) y \\(P(B|no-A)\\) analizando cómo la característica B se distribuye entre buenos y malos pagadores en nuestros datos históricos.\nEl cálculo de la probabilidad marginal \\(P(B)\\) se realiza mediante la ley de probabilidad total:\n\\[\nP(B) = P(B|A)\\cdot P(A) + P(B|no-A)\\cdot P(no-A)\n\\]\nEsta fórmula nos permite determinar la probabilidad total de observar la característica B en toda nuestra cartera de clientes, independientemente de si son buenos o malos pagadores. Este valor actúa como un factor de normalización en el Teorema de Bayes.\nLa aplicación rigurosa de estos componentes matemáticos permite a los analistas de crédito desarrollar modelos de scoring sofisticados que van más allá de las reglas fijas y heurísticas, incorporando un enfoque probabilístico fundamentado en principios estadísticos sólidos. Esto resulta en una evaluación de riesgo más precisa y matizada que se adapta continuamente a medida que se dispone de nueva información."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#ventajas-del-enfoque-bayesiano-en-análisis-crediticio",
    "href": "posts/PDs_Bayesianas/index.html#ventajas-del-enfoque-bayesiano-en-análisis-crediticio",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Ventajas del Enfoque Bayesiano en Análisis Crediticio",
    "text": "Ventajas del Enfoque Bayesiano en Análisis Crediticio\nEl enfoque bayesiano ofrece numerosas ventajas significativas para las instituciones financieras en comparación con los métodos tradicionales de evaluación crediticia. Estas ventajas se traducen directamente en mejores resultados financieros y operativos.\n\nEvaluación dinámica del riesgo\nPermite actualizar las estimaciones de riesgo de manera continua a medida que se dispone de nueva información, en lugar de depender exclusivamente de evaluaciones estáticas realizadas en el momento de la solicitud.\n\n\nIncorporación de conocimiento experto\nFacilita la integración formal del juicio experto a través de las probabilidades a priori, combinando la experiencia humana con el análisis de datos de manera matemáticamente coherente.\n\n\nTransparencia en la toma de decisiones\nProporciona un marco explícito para la actualización de creencias, lo que aumenta la transparencia y facilita la explicación de las decisiones crediticiasa reguladores y clientes.\n\n\nGestión eficiente de la incertidumbre\nAborda directamente la incertidumbre inherente a la evaluación crediticia, cuantificándola y gestionándola de manera sistemática.\n\n\nPersonalización granular\nPermite una segmentación más refinada de los clientes según su perfil de riesgo específico, superandolas limitaciones de las categorías predefinidas.\n\n\nAdaptabilidad a condiciones cambiantes\nFacilita la adaptación a entornos económicos cambiantes mediantes la actualización sistemática de las estimaciones de riesgo basadas en nuevas condiciones de mercado.\n\nEstas ventajas hacen que el enfoque bayesiano sea particularmente valioso en contextos financieros caracterizados por altos niveles de incertidumbre, información asimétrica y la necesidad de tomar decisiones rápidas pero fundamentadas sobre el riesgo crediticio."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#desarrollo-matemático-del-teorema-de-bayes",
    "href": "posts/PDs_Bayesianas/index.html#desarrollo-matemático-del-teorema-de-bayes",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Desarrollo Matemático del Teorema de Bayes",
    "text": "Desarrollo Matemático del Teorema de Bayes\nEl desarrollo matemático completo del Teorema de Bayes se fundamenta en los axiomas básicos de la teoría de la probabilidad. A continuación, presentamos una derivación rigurosa que muestra cómo surge naturalmente de la definición de probabilidad condicional.\nComenzamos con la definición de probabilidad condicional para dos eventos A y B:\n\\[\nP(A|B) = \\frac{P(A\\cap B)}{P(B)}\n\\]\nDe manera similar, podemos expresar:\n\\[\nP(B|A) = \\frac{P(A\\cap B)}{P(A)}\n\\]\nDespejando \\(P(A\\cap B)\\) de la segunda ecuación:\n\\[\nP(A\\cap B) = P(B|A)\\cdot P(A)\n\\]\nSustituyendo esta expresión en la primera ecuación:\n\\[\nP(A|B) = \\frac{P(B|A)\\cdot P(A)}{P(B)}\n\\]\nPara calcular \\(P(B)\\), podemos utilizar la ley de probabilidad total, que establece que para una participación completa del espacio muestral (A y no-A):\n\\[\nP(B) = P(B|A)\\cdot P(A) + P(B|no-A)\\cdot P(no-A)\n\\]\nDonde \\(P(no-A) = 1 - P(A)\\). Sustituyendo esta expresión en la fórmula anterior, obtenemos la forma completa del Teorema de Bayes:\n\\[\nP(A|B) = \\frac{P(B|A)\\cdot P(A)}{P(B|A)\\cdot P(A) + P(B|no-A)\\cdot P(no-A)}\n\\]\nEsta formulación matemática rigurosa proporciona la base para todas las aplicaciones del Teorema de Bayes en el análisis crediticioy otros campos, permitiendo actualizar sistemáticamente nuestras estimaciones de probabilidad a medida que obtenemos nueva información."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#aplicación-al-scoring-crediticio-marco-conceptual",
    "href": "posts/PDs_Bayesianas/index.html#aplicación-al-scoring-crediticio-marco-conceptual",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Aplicación al Scoring Crediticio: Marco Conceptual",
    "text": "Aplicación al Scoring Crediticio: Marco Conceptual\nEl scoring crediticio es un proceso mediante el cual las instituciones financieras evalúan la probabilidad de que un solicitante de crédito incumpla con sus obligaciones de pago. El enfoque bayesiano proporciona un marco conceptual sólido para este proceso, permitiendo incorporar tanto información general del mercado como características específicas del solicitante.\n\nEstablecimiento de probabilidades a priori\nSe determinan las tasas generales de incumplimiento en la cartera o segmento relevante, estableciendo P(Default) y P(No Default) basadas en datos históricos agregados.\n\n\nIdentificación de características predictivas\nSe identifican variables con capacidad predictiva (ingresos, historial crediticio, establecimiento laboral, etc.) y se cuentifica su relación con el comportamiento de pago mediante P(Característica|Default) y P(Característica|No Default).\n\n\nActualización bayesiana\nSe aplica el Teorema de Bayes para actualizar la probabilidad de incumplimiento considerando las características del solicitante, calculando P(Default|Características).\n\n\nToma de decisiones basada en umbrales\nSe compara la probabilidad actualizada con umbrales predefinidos para tomar decisiones de aprobación, rechazo o revisión adicional, optimizando el balance entre riesgo y rentabilidad.\n\nEste marco conceptual permite a las instituciones financieras evolucionar desde decisiones basadas en reglas rígidas hacia un enfoque probabilístico más flexible y adaptativo, que reconoce explícitamente la incertidumbre inherente a la evaluación crediticia y proporciona mecanismos formales para reducirla a medida que se dispone de más información."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#variables-clave-en-el-análisis-bayesiano-de-crédito",
    "href": "posts/PDs_Bayesianas/index.html#variables-clave-en-el-análisis-bayesiano-de-crédito",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Variables Clave en el Análisis Bayesiano de Crédito",
    "text": "Variables Clave en el Análisis Bayesiano de Crédito\nLa efectividad del enfoque bayesiano en el scoring crediticiodepende fundamentalmente de la selección e incorporación adecuada de variables predictivas. Estas variables actúan como la “evidencia” en el teorema de Bayes, permitiendo actulizar las estimaciones de riesgo basadas en características específicas de cada solicitante.\n\nHistorial de pagos\nEl comportamiento histórico de pagos es uno de los predictores más potentes del comportamiento futuro. Variables como días de atraso, incumplimientos previos y patrones de pago proporcionan información valiosa para la actualización bayesiana.\n\n\nCapacidad financiera\nIndicadores como nivel de ingresos, estabilidad laboral, ratio de endeudamiento y patrimonio neto ayudan a evaluar la capacidad del solicitante para cumplir con sus obligaciones financieras.\n\n\nVariables conductuales\nPatrones de comportamiento como frecuencia de transacciones, uso de productos financieros y hábitos de consumo pueden revelar información importante sobre la probabilidad de incumplimiento.\nPara cada variable, es crucial determinar empíricamente las probabilidades condicionales P(Variable|Default) y P(Variable|No Default) mediante análisis estadístico de datos históricos. Estas probabilidades condicionales cuantifican la relación entre cada variable y el comportamiento de pago, permitiendo una actualización bayesiana precisa.\nLa incorporación de múltiples variables plantea desafíos matemáticos adicionales , ya que requiere considerar las dependencias entre variables y evitar el doble conteo de información. Técnicas como las redes bayesianas y los modelos jerárquicos bayesianos proporcionan marcos formales para abordar estos desafíos y realizar actualizaciones bayesianas multivariadas de manera matemáticamente coherente."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#el-proceso-de-actualización-bayesiana-en-crédito",
    "href": "posts/PDs_Bayesianas/index.html#el-proceso-de-actualización-bayesiana-en-crédito",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "El proceso de Actualización Bayesiana en Crédito",
    "text": "El proceso de Actualización Bayesiana en Crédito\nEl proceso de actualización bayesiana en el contexto de análisis crediticio consiste en refinar progresivamente nuestra estimación del riesgo de incumplimiento a medida que obtenemos información adicional sobre el solicitante. Este proceso puede visualizarse como una serie de actualizaciones secuenciales donde cada nueva característica observada modifica nuestra evaluación previa.\n\nProbabilidad base de la población\nComenzamos con la tasa general de incumplimiento en nuestra cartera o segmento, por ejemplo, P(Default) = 0.05.\n\n\nPrimera actualización: Historial crediticio\nObservamos el historial crediticio del solicitane y actualizamos la probabilidad de incumplimiento utilizando el Teorema de Bayes: P(Default|Historial).\n\n\nSegunda actualización: Nivel de ingresos\nIncorporamos información sobre el nivel de ingresos, utilizando la probabilidad actualizada anterior como nueva probabilidad a priori: P(Default|Historial, Ingresos).\n\n\nProbabilidad final actualizada\nContinuamos este proceso para cada característica relevante, obteniendo una estimación final refinada del riesgo de incumplimiento.\nMatemáticamente, cada actualización sigue la misma estructura del Teorema de Bayes:\n\\[\nP(Default|E_n) = \\frac{P(E_n|Default)\\cdot P(Default|E_{n-1})}{P(E_n |Default)\\cdot P(Default|E_{n-1}) + P(E_n|NoDefault)\\cdot P(NoDefault|E_{n-1})}\n\\]\nDonde \\(E_n\\) representa la n-ésima característica observada y \\(P(Default|E_{n-1})\\) es la probabilidad de incumplimiento actualizada después de considerar las \\(n-1\\) características anteriores. Este enfoque secuencial permite incorporar nueva información de manera eficiente, actualizando continuamente nuestra evaluación del riesgo crediticio."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#caso-práctico-actualización-por-cuotas-atrasadas",
    "href": "posts/PDs_Bayesianas/index.html#caso-práctico-actualización-por-cuotas-atrasadas",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Caso Práctico: Actualización por Cuotas Atrasadas",
    "text": "Caso Práctico: Actualización por Cuotas Atrasadas\nPresentamos un caso práctico detallado que ilustra la aplicación del Teorema de Bayes para actualizar la evaluación de riesgo de un cliente basándose en información sobre cuotas atrasadas. Este ejemplo demuestra el proceso completo de actualización bayesiana en un contexto crediticio real.\n\nEscenario inicial\nUn banco desea actualizar la probabilidad de que un solicitante incumpla con su préstamo (sea un “mal pagador”) después de observar que tiene un historial de 3 o más cuotas atrasadas en otros créditos.\n\nDefinición de eventos\n\nM: El cliente es un mal pagador (incumplirá el préstamo).\nB: El cliente es un buen pagador (no incumplirá el préstamo).\nE: El cliente tiene un historial de 3 o más cuotras atrasadas.\n\n\n\nProbabilidades a priori\nBasados en la experiencia histórica general del banco:\n\n\\(P(M) = 0.05\\) (5% de incumplimiento histórico)\n\\(P(B) = 0.95\\) (95% de pago a tiempo histórico)\n\n\n\nProbabilidades condicionales\nDeterminadas mediante análisis de datos históricos:\n\n\\(P(E|M) = 0.70\\) (70% de los malos pagadores tuvieron 3+ cuotas atrasadas)\n\\(P(E|B) = 0.10\\) (10% de los buenos pagadores tuvieron 3+ cuotas atrasadas)\n\n\n\nCálculo de la probabilidad total\n\\[\nP(E) = P(E|M)\\cdot P(M) + P(E|B)\\cdot P(B) = (0.70\\cdot 0.05) + (0.10\\cdot 0.95) = \\fbox{0.13}\n\\]\n\n\nAplicación del Teorema de Bayes\n\\[\nP(M|E) = \\frac{P(E|M)\\cdot P(M)}{P(E)} = \\frac{0.70\\cdot 0.05}{0.13} = \\fbox{0.2692}\n\\]\nEntonces, notemos que:\n\n\nRiesgo Inicial (5%): Probabilidad a priori de incumplimiento antes de considerar el historial de cuotas atrasadas.\nRiesgo Actualizado (26.92%): Probabilidad a posteriori de incumplimiento después de considerar el historial de cuotas atrasadas.\nFactor de incremento (5.38x): Aumento relativo en la estimación de riesgo debido a la nueva información.\n\n\nEste ejemplo demuestra cómo una sola pieza de información (historial de cuotas atrasadas) puede cambiar significativamente nuestra evaluación del riesgo crediticio, pasando de un 5% a casi un 27%. Esta actualización bayesiana proporciona una base cuantitativa sólida para ajustar las decisiones crediticias, posiblemente resultando en tasas de interés más altas, limites de crédito reducidos o requisitos adicionales de garantía para este solicitante."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#implicaciones-de-la-actualización-bayesiana-en-decisiones-crediticias",
    "href": "posts/PDs_Bayesianas/index.html#implicaciones-de-la-actualización-bayesiana-en-decisiones-crediticias",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Implicaciones de la Actualización Bayesiana en Decisiones Crediticias",
    "text": "Implicaciones de la Actualización Bayesiana en Decisiones Crediticias\nLa actualización bayesiana de la probabilidad de incumplimiento tiene profundas implicaciones para la toma de decisiones crediticias. Permite a las instituciones financieras ir más allá de reglas fijas y categorías predefinidas, adoptando un enfoque más matizado y personalizado para la gestión del riesgo crediticio.\n\nDecisiones de aprobación/rechazo\nLas probabilidades de incumplimiento actualizadas pueden compararse con umbrales predefinidos para tomar decisiones automáticas de aprobación o rechazo. Por ejemplo, si \\(P(Default|Caracteristicas)&gt;20\\%\\), la solicitud podría ser rechazada automáticamente.\n\n\nFijación de tasas de interés\nLas tasas de interés pueden ajustarse en función del riesgo actualizado, siguiendo el principio de que mayor riesgo justifica un mayor retorno. Esto puede implementarse mediante fórmulas que relacionan directamente la probabilidad de incumplimiento con la prima de riesgo.\n\n\nDeterminación de límites de crédito\nLos límites de crédito pueden calibrarse inversamente proporcionales a la probabilidad de incumplimiento actualizada, garantizando que la exposición al riesgo sea coherente con la capacidad de pago estimada.\n\n\nRequisitos de garantía\nPara solicitantes con probabilidad de incumplimiento elevadas pero no prohibitivas, pueden solicitarse garantías adicionales para mitigar el riesgo, equilibrando la inclusión financiera con la gestión prudente del riesgo.\nEl enfoque bayesiano también facilita el desarrollo de estrategias de decisión adaptativas que pueden ajustarse dinámicamente a medida que cambian las condiciones del mercado o se dispone de nueva información. Esto permite una gestión más proactiva y ágil del riesgo crediticio, especialmente valiosa en entornos económicos volátiles.\nAdemás, la transparencia matemática del proceso bayesiano facilita la explicación de las decisiones crediticias tanto a reguladores como a clientes, contribuyendo a prácticas de préstamo más justas y comprensibles que pueden fortalecer la confianza en el sistema financiero."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#extensiones-matemáticas-del-modelo-bayesiano-básico",
    "href": "posts/PDs_Bayesianas/index.html#extensiones-matemáticas-del-modelo-bayesiano-básico",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Extensiones Matemáticas del Modelo Bayesiano Básico",
    "text": "Extensiones Matemáticas del Modelo Bayesiano Básico\nEl modelo bayesiano básico presentado anteriormente puede extenderse en varias direcciones para aumentar su sofisticación y capacidad predictiva. Estas extensiones incorporan aspectos más complejos de la teoría de la probabilidad y la estadística, permitiendo un análisis más matizado del riesgo crediticio.\n\nActualización multivariada\nEn lugar de actualizar secuencialmente variable por variable, podemos considerar múltiples características simultáneamente mediante vectores de características:\n\\[\nP(Default|\\vec{X}) = \\frac{P(\\vec{X}|Default)\\cdot P(Default)}{P(\\vec{X})}\n\\]\nDonde \\(\\vec{X}\\) representa un vector de características del cliente. Este enfoque requiere estimar la distribución conjunta \\(P(\\vec{X}|Default)\\), lo que puede hacerse mediante modelos paramétricos como distribuciones multivariadas o métodos no paramétricos.\n\n\nRedes bayesianas\nLas redes bayesianas permiten modelar explícitamente las dependencias entre variables mediante grafos dirigidos acíclicos:\n\\[\nP(X_1, X_2, . . . , X_n) = \\prod_{i=1}^n P(X_i|P_a(X_i))\n\\]\nDonde \\(P_a(X_i)\\) representa los “padres” de \\(X_i\\) en el grafo. Esta estructura facilita la incorporación de conocimiento experto sobre las relaciones causales entre variables y permite actualizaciones más eficientes cuando se observan subconjuntos de variables.\n\n\nModelos jerárquicos bayesianos\nEstos modelos introducen niveles adicionales de parámetros para capturar variaciones entre diferentes segmentos o grupos:\n\\[\nP(Default|\\vec{X}_i,\\theta_g) = f(\\vec{X}_i,\\theta_g),\\hspace{1cm}\\theta_g\\sim p(\\theta|\\phi)\\hspace{1cm} \\phi\\sim p(\\phi)\n\\]\nDonde \\(g\\) indica el grupo al que pertenece el cliente \\(i\\), \\(\\theta_g\\) son parámetros específicos del grupo, y \\(\\phi\\) son hiperparámetros globales. Este enfoque permite “tomar prestada fuerza” entre grupos, mejorando las estimaciones para segmentos con datos limitados.\nEstas extensiones matemáticas aumentan significativamente el poder predictivo de los modelos bayesianos, permitiendo capturar interacciones complejas entre variables y estructuras jerárquicas en los datos. Sin embargo, también introducen desafíos computacionales y requieren técnicas avanzadas como los métodos de Monte Carlo basados en cadenas de Markov (MCMC) o aproximaciones variacionales para la inferencia de parámetros."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#calibración-y-validación-de-modelos-bayesianos-de-crédito",
    "href": "posts/PDs_Bayesianas/index.html#calibración-y-validación-de-modelos-bayesianos-de-crédito",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Calibración y Validación de Modelos Bayesianos de Crédito",
    "text": "Calibración y Validación de Modelos Bayesianos de Crédito\nLa implementación efectiva de modelos bayesianos para scoring crediticio requiere procesos rigurosos de calibración y validación para garantizar su precisión y robustez. Estos procesos son fundamentales para asegurar que las probabilidades de incumplimiento estimadas reflejen adecuadamente el riesgo real.\n\nCalibración de probabilidades\nLa calibración se refiere al grado en que las probabilidades de incumplimiento estimadas coinciden con las frecuencias de incumplimiento observadas. Un modelo bien calibrado debe satisfacer:\n\\[\nP(Default|\\hat{P}(Default|X) = p) = p \\hspace{1cm} \\forall p\\in [0,1]\n\\]\nEs decir, entre todos los clientes a los que se les asigno una probabilidad de incumplimiento \\(\\%p\\), aproximadamente el \\(\\%p\\) debería incumplir realmente. La calibración puede evaluarse mediante:\n\nPruebas de Hosmer-Lemeshow: Comparan las tasas de incumplimiento observadas y esperadas en diferentes intervalos de probabilidad.\nGráficos de calibración: Representan visualmente la relación entre probabilidades predichas y frecuencias observadas.\nBrier Score: Mide el error cuadrático medio entre las probabilidades predichas y los resultados observados.\n\n\n\nValidación del poder discriminativo\nAdemás de la calibración, es crucial evaluar la capacidad del modelo para discriminar entre buenos y malos pagadores. Esto puede hacerse mediante:\n\nArea bajo la curva ROC (AUC): Mide la probabilidad de que un cliente que incumple reciba una puntación de riesgo más alta que uno que no incumple.\nIndice de GINI: Relacionado con el AUV como \\(Gini = 2\\cdot AUC -1\\), mide la concentración de malos pagadores en las puntuaciones de mayor riesgo.\nEstadística de Kolmogorov-Smirnov (K-S): Cuantifica la máximaa separación entre las distribuciones acumulativas de buenos y malos pagadores.\n\nUn modelo bayesiano bien validado debe demostrar tanto una buena calibración como un alto poder discriminativo, garantizando que las decisiones crediticias basadas en él sean tanto precisas como eficaces en la separación de clientes de alto y bajo riesgo."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#comparación-con-otros-enfoques-de-scoring-crediticio",
    "href": "posts/PDs_Bayesianas/index.html#comparación-con-otros-enfoques-de-scoring-crediticio",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Comparación con Otros Enfoques de Scoring Crediticio",
    "text": "Comparación con Otros Enfoques de Scoring Crediticio\nEl enfoque bayesiano para el scoring crediticio coexiste con otros métodos y de aprendizaje automático. Cada metodología tiene sus propias fortalezas y limitaciones, y la elección entre ellas depende de los objetivos específicos, restricciones y recursos disponibles.\n\nModelo de Regresión Logística\n\nFortalezas: Simplicidad, interpretabilidad, eficiencia computacional, buenos resultados en la práctica.\nLimitaciones: Asume relaciones lineales, sensible a valores atípicos, no incorpora naturalmente conocimiento previo.\nComparación bayesiana: Los modelos bayesianos pueden verse como una extensión que incorpora conocimiento previo y cuantifica la incertidumbre en los parámetros.\n\n\n\nArboles de Decisión y Random Forest\n\nFortalezas: Capturan interacciones no lineales, robustos ante datos faltantes, manejan naturalmente variables categóricas.\nLimitaciones: Pueden sobreajustar, las probabilidades pueden no estar bien calibradas, “cajas negras” relativas.\nComparación bayesiana: Los modelos bayesianos ofrecen mejor cuantificación de la incertidumbre pero pueden ser menos flexibles para capturar relaciones muy complejas.\n\n\n\nRedes Neuronales\n\nFortalezas: Alta capacidad predictiva, capturan patrones complejos, adaptables a diversos tipos de datos.\nLimitaciones: Baja interpretabilidad, requieren grandes volúmenes de datos, costosas computacionalmente.\nComparación bayesiana: Los modelos bayesianos son generalmente más interpretables y requieren menos datos, pero pueden tener menor capacidad predictiva en problemas muy complejos.\n\nLos enfoques híbridos que combinan las fortalezas de múltiples metodologías están ganando popularidad. Por ejemplo, las redes bayesianas profundas integran los principios bayesianos con las arquitecturas de aprendizaje profundo, ofreciendo tanto capacidad predictiva como cuantificación de la incertidumbre. En la práctica, muchas instituciones financieras implementan múltiples modelos en paralelo o en forma secuencial, aprovechando las ventajas complementarias de diferentes enfoques."
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#implementación-práctica-en-sistemas-de-decisión-crediticia",
    "href": "posts/PDs_Bayesianas/index.html#implementación-práctica-en-sistemas-de-decisión-crediticia",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Implementación Práctica en Sistemas de Decisión Crediticia",
    "text": "Implementación Práctica en Sistemas de Decisión Crediticia\nLa implementación de modelos bayesianos en sistemasa operativos de decisión crediticia plantea desafíos técnicos y organizacionales significativos. A continuación, se presenta una hoja de ruta para la implementación efectiva de estos modelos en entornos de producción.\n\nLa implementación técnica debe considerar varios aspectos prácticos:\n\nEficiencia computacional\nPara aplicaciones en tiempo real, es crucial optimizar los cálculos bayesianos. Esto puede lograrse mediante:\n\nPrecálculo de probabilidades condicionales para combinaciones comunes de características\nImplementación de aproximaciones eficientes para modelos complejos\nUso de arquitecturas distribuidas para cálculos paralelos\n\n\n\nGestión de datos faltantes\nLos modelos bayesianos deben manejar elegantemente situaciones donde faltan datos del solicitante, ya sea:\n\nMarginalizando sobre variables no observadas\nImplementando modelos de imputación bayesianos\nUtilizando estructuras de dependencia condicional para inferir información faltante\n\n\n\nIntegración con sistemas existentes\nLa mayoría de las instituciones financieras tienen sistemas legados que deben integrarse con nuevos modelos bayesianos:\n\nDesarrollo de APIs para comunicación entre sistemas\nConversión entre diferentes representaciones de riesgo\nMecanismos de transición gradual entre modelos antiguos y nuevos\n\n\n\nCumplimiento regulatorio\nLos modelos bayesianos deben satisfacer requisitos regulatorios cada vez más estrictos:\n\nDocumentación detallada de metodologías y supuestos\nMecanismos de explicabilidad para decisiones individuales\nPruebas de equidad y no discriminación en las decisiones generadas"
  },
  {
    "objectID": "posts/PDs_Bayesianas/index.html#desafíos-en-la-aplicación-del-teorema-de-bayes",
    "href": "posts/PDs_Bayesianas/index.html#desafíos-en-la-aplicación-del-teorema-de-bayes",
    "title": "Teorema de Bayes Aplicado al Credit Scoring",
    "section": "Desafíos en la Aplicación del Teorema de Bayes",
    "text": "Desafíos en la Aplicación del Teorema de Bayes\nA pesar de sus sólidos fundamentos teóricos y ventajas prácticas, la aplicación del Teorema de Bayes en el scoring crediticio enfrente varios desafíos significativos que deben ser abordados para una implementación efectiva.\n\nAbordar estos desafíos requiere una combinación de rigor matemático, conocimiento del dominio financiero y pragmatismo en la implementación. Los avances en estadística computacional y el creciente poder de procesamiento están facilitando superar muchas de estas limitaciones, ampliando el alcance práctico de los modelos bayesianos en el scoring crediticio."
  }
]