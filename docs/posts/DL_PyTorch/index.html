<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Juan Isaula">
<meta name="dcterms.date" content="2025-07-01">

<title>Deep Learning – Juan Isaula</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-b4960eac7a6f9fac35b610a7dc5af25a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Deep Learning – Juan Isaula">
<meta property="og:description" content="PyTorch">
<meta property="og:image" content="https://j-isaula.github.io/Web_JI/posts/DL_PyTorch/fondo.webp">
<meta property="og:site_name" content="Juan Isaula">
<meta property="og:locale" content="en_US">
<meta name="twitter:title" content="Deep Learning – Juan Isaula">
<meta name="twitter:description" content="PyTorch">
<meta name="twitter:image" content="https://j-isaula.github.io/Web_JI/posts/DL_PyTorch/fondo.webp">
<meta name="twitter:creator" content="@isaula_im">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Juan Isaula</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers"> 
<span class="menu-text">Articles</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cursos_impartidos"> 
<span class="menu-text">UNAH Courses</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Dasboard"> 
<span class="menu-text">Dashboards</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Sobre_mi"> 
<span class="menu-text">Historias</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introducción-a-pytorch-biblioteca-de-deep-learning" id="toc-introducción-a-pytorch-biblioteca-de-deep-learning" class="nav-link active" data-scroll-target="#introducción-a-pytorch-biblioteca-de-deep-learning">Introducción a PyTorch (biblioteca de Deep Learning)</a>
  <ul class="collapse">
  <li><a href="#qué-es-deep-learning" id="toc-qué-es-deep-learning" class="nav-link" data-scroll-target="#qué-es-deep-learning">Qué es Deep Learning?</a></li>
  <li><a href="#pytorch-un-framework-del-deep-learning" id="toc-pytorch-un-framework-del-deep-learning" class="nav-link" data-scroll-target="#pytorch-un-framework-del-deep-learning">PyTorch: un framework del deep learning</a>
  <ul class="collapse">
  <li><a href="#atributos-de-los-tensores" id="toc-atributos-de-los-tensores" class="nav-link" data-scroll-target="#atributos-de-los-tensores">Atributos de los Tensores</a></li>
  </ul></li>
  <li><a href="#redes-neuronales-y-capas" id="toc-redes-neuronales-y-capas" class="nav-link" data-scroll-target="#redes-neuronales-y-capas">Redes Neuronales y Capas</a></li>
  <li><a href="#pesos-weights-y-sesgos-biases" id="toc-pesos-weights-y-sesgos-biases" class="nav-link" data-scroll-target="#pesos-weights-y-sesgos-biases">Pesos (weights) y Sesgos (biases)</a></li>
  <li><a href="#capaz-y-parámetros-ocultos" id="toc-capaz-y-parámetros-ocultos" class="nav-link" data-scroll-target="#capaz-y-parámetros-ocultos">Capaz y Parámetros Ocultos</a>
  <ul class="collapse">
  <li><a href="#apilamiento-de-capaz-con-nn.sequential" id="toc-apilamiento-de-capaz-con-nn.sequential" class="nav-link" data-scroll-target="#apilamiento-de-capaz-con-nn.sequential">Apilamiento de capaz con nn.Sequential()</a></li>
  <li><a href="#adición-de-capas" id="toc-adición-de-capas" class="nav-link" data-scroll-target="#adición-de-capas">Adición de capas</a></li>
  <li><a href="#las-capas-están-hechas-de-neuronas" id="toc-las-capas-están-hechas-de-neuronas" class="nav-link" data-scroll-target="#las-capas-están-hechas-de-neuronas">Las capas están hechas de neuronas</a></li>
  <li><a href="#parámetros-y-capacidad-del-modelo" id="toc-parámetros-y-capacidad-del-modelo" class="nav-link" data-scroll-target="#parámetros-y-capacidad-del-modelo">Parámetros y Capacidad del Modelo</a></li>
  <li><a href="#balance-entre-complejidad-y-eficiencia" id="toc-balance-entre-complejidad-y-eficiencia" class="nav-link" data-scroll-target="#balance-entre-complejidad-y-eficiencia">Balance entre complejidad y eficiencia</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#hiperparámetros-y-arquitectura-de-redes-neuronales" id="toc-hiperparámetros-y-arquitectura-de-redes-neuronales" class="nav-link" data-scroll-target="#hiperparámetros-y-arquitectura-de-redes-neuronales">Hiperparámetros y arquitectura de redes neuronales</a>
  <ul class="collapse">
  <li><a href="#funciones-de-activación" id="toc-funciones-de-activación" class="nav-link" data-scroll-target="#funciones-de-activación">Funciones de Activación</a>
  <ul class="collapse">
  <li><a href="#función-sigmoid" id="toc-función-sigmoid" class="nav-link" data-scroll-target="#función-sigmoid">Función Sigmoid</a></li>
  <li><a href="#función-softmax" id="toc-función-softmax" class="nav-link" data-scroll-target="#función-softmax">Función Softmax</a></li>
  </ul></li>
  <li><a href="#paso-hacia-adelante" id="toc-paso-hacia-adelante" class="nav-link" data-scroll-target="#paso-hacia-adelante">Paso hacia adelante</a>
  <ul class="collapse">
  <li><a href="#qué-es-una-paso-hacia-adelante-forward-pass" id="toc-qué-es-una-paso-hacia-adelante-forward-pass" class="nav-link" data-scroll-target="#qué-es-una-paso-hacia-adelante-forward-pass">Qué es una paso hacia adelante (Forward Pass)?</a></li>
  <li><a href="#clasificación-multi-class-forward-pass" id="toc-clasificación-multi-class-forward-pass" class="nav-link" data-scroll-target="#clasificación-multi-class-forward-pass">Clasificación Multi-Class: Forward Pass</a></li>
  <li><a href="#regresión-forward-pass" id="toc-regresión-forward-pass" class="nav-link" data-scroll-target="#regresión-forward-pass">Regresión: Forward Pass</a></li>
  </ul></li>
  <li><a href="#funciones-de-pérdida-para-evaluar-las-predicciones-del-modelo" id="toc-funciones-de-pérdida-para-evaluar-las-predicciones-del-modelo" class="nav-link" data-scroll-target="#funciones-de-pérdida-para-evaluar-las-predicciones-del-modelo">Funciones de Pérdida para Evaluar las Predicciones del Modelo</a>
  <ul class="collapse">
  <li><a href="#función-de-pérdida-cross-entropy-en-pytorch" id="toc-función-de-pérdida-cross-entropy-en-pytorch" class="nav-link" data-scroll-target="#función-de-pérdida-cross-entropy-en-pytorch">Función de Pérdida Cross Entropy en PyTorch</a></li>
  </ul></li>
  <li><a href="#utilizar-derivadas-para-actualizar-los-parámetros-del-modelo" id="toc-utilizar-derivadas-para-actualizar-los-parámetros-del-modelo" class="nav-link" data-scroll-target="#utilizar-derivadas-para-actualizar-los-parámetros-del-modelo">Utilizar derivadas para Actualizar los Parámetros del Modelo</a>
  <ul class="collapse">
  <li><a href="#funciones-convexas-y-no-convexas" id="toc-funciones-convexas-y-no-convexas" class="nav-link" data-scroll-target="#funciones-convexas-y-no-convexas">Funciones Convexas y No-Convexas</a></li>
  <li><a href="#conexión-de-derivadas-y-entrenamiento-de-modelos" id="toc-conexión-de-derivadas-y-entrenamiento-de-modelos" class="nav-link" data-scroll-target="#conexión-de-derivadas-y-entrenamiento-de-modelos">Conexión de derivadas y entrenamiento de modelos</a></li>
  <li><a href="#backpropagation-retropropagación" id="toc-backpropagation-retropropagación" class="nav-link" data-scroll-target="#backpropagation-retropropagación">Backpropagation (Retropropagación)</a></li>
  <li><a href="#actualizar-manualmente-los-parámetros-del-modelo" id="toc-actualizar-manualmente-los-parámetros-del-modelo" class="nav-link" data-scroll-target="#actualizar-manualmente-los-parámetros-del-modelo">Actualizar Manualmente los Parámetros del Modelo</a></li>
  <li><a href="#gradiente-descendente" id="toc-gradiente-descendente" class="nav-link" data-scroll-target="#gradiente-descendente">Gradiente Descendente</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Deep Learning</h1>
<p class="subtitle lead">PyTorch</p>
  <div class="quarto-categories">
    <div class="quarto-category">Python</div>
    <div class="quarto-category">PyTorch</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Juan Isaula </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>El Deep Learning está en todas partes, desde las cámaras de los smartphones hasta los asistentes de vos o los vehículos autónomos. En este curso, descubriras esta potente tecnología y aprenderás a aprovecharla con <code>PyTorch</code>, una de las bibliotecas de aprendizaje profundo más populares. Al finalizar tu recorrido por este documento, serás capaz de aprovechar PyTorch para resolver problemas de clasificación y regresión utilizando el aprendizaje profundo.</p>
<section id="introducción-a-pytorch-biblioteca-de-deep-learning" class="level1">
<h1>Introducción a PyTorch (biblioteca de Deep Learning)</h1>
<p>Antes de comenzar a crear modelos complejos, te haré conocer PyTorch, un librería de aprendizaje profundo. Aprenderás a manipular tensores, crear estructuras de datos de PyTorch y construir tu primera red neuronal en PyTorch con capas lineales.</p>
<p>El Deep Learning impulsa muchas innovaciones recientes y emocionantes, tales como la <em>traducción de idiomas</em>, <em>coches autónomos</em>, <em>diagnósticos médicos y chatbots.</em></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
<section id="qué-es-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="qué-es-deep-learning">Qué es Deep Learning?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="200"></p>
</figure>
</div>
<p>Deep Learning (aprendizaje profundo) es un subconjunto del aprendizaje automático (machine learning). La estructura del modelo es una red de entradas (input), capas ocultas (hidden layers) y salidas (output), como se muestra en la siguiente imagen:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="200"></p>
</figure>
</div>
<p>Como apreciamos en la figura, una red puede tener una o muchas capas ocultas</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="250"></p>
</figure>
</div>
<p>La intuición original detrás del aprendizaje profundo era crear modelos inspirados en el cerebro humano, sobre todo por cómo aprende el cerebro humano: a través de células interconectadas llamadas neuronas. Es por esto que llamamos a los modelos de aprendizaje profundo <strong><code>Redes Neuronales</code></strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="150"></p>
</figure>
</div>
<p>Estas estructuras de modelos en capas requieren muchos más datos en comparación con otros modelos de aprendizaje automático para derivar patrones. Generalmente hablamos de al menos cientos de miles de puntos de datos.</p>
</section>
<section id="pytorch-un-framework-del-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-un-framework-del-deep-learning">PyTorch: un framework del deep learning</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig6.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="180"></p>
</figure>
</div>
<p>Si bien existen varios framework y paquetes para implementar el aprendizaje profundo en cuanto a algoritmos, nos centraremos en PyTorch, uno de los frameworks más populares y mejor mantenidos. <em>PyTorch fue desarrollado originalmente por Meta IA como parte del laboratorio de investigación de inteligencia artificial de Facebook antes de que pasara a depender de la fundación Linux.</em></p>
<p>Está diseñado para ser intuitivo y fácil de usar, compartiendo muchas similitudes con la biblioteca de Python NumPy.</p>
<section id="pytorch-tensors" class="level4">
<h4 class="anchored" data-anchor-id="pytorch-tensors">PyTorch Tensors</h4>
<p>Podemos importar el módulo PyTorch llamando a</p>
<div id="3e46b39e" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p>La estructura de datos fundamental en PyTorch es un tensor, que es similar a una matriz.</p></li>
<li><p>Puede soportar muchas operaciones matemáticas y constituye un componente básico para nuestras redes neuronales.</p></li>
<li><p>Se pueden crear tensores a partir de listas de Python o matrices NumPy utilizando la clase <code>torch.tensor()</code> esta clase convierte los datos a un formato compatible para el aprendizaje profundo.</p></li>
</ul>
<div id="6a16d412" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>mi_lista <span class="op">=</span> [[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>], [<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>]]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>tensor <span class="op">=</span> torch.tensor(mi_lista)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])</code></pre>
</div>
</div>
</section>
<section id="atributos-de-los-tensores" class="level3">
<h3 class="anchored" data-anchor-id="atributos-de-los-tensores">Atributos de los Tensores</h3>
<p>Podemos llamar a <code>tensor.shape</code> para mostrar la forma de nuestro objeto recién creado.</p>
<div id="30a7afb8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tensor.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([2, 3])</code></pre>
</div>
</div>
<p>Y <code>tensor.dtype()</code> para mostrar su tipo de datos, aquí un entero de 64 bits.</p>
<div id="481ff2e6" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tensor.dtype)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.int64</code></pre>
</div>
</div>
<p>Verificar la forma y el tipo de datos garantiza que los tensores se alineen correctamente con nuestro modelo y tarea, y puede ayudarnos en caso de depuración.</p>
<section id="operaciones-con-tensores" class="level4">
<h4 class="anchored" data-anchor-id="operaciones-con-tensores">Operaciones con Tensores</h4>
<p>Se pueden sumar o restar tensores de PyTorch, siempre que sus formas sean compatibles.</p>
<div id="4889b8de" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([[<span class="dv">1</span>,<span class="dv">1</span>], [<span class="dv">2</span>,<span class="dv">2</span>]])</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor([[<span class="dv">2</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">3</span>]])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.tensor([[<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>], [<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b8623b8e" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a <span class="op">+</span> b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[3, 3],
        [5, 5]])</code></pre>
</div>
</div>
<p>Cuando las dimensiones no son compatibles, obtendremos un error.</p>
<p>También podemos realizar la multiplicación por elemento, lo que implica multiplicar cada elemento correspondiente.</p>
<div id="60246661" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a<span class="op">*</span>b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[2, 2],
        [6, 6]])</code></pre>
</div>
</div>
<p>También esta incluida la multiplicación de matrices, que no es más que uno forma de combinar dos matrices para crear una nueva.</p>
<div id="ef06fa68" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a <span class="op">@</span> b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[ 5,  5],
        [10, 10]])</code></pre>
</div>
</div>
<p>Detras de escena, los modelos de aprendizaje profundo realizan innumerables operaciones como la suma y multiplicación para procesar datos y aprender patrones.</p>
</section>
</section>
</section>
<section id="redes-neuronales-y-capas" class="level2">
<h2 class="anchored" data-anchor-id="redes-neuronales-y-capas">Redes Neuronales y Capas</h2>
<p>Vamos a contruir nuestra primer red neuronal usando tensores de PyTorch.</p>
<p>Una red neuronal consta de capas de entrada, ocultas y de salida.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig7.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="200"></p>
</figure>
</div>
<p>La <strong>capa de entrada</strong> contiene las características del conjunto de datos,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig8.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="200"></p>
</figure>
</div>
<p>La <strong>capa de salida</strong> contiene las predicciones,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig9.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="200"></p>
</figure>
</div>
<p>Y hay <strong>capas ocultas (hidden layers)</strong> en el medio</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig10.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="200"></p>
</figure>
</div>
<p>Si bien una red puede tener cualquier cantidad de capas ocultas, comenzaremos construyendo una red sin capas ocultas donde la capa de salida es una capa lineal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig11.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="200"></p>
</figure>
</div>
<ul>
<li><p>Aquí, cada neurona de entrada se conecta a cada neurona de salida, lo que se denomina una red “totalmente conectada”.</p></li>
<li><p>Esta red es equivalente a un modelo lineal y nos ayuda a comprender los fundamentos antes de agregar complejidad.</p></li>
</ul>
<p>Usaremos el módulo <code>torch.nn</code> para construir nuestras redes. Esto hace que el código de la red sea más conciso y flexible y se importa convencionalmente como <code>nn</code>.</p>
<div id="71625d08" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Al diseñar una red neuronal, las dimensiones de las capas de entrada y salida están predefinidas.</p>
<ul>
<li><p>La cantidad de neuronas en la capa de entrada es la cantidad de características en nuestro conjunto de datos.</p></li>
<li><p>Y el número de neuronas en la capa de salida es el número de clases que queremos predecir.</p></li>
</ul>
<p>Digamos que creamos un input_tensor con forma de <span class="math inline">\(1\times 3\)</span>.</p>
<div id="ca78ab63" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> torch.tensor(</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  [[<span class="fl">0.3471</span>, <span class="fl">0.4547</span>, <span class="op">-</span><span class="fl">0.2356</span>]]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Podemos pensar en esto como una fila con tres <em>“carectísticas”</em> o <em>“neuronas”</em> .</p>
<p>A continuación, pasamos este input_tensor a una capa lineal, que aplica una función lineal para realizar predicciones.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig12.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="100"></p>
</figure>
</div>
<p>Para ello usaremos <code>nn.Linear()</code> toma dos argumentos: <code>int_features</code> es el número de características en nuestra entrada ( en este caso, tres) y <code>out_features</code> es el tamaño del tensor de salida (en este caso, dos).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig13.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="100"></p>
</figure>
</div>
<div id="e8a9757f" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>linear_layer <span class="op">=</span> nn.Linear(</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  in_features <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  out_features <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Especificar correctamente <code>in_features</code> garantiza que nuestra capa lineal pueda recibir el input_tensor.</p>
<p>Por último, pasamos input_tensor a linear_layer para generar una salida.</p>
<div id="cee709ad" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> linear_layer(input_tensor)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.4595, 0.1513]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>Tenga en cuenta que esta salida tiene dos características o neuronas debido a las <code>out_features</code> especificadas en nuestra capa lineal.</p>
<p>Cuando input_tensor se pasa a linear_layer, se realiza una operación lineal para incluir pesos y sesgos.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig14.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500"></p>
</figure>
</div>
</section>
<section id="pesos-weights-y-sesgos-biases" class="level2">
<h2 class="anchored" data-anchor-id="pesos-weights-y-sesgos-biases">Pesos (weights) y Sesgos (biases)</h2>
<p>Cada capa lineal tiene un conjunto de pesos y sesgos asociados. Estas son las cantidades clave que definen una neurona.</p>
<div id="16465933" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(linear_layer.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor([[ 0.5239,  0.1483,  0.5702],
        [-0.1302,  0.1663,  0.4756]], requires_grad=True)</code></pre>
</div>
</div>
<div id="09077201" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(linear_layer.bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor([0.3446, 0.2329], requires_grad=True)</code></pre>
</div>
</div>
<ul>
<li><p>Los pesos reflejan la importancia de diferentes características.</p></li>
<li><p>El sesgos es un término adicional que es independiente de los pesos, y proporciona a las neurona una salida de referencia.</p></li>
</ul>
<p>Al principio, la capa lineal asigna pesos y sesgos aleatorios; estos se ajustan posteriormente.</p>
<p>Imaginemos nuestra red totalmente conectada en acción.</p>
<p>Digamos que tenemos un conjunto de datos meteorológicos con tres características: <em>temperatura (temperature), humedad (humidity) y viento (wind).</em> Y queremos predecir si <em>lloverá (rain) o estará nublado (cloudy).</em></p>
<ol type="1">
<li><p>La característica humeda tendrá un peso más significativo en comparación a las demás características, ya que es un fuerte predictor de lluvia y nubes.</p></li>
<li><p>Los datos meteorológicos corresponden a una región tropical con alta probabilidad de lluvia, por lo que agrega un sesgo para tener en cuenta esta información de referencia.</p></li>
</ol>
<p>Con esta información, nuestro modelo hace una predicción.</p>
</section>
<section id="capaz-y-parámetros-ocultos" class="level2">
<h2 class="anchored" data-anchor-id="capaz-y-parámetros-ocultos">Capaz y Parámetros Ocultos</h2>
<p>Hasta ahora, hemos utilizado una capa de entrada y una capa de lineal. Ahora, agregaremos más capas para ayudar a la red a aprender patrones complejos.</p>
<section id="apilamiento-de-capaz-con-nn.sequential" class="level3">
<h3 class="anchored" data-anchor-id="apilamiento-de-capaz-con-nn.sequential">Apilamiento de capaz con nn.Sequential()</h3>
<p>Apilaremos tres capas lineales usando <code>nn.Sequential()</code>, un contenedor de PyTorch para apilar capas en secuencia. Esta red toma la entrada, la pasa a cada capa lineal en secuencia y devuelve la salida.</p>
<pre class="{bash}"><code>model = nn.Sequential(
  nn.Linear(n_features, 8),
  nn.Linear(8, 4),
  nn.Linear(4, n_classes)
)</code></pre>
<ul>
<li><p>En este caso, las capas dentro de <code>nn.Sequential()</code> son capas ocultas.</p></li>
<li><p><code>n_features</code> representa el número de características de entrada y <code>n_classes</code> representa el número de clases de salida, ambas definidas por el conjunto de datos.</p></li>
</ul>
</section>
<section id="adición-de-capas" class="level3">
<h3 class="anchored" data-anchor-id="adición-de-capas">Adición de capas</h3>
<p>Podemos añadir tantas capas ocultas como queramos.</p>
<p><img src="img/fig17.png" class="img-fluid" width="450"></p>
<p>La dimensión de cada capa coincide con la dimensión de salida de la anterior.</p>
<div id="7441f098" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">10</span>, <span class="dv">18</span>),</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">18</span>, <span class="dv">20</span>),</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">20</span>, <span class="dv">5</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>En nuestro ejemplo de tres capas, la primera capa toma 10 características y genera 18. La segunda toda 18 y genera 20. Finalmente, la tercera toma 20 y genera 5.</p>
</section>
<section id="las-capas-están-hechas-de-neuronas" class="level3">
<h3 class="anchored" data-anchor-id="las-capas-están-hechas-de-neuronas">Las capas están hechas de neuronas</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig18.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="200"></p>
</figure>
</div>
<p>Una capa está completamente conectada cuando cada neurona se vincula a todas las neuronas de la capa anterior, como se muestra en rojo en la figura.</p>
<p>Cada neurona es una capa lineal:</p>
<ul>
<li><p>realiza una operación lineal utilizando todas las neuonras de la capa anterior.</p></li>
<li><p>Por tanto, una sola neurona tiene <span class="math inline">\(N+1\)</span> parámetros que se puede aprender, siendo la dimensión de salida la capa anterior, más 1 para el sesgo.</p></li>
</ul>
</section>
<section id="parámetros-y-capacidad-del-modelo" class="level3">
<h3 class="anchored" data-anchor-id="parámetros-y-capacidad-del-modelo">Parámetros y Capacidad del Modelo</h3>
<p>Aumetar el número de capas ocultas aumenta el número total de parámetros en el modelo, también conocido como capacidad del modelo. Los modelos de mayor capacidad pueden manejar conjuntos de datos más complejos, pero su entrenamiento puede llevar más tiempo.</p>
<p>Una forma eficaz de evaluar la capacidad de un modelo es calcular su número total de parámetros.</p>
<p>Vamos a desglosarlo con una red de dos capas,</p>
<div id="59304e55" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">8</span>, <span class="dv">4</span>),</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p><strong>La primera capa</strong> tiene 4 neuronas, cada neurona tiene 8 pesos y un sesgo, lo que da como resultado 36 parámetros.</p></li>
<li><p>La segunda capa tiene 2 neuronas, cada neurona tiene 4 pesos y un sesgo, para un total de 10 parámetros.</p></li>
<li><p>Sumándolos todos, este modelo tiene 46 parámetros que se pueden aprender en total</p></li>
</ul>
<p>También podemos calcular esto en PyTorch usando el método <code>.numel()</code>. Este método devuelve el número de elementos de un tensor.</p>
<div id="fab949d7" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> parameter <span class="kw">in</span> model.parameters():</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  total <span class="op">+=</span> parameter.numel()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(total)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>46</code></pre>
</div>
</div>
</section>
<section id="balance-entre-complejidad-y-eficiencia" class="level3">
<h3 class="anchored" data-anchor-id="balance-entre-complejidad-y-eficiencia">Balance entre complejidad y eficiencia</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig19.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="400"></p>
</figure>
</div>
<p>Comprender el recuento de parámetros nos ayuda a equilibrar la complejidad y la eficiencia del modelo. Demasiados parámetros pueden dar lugar a tiempos de entrenamiento largos o sobreajuste, mientras que muy pocos pueden limitar la capacidad de aprendizaje.</p>
</section>
</section>
</section>
<section id="hiperparámetros-y-arquitectura-de-redes-neuronales" class="level1">
<h1>Hiperparámetros y arquitectura de redes neuronales</h1>
<p>Para entrenar una red neuronal en PyTorch, primero tendremos que entender componentes adicionales, como las funciones de activación y pérdida. Entonces nos daremos cuenta de que entrenar una red requiere reducir mínimo esa función de pérdida, lo que se hace calculando gradientes. Aprenderemos a utilizar estos gradientes para actualizar los parámetros de tu modelo.</p>
<section id="funciones-de-activación" class="level2">
<h2 class="anchored" data-anchor-id="funciones-de-activación">Funciones de Activación</h2>
<p>Hasta ahora hemos visto redes neuronales formadas únicamente por capas lineales.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig20.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="250"></p>
</figure>
</div>
<p>Podemos agregar <strong>no linealidad</strong> a nuestros modelos usando funciones de activación. Discutiremos dos funciones de activación:</p>
<ul>
<li><p><strong>Sigmoid</strong> para clasificación binaria y,</p></li>
<li><p><strong>Softmax</strong> para clasificación multiclase.</p></li>
</ul>
<p>Esta no linealidad permite que las redes aprendan cosas más complejas, interacciones entre entradas y objetivos que son relaciones <strong>no linealeales.</strong></p>
<p>Llamaremos a la salida de la última capa lineal la <strong>“pre-activación”.</strong> Salida, que pasaremos a funciones de activación para obtener la salida transformada.</p>
<section id="función-sigmoid" class="level3">
<h3 class="anchored" data-anchor-id="función-sigmoid">Función Sigmoid</h3>
<p>La función de activación sigmoidea se utiliza ampliamente para problemas de clasificación binaria. Digamos que estamos tratando de clasificar un animal como mamífero o no?. Tenemos tres datos: el número de extremidades, si pone huevos y si tiene pelo. Las dos últimas son variables binarias: 1 si es si, 0 si no.</p>
<p><img src="img/fig21.png" class="img-fluid" width="700"></p>
<p>Pasar la entrada a un modelo con dos capas lineales devuelve una única salida: el número 6, tal como apreciamos en la siguiente figura:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig22.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="700"></p>
</figure>
</div>
<p>Este número aún no es interpretable. <strong>Tenemos que pasar el número 6 por la función sigmoide</strong>, transformandolo en un rango que represente la probabilidad entre cero y uno.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig23.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="700"></p>
</figure>
</div>
<p>Si el resultado está más cerca de uno (mayor que 0.5), lo etiquetamos como clase uno (mamífero). Si fuese menor que 0.5 la predección sería cero (no un mamifero).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig24.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="700"></p>
</figure>
</div>
<p>Ahora, implementemos sigmoide en PyTorch.</p>
<div id="5f37ebea" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> torch.tensor([[<span class="dv">6</span>]])</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> sigmoid(input_tensor)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.9975]])</code></pre>
</div>
</div>
<p>Normalmente, <code>nn.Sigmoid()</code> se agrega como el último paso en <code>nn.Sequential()</code>, transformando automáticamente la salida de la capa lineal final.</p>
<div id="4d605437" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">6</span>, <span class="dv">4</span>), <span class="co"># Primera capa lineal </span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">4</span>, <span class="dv">1</span>), <span class="co"># Segunda capa lineal</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  nn.Sigmoid()     <span class="co"># Función de activación</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Curiosamente, una red neuronal con solo capas lineales y una activación sigmoidea se comporta como una <strong>Regresión Logística.</strong> Más adelante agregaremos más capas y activaciones para comprender realmente el verdadero potencial del Deep Learning.</p>
</section>
<section id="función-softmax" class="level3">
<h3 class="anchored" data-anchor-id="función-softmax">Función Softmax</h3>
<p>Usamos softmax, otra función de activación popular, para clasificación multiclase que implica más de dos etiquetas de clase.</p>
<p>Digamos que tenemos tres clases:</p>
<ol type="1">
<li><p>Pajaro o Bird (0)</p></li>
<li><p>Mamífero o Mammal (1)</p></li>
<li><p>Reptil o Reptile (2)</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig25.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="400"></p>
</figure>
</div>
<p>En esta red, Softmax toma una dimensión tridimensional, salida de preactivación y genera una salida de la misma forma, una por tres.</p>
<p>La salida es una distribución de probabilidad:</p>
<ul>
<li><p>Por cada elemento está entre cero y uno, y</p></li>
<li><p>los valores suman uno.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig26.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="350"></p>
</figure>
</div>
<p>Aquí, la predicción es para la segunda clase, mamíferos, que tiene la probabilidad más alta 0.842.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig27.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="350"></p>
</figure>
</div>
<p>En PyTorch, usamos <code>nn.Softmax()</code></p>
<div id="a269ee5c" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> torch.tensor(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  [[<span class="fl">4.3</span>, <span class="fl">6.1</span>, <span class="fl">2.3</span>]]</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> nn.Softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># dim = -1 indica que softmax se aplica a la última dimensión de input_tensor</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>output_tensor <span class="op">=</span> probabilities(input_tensor)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.1392, 0.8420, 0.0188]])</code></pre>
</div>
</div>
<p>Similar a sigmoide, softmax puede ser la última capa en nn.Sequential.<br>
</p>
</section>
</section>
<section id="paso-hacia-adelante" class="level2">
<h2 class="anchored" data-anchor-id="paso-hacia-adelante">Paso hacia adelante</h2>
<p>Hemos explorado tensores, redes pequeñas y funciones de activación. Ahora profundicemos en la generación de predicciones.</p>
<p>Este proceso se llama <strong>“ejecutar un paso hacia adelante”</strong> a través de una red.</p>
<section id="qué-es-una-paso-hacia-adelante-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="qué-es-una-paso-hacia-adelante-forward-pass">Qué es una paso hacia adelante (Forward Pass)?</h3>
<p>Es cuando los datos de entrada fluyen a través de una red neuronal en dirección hacia adelante, para producir resultados o predicciones, pasa a través de cada capa de red.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig28.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="350"></p>
</figure>
</div>
<p>Los calculos transforman los datos en nuevas representaciones en cada capa, que pasa a la siguiente capa hasta que se produce el resultado final.</p>
<p>El propósito del paso hacia adelante es pasar datos de entrada a través de la red y producir predicciones o resultados basados en los parámetros aprendidos del modelo, también conocidos como pesos y sesgos.</p>
<p>Este proceso es esencial tanto para el entrenamiento como para realizar nuevas predicciones.</p>
<p>El resultado final puede ser clasificaciones binarias, clasificaciones multiclase o predicciones numéricas (regresiones).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig29.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="400"></p>
</figure>
</div>
<p>Veremos un ejemplo de cada uno.</p>
<p>Digamos que tenemos datos de entrada de cinco animales, con seís características o neuronas por punto de datos.</p>
<div id="bfa6c7af" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>input_data <span class="op">=</span> torch.tensor(</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  [[<span class="op">-</span><span class="fl">0.4421</span>, <span class="fl">1.5207</span>, <span class="fl">2.0607</span>, <span class="op">-</span><span class="fl">0.3647</span>, <span class="fl">0.4691</span>, <span class="fl">0.0946</span>],</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  [<span class="op">-</span><span class="fl">0.9155</span>, <span class="op">-</span><span class="fl">0.0475</span>, <span class="op">-</span><span class="fl">1.3645</span>, <span class="op">-</span><span class="fl">0.6336</span>, <span class="op">-</span><span class="fl">1.9520</span>, <span class="op">-</span><span class="fl">0.3398</span>],</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.7406</span>, <span class="fl">1.6763</span>, <span class="op">-</span><span class="fl">0.8511</span>, <span class="fl">0.2432</span>, <span class="fl">0.1123</span>, <span class="op">-</span><span class="fl">0.0633</span>],</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  [<span class="op">-</span><span class="fl">1.6630</span>, <span class="op">-</span><span class="fl">0.0718</span>, <span class="op">-</span><span class="fl">0.1285</span>, <span class="fl">0.5396</span>, <span class="op">-</span><span class="fl">0.0288</span>, <span class="op">-</span><span class="fl">0.8622</span>],</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>  [<span class="op">-</span><span class="fl">0.7413</span>, <span class="fl">1.7920</span>, <span class="op">-</span><span class="fl">0.0883</span>, <span class="op">-</span><span class="fl">0.6685</span>, <span class="fl">0.4745</span>, <span class="op">-</span><span class="fl">0.4245</span>]]</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Creamos una pequeña red con dos capas lineales y una función de activación sigmoidea en secuencia.</p>
<div id="6d09713a" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">6</span>, <span class="dv">4</span>),</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">4</span>, <span class="dv">1</span>),</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>  nn.Sigmoid()</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p>La primera capa toma seis características como entrada, genera cuatro.</p></li>
<li><p>La segunda capa procesa esto para obtener una probailidad final.</p></li>
</ul>
<p>El resultado de nuestra clasificación binaria es una única probabilidad entre cero y uno para cada uno de nuestros cinco animales.</p>
<div id="3f478088" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(input_data)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.5871],
        [0.6348],
        [0.6460],
        [0.5515],
        [0.5812]], grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>Recuerde que comúnmente utilizamos un umbral de 0.5 para convertirlos en etiquetaas de 0 y 1, es decir:</p>
<ul>
<li><p>Class = 1 para <span class="math inline">\(output \geq 0.5\)</span></p></li>
<li><p>Class = 0 para <span class="math inline">\(output \leq 0.5\)</span></p></li>
</ul>
<p>Esta salida no será significativa hasta que usemos retropropagación para actualizar los pesos y sesgos de las capas. Hablaremos más sobre esto más adelante.</p>
</section>
<section id="clasificación-multi-class-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="clasificación-multi-class-forward-pass">Clasificación Multi-Class: Forward Pass</h3>
<p>El modelo sería similar si quisiéramos ejecutar una clasificación de múltiples clases.</p>
<p>Digamos que estamos prediciendo tres clases: <em>mamíferos (Class 1), aves (Class 2) o reptiles (Class 3).</em></p>
<p>Específicamos que nuestro modelo tiene tres clases, estableciendo este valor como la dimensión de salida de la última capa lineal.</p>
<div id="0b85cd1d" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">6</span>, <span class="dv">4</span>),</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">4</span>, n_classes),</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>  nn.Softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Usamos Softmax en lugar de Sigmoid, con <span class="math inline">\(dim = -1\)</span> para indicar los 5 animales. Los anímales tiene la misma última dimensión que la salida de la última capa lineal.</p>
<div id="b9d28c67" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(input_data)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([5, 3])</code></pre>
</div>
</div>
<p>Utilizando la misma entrada que antes, la forma de salida es <span class="math inline">\(5\times 3\)</span>.</p>
<div id="f22297b6" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.2331, 0.1964, 0.5705],
        [0.2753, 0.2533, 0.4714],
        [0.3290, 0.2963, 0.3747],
        [0.2609, 0.2603, 0.4789],
        [0.3058, 0.2437, 0.4504]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
</div>
</div>
<p>Note que cuando imprimimos la salida, cada fila representa las probabilidades de tres clases, que suman uno. La etiqueta prevista para cada fila se asigna a la clase con la mayor probabilidad.</p>
<p>En nuestro ejemplo, todas las filas son mamíferos.</p>
</section>
<section id="regresión-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="regresión-forward-pass">Regresión: Forward Pass</h3>
<p>El último modelo que analizaremos es la regresión: predecir valores numéricos continuos.</p>
<p>Ahora usaremos las mismos datos para predecir el peso de los animales en función de sus propiedades.</p>
<div id="cef676e9" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">6</span>, <span class="dv">4</span>), </span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  nn.Linear(<span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(input_data)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[-0.1035],
        [ 0.1643],
        [ 0.2094],
        [ 0.6947],
        [ 0.2547]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>Esta vez no hay función de activación al final, y la última dimensión de la última capa lineal devuelve una salida con una característica.</p>
<p>Las dimensiones de salida son <span class="math inline">\(5\times 1\)</span>: cinco valores continuos, uno para cada fila.</p>
</section>
</section>
<section id="funciones-de-pérdida-para-evaluar-las-predicciones-del-modelo" class="level2">
<h2 class="anchored" data-anchor-id="funciones-de-pérdida-para-evaluar-las-predicciones-del-modelo">Funciones de Pérdida para Evaluar las Predicciones del Modelo</h2>
<p>Hemos generado predicciones ejecutando un paso hacia adelante, el siguiente paso es ver qué tan buenas son nuestras predicciones en comparación con los valores reales.</p>
<section id="función-de-pérdida" class="level4">
<h4 class="anchored" data-anchor-id="función-de-pérdida">Función de pérdida</h4>
<p>La función de pérdida, otro componente de las redes neuronales, nos dicen qué tan bueno es nuestro modelo para hacer predicciones durante el entrenamiento.</p>
<p>Toma una predicción del modelo <span class="math inline">\((\hat{y})\)</span> y una etiqueta verdadera <span class="math inline">\(y\)</span>, o dato real, y genera un dato flotante, tal como se puede apreciar en el siguiente esquema</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig30.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="800"></p>
</figure>
</div>
<p>Utilicemos nuestra multiclase</p>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Hair</th>
<th>Feathers</th>
<th>Eggs</th>
<th>Milk</th>
<th>Fins</th>
<th>Legs</th>
<th>Tail</th>
<th>Domestic</th>
<th>Catsize</th>
<th>Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>modelo de clasificación que predice si un animal es un mamífero (0), ave (1) o reptil (2).</p>
<ul>
<li><p>Si nuestro modelo predice que la clase es igual a cero, es correcto y el valor de la pérdida será bajo.</p></li>
<li><p>Una predicción incorrecta haría que el valor de la pérdida fuera alto.</p></li>
<li><p>Nuestro objetivo es minimizar las pérdidas.</p></li>
</ul>
</section>
<section id="calculo-de-la-pérdida" class="level4">
<h4 class="anchored" data-anchor-id="calculo-de-la-pérdida">Calculo de la pérdida</h4>
<p>La pérdida se calcula utilizando una función de pérdida, <span class="math inline">\(F\)</span>, que toma el dato real y el predicho, es decir,</p>
<p><span class="math display">\[
Loss = F(y, \hat{y})
\]</span></p>
<ul>
<li><p>En nuestro ejemplo de los animales, los valores posibles para nuestra verdadera clase de <span class="math inline">\(y\)</span> son los n<u>úmeros enteros 0, 1 o 2, es decir,</u> <span class="math inline">\(y \in \{0, 1 , 2\}\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{y}\)</span> es un tensor con las mismas dimensiones que el número de clases <span class="math inline">\(N\)</span>, es decir, <span class="math inline">\(\hat{y}\in \{-5.2, 4.6, 0.8\}\)</span>. Si <span class="math inline">\(N=3\)</span> entonces la salidad softmax es un tensor de forma <span class="math inline">\(1\times 3\)</span>.</p></li>
</ul>
</section>
<section id="codificación-one-hot" class="level4">
<h4 class="anchored" data-anchor-id="codificación-one-hot">Codificación one-hot</h4>
<p>Usamos codificación one-hot para convertir un entero <span class="math inline">\(y\)</span> en un tensor de ceros y unos para que podamos comparar para evaluar el rendimiento del modelo.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig31.png" class="img-fluid figure-img" width="300"></p>
<figcaption>Figura 1</figcaption>
</figure>
</div>
<p>Por ejemplo, si <span class="math inline">\(y=0\)</span> con tres clases, la forma codificada es 1, 0, 0 como se aprecia en <strong>Figura 1.</strong></p>
<p>Podemos importar <code>torch.nn.functional</code> como <code>F</code> para evitar la codificación one-hot manual.</p>
<div id="bb81b8b4" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(F.one_hot(torch.tensor(<span class="dv">0</span>), num_classes <span class="op">=</span> <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1, 0, 0])</code></pre>
</div>
</div>
<p>En el primer ejemplo, la verdad fundamental es cero (la primera clase). Tenemos 3 clases, por lo que la función genera un tensor de tres elementos con uno en la primera posición y ceros en el resto.</p>
<div id="5b208b92" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(F.one_hot(torch.tensor(<span class="dv">1</span>), num_classes <span class="op">=</span> <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0, 1, 0])</code></pre>
</div>
</div>
<p>Notemos ahora que si <span class="math inline">\(y=1\)</span> (la segunda clase), el tensor de salida tiene un uno en la segunda posición y ceros en caso contrario.</p>
<div id="d64a799a" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(F.one_hot(torch.tensor(<span class="dv">2</span>), num_classes <span class="op">=</span> <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0, 0, 1])</code></pre>
</div>
</div>
<p>Por último, si <span class="math inline">\(y=2\)</span> (tercera clase), el tensor de salida tiene un uno en la última posición y ceros en el resto de los casos.</p>
</section>
<section id="función-de-pérdida-cross-entropy-en-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="función-de-pérdida-cross-entropy-en-pytorch">Función de Pérdida Cross Entropy en PyTorch</h3>
<p>Una vez completada la codificación, podemos pasarla junto con nuestras predicciones a una función de pérdida. Lo que almacenaríamos será el tensor de <em>“puntuaciones”</em>.</p>
<div id="eb15c700" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> CrossEntropyLoss</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> torch.tensor([<span class="op">-</span><span class="fl">5.2</span>, <span class="fl">4.6</span>, <span class="fl">0.8</span>])</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>one_hot_target <span class="op">=</span> torch.tensor([<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La función de pérdida más comunmente utilizada para la clasificaci´øn es la pérdida de entropía cruzada.</p>
<p>Comencemos definiendo nuestra función de pérdida como <em>“criterio”.</em> Luego le pasamos el método <code>.double()</code> del tensor de puntuaciones y del tensor <code>one_hot_target.</code></p>
<div id="5dff0df1" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> CrossEntropyLoss()</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(criterion(scores.double(), one_hot_target.double()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(9.8222, dtype=torch.float64)</code></pre>
</div>
</div>
<p>Esto garantiza que los tensores estén en el formato correcto para la función de pérdida. La salida es el valor de pérdida calculado.</p>
<p>En resumen, la función de pérdida toma como entrada el tensor de puntuaciones, que es el modelo, predice antes de la función softmax final y la etiqueta de verdad codificada one-hot. Emite un único flotante, la pérdida de esa muestra.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig32.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="550"></p>
</figure>
</div>
<p>Recordemos que nuestro objetivo es minimizar esa pérdida.</p>
</section>
</section>
<section id="utilizar-derivadas-para-actualizar-los-parámetros-del-modelo" class="level2">
<h2 class="anchored" data-anchor-id="utilizar-derivadas-para-actualizar-los-parámetros-del-modelo">Utilizar derivadas para Actualizar los Parámetros del Modelo</h2>
<p>Veamos ahora cómo podemos minímizar la pérdida. Sabemos que un modelo predice mal cuando la pérdida es alta. Podemos utilizar derivadas o gradientes para minimizar esta pérdida.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig33.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="300"></p>
</figure>
</div>
<p>Imaginemos la función de pérdida como un valle. La derivada representa la pendiente, es decir qué tan pronunciada sube o baja la curva.</p>
<ul>
<li><p>Las pendientes pronunciadas, mostradas con flechas rojas, indican derivadas altas y pesos grandes.</p></li>
<li><p>Las pendientes más suaves, representadas por flechas verdes, tienen derivadas pequeñas y pesos más pequeños.</p></li>
<li><p>En el fondo del valle, mostrado por la flecha azul, la pendiente es plana y la derivada es cero. <strong>Este punto es el mínimo de la función de pérdida que pretendemos alcanzar.</strong></p></li>
</ul>
<section id="funciones-convexas-y-no-convexas" class="level3">
<h3 class="anchored" data-anchor-id="funciones-convexas-y-no-convexas">Funciones Convexas y No-Convexas</h3>
<p>Las funciones convexas tienen un mínimo global.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig34.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1000"></p>
</figure>
</div>
<p>Las funciones no convexas, como las funciones de pérdida, tienen múltiples mínimos locales, donde el valor es inferior al de los puntos cercanos pero no el más bajo en general.</p>
<p>Al minimizar las funciones de pérdida, nuestro objetivo es localizar el mínimo global cuando <span class="math inline">\(x\)</span> es aproximadamente uno.</p>
</section>
<section id="conexión-de-derivadas-y-entrenamiento-de-modelos" class="level3">
<h3 class="anchored" data-anchor-id="conexión-de-derivadas-y-entrenamiento-de-modelos">Conexión de derivadas y entrenamiento de modelos</h3>
<p>Durante el entrenamiento, ejecutamos un <strong><em>paso hacia adelante</em></strong> en las características y calculamos la pérdida comparando las predicciones con el valor objetivo.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig38.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500"></p>
</figure>
</div>
<p>Recuerde que los <strong><em>pesos</em></strong> y <strong><em>sesgos</em></strong> de las capas se inicializan aleatoriamente cuando se crea un modelo. Los actualizamos durante el entrenamiento mediante un <strong><em>paso hacia atrás</em></strong> o <strong><em>retropropogación.</em></strong></p>
<p>En el Deep Learning, <strong>las derivadas se conocen como gradientes.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig39.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
<p>Calculamos los gradientes de la función de pérdida y los usamos para actualizar los parámetros del modelo. Incluyendo pesos y sesgos, con retropropagación, repitiendo hasta que las capas esten sintonizadas.</p>
</section>
<section id="backpropagation-retropropagación" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-retropropagación">Backpropagation (Retropropagación)</h3>
<p>Durante la retropropagación, si consideramos una red de tres capas lineales:</p>
<ul>
<li><p>podemos calcular gradientes de pérdida locales con respecto a <span class="math inline">\(L_2\)</span></p></li>
<li><p>Usamos <span class="math inline">\(L_2\)</span> para calcular el gradiente <span class="math inline">\(L_1\)</span></p></li>
<li><p>Y repetimos hasta llegar a la capa inicial <span class="math inline">\(L_0\)</span>.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/fig40.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="300"></p>
</figure>
</div>
<p>Veamos esto con PyTorch:</p>
<pre class="{bash}"><code>model = nn.Sequential(nn.Linear(16, 8),
nn.Linear(8, 4),
nn.Linear(4, 2))
prediction = model(sample)

criterion = CrossEntropyLoss()
loss = criterion(prediction, target)
loss.backward</code></pre>
<p>Después de ejecutar un paso hacia adelante, definimos una función de pérdida, aquí <code>CrossEntropyLoss()</code> y úselo para comparar predicciones con valores objetivo.</p>
<p>Usando <code>.backward()</code>, calculamos gradientes basados en esta pérdida, que se almacenan en los atributos <code>.grad</code> de los pesos y <code>.bias</code> de los sesgos de cada capa.</p>
<pre class="{bash}"><code>model[0].weight.grad
model[0].bias.grad
model[1].weight.grad
model[1].bias.grad
model[2].weight.grad
model[2].bias.grad</code></pre>
<p>Cada capa del modelo se puede indexar comenzando desde cero para acceder a sus pesos, sesgos y gradientes.</p>
</section>
<section id="actualizar-manualmente-los-parámetros-del-modelo" class="level3">
<h3 class="anchored" data-anchor-id="actualizar-manualmente-los-parámetros-del-modelo">Actualizar Manualmente los Parámetros del Modelo</h3>
<p>Para actualizar manualmente los parámetros del modelo, accedemos al gradiente de cada capa.</p>
<pre class="{bash}"><code># Tasa de aprendizaje tipicamente pequeña
lr = 0.001

# updater the pesos
weight = model[0].weight
weight_grad = model[0].weight.grad

# update de sesgos 
bias = model[0].bias
bias_grad = model[0].bias.grad </code></pre>
<p>luego multiplicamos por la tasa de aprendizaje y restamos este producto del peso.</p>
<pre class="{bash}"><code>bias = bias - lr*bias_grad </code></pre>
<p>La tasa de aprendizaje es otro parámetros ajustable. Discutiremos esto y el ciclo de entranamiento más adelante en este documento.</p>
</section>
<section id="gradiente-descendente" class="level3">
<h3 class="anchored" data-anchor-id="gradiente-descendente">Gradiente Descendente</h3>
<ul>
<li><p>Utilizamos un mecanismo llamado <em>“gradiente desendiente”</em> para encontrar el mínimo global de las funciones de pérdida.</p></li>
<li><p>PyTorch simplifica esto con optimizadores, como el descenso de gradiente estocástico (SGD).</p></li>
</ul>
<div id="550b4c00" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Creamos el optimizador</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr <span class="op">=</span> <span class="fl">0.001</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p>Usamos <code>optim</code> para instanciar <code>SGD.</code></p></li>
<li><p><code>.parameters()</code> devuelve un iterable de todos los parámetros del modelo, que pasamos al optimizador.</p></li>
<li><p>Aquí utilizamos una tasa de aprendizaje estándar, “lr”.</p></li>
</ul>
<p>El optimizador calcula automáticamente los gradientes y actualiza los parámetros del modelo con <code>.step()</code></p>
<div id="fb05a7da" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/j-isaula\.github\.io\/Web_JI\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="Isaula/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2022 Juan Isaula</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>