{
  "hash": "a1c03569dc294d11c4635f49f17bc063",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deep Learning\"\nsubtitle: \"PyTorch\"\nauthor: \"Juan Isaula\"\ndate: \"2025-07-01\"\ncategories: [Python, PyTorch]\nimage: \"fondo.webp\"\n---\n\n\nEl Deep Learning está en todas partes, desde las cámaras de los smartphones hasta los asistentes de vos o los vehículos autónomos. En este curso, descubriras esta potente tecnología y aprenderás a aprovecharla con `PyTorch`, una de las bibliotecas de aprendizaje profundo más populares. Al finalizar tu recorrido por este documento, serás capaz de aprovechar PyTorch para resolver problemas de clasificación y regresión utilizando el aprendizaje profundo.\n\n# Introducción a PyTorch (biblioteca de Deep Learning)\n\nAntes de comenzar a crear modelos complejos, te haré conocer PyTorch, un librería de aprendizaje profundo. Aprenderás a manipular tensores, crear estructuras de datos de PyTorch y construir tu primera red neuronal en PyTorch con capas lineales.\n\nEl Deep Learning impulsa muchas innovaciones recientes y emocionantes, tales como la *traducción de idiomas*, *coches autónomos*, *diagnósticos médicos y chatbots.*\n\n![](img/fig1.png){fig-align=\"center\" width=\"600\"}\n\n## Qué es Deep Learning?\n\n![](img/fig2.png){fig-align=\"center\" width=\"200\"}\n\nDeep Learning (aprendizaje profundo) es un subconjunto del aprendizaje automático (machine learning). La estructura del modelo es una red de entradas (input), capas ocultas (hidden layers) y salidas (output), como se muestra en la siguiente imagen:\n\n![](img/fig3.png){fig-align=\"center\" width=\"200\"}\n\nComo apreciamos en la figura, una red puede tener una o muchas capas ocultas\n\n![](img/fig4.png){fig-align=\"center\" width=\"250\"}\n\nLa intuición original detrás del aprendizaje profundo era crear modelos inspirados en el cerebro humano, sobre todo por cómo aprende el cerebro humano: a través de células interconectadas llamadas neuronas. Es por esto que llamamos a los modelos de aprendizaje profundo **`Redes Neuronales`**.\n\n![](img/fig5.png){fig-align=\"center\" width=\"150\"}\n\nEstas estructuras de modelos en capas requieren muchos más datos en comparación con otros modelos de aprendizaje automático para derivar patrones. Generalmente hablamos de al menos cientos de miles de puntos de datos.\n\n## PyTorch: un framework del deep learning\n\n![](img/fig6.png){fig-align=\"center\" width=\"180\"}\n\nSi bien existen varios framework y paquetes para implementar el aprendizaje profundo en cuanto a algoritmos, nos centraremos en PyTorch, uno de los frameworks más populares y mejor mantenidos. *PyTorch fue desarrollado originalmente por Meta IA como parte del laboratorio de investigación de inteligencia artificial de Facebook antes de que pasara a depender de la fundación Linux.*\n\nEstá diseñado para ser intuitivo y fácil de usar, compartiendo muchas similitudes con la biblioteca de Python NumPy.\n\n#### PyTorch Tensors\n\nPodemos importar el módulo PyTorch llamando a\n\n::: {#a5d71cb1 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\n-   La estructura de datos fundamental en PyTorch es un tensor, que es similar a una matriz.\n\n-   Puede soportar muchas operaciones matemáticas y constituye un componente básico para nuestras redes neuronales.\n\n-   Se pueden crear tensores a partir de listas de Python o matrices NumPy utilizando la clase `torch.tensor()` esta clase convierte los datos a un formato compatible para el aprendizaje profundo.\n\n::: {#de5beaf6 .cell execution_count=2}\n``` {.python .cell-code}\nmi_lista = [[1,2,3], [4,5,6]]\ntensor = torch.tensor(mi_lista)\nprint(tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n```\n:::\n:::\n\n\n### Atributos de los Tensores\n\nPodemos llamar a `tensor.shape` para mostrar la forma de nuestro objeto recién creado.\n\n::: {#50604197 .cell execution_count=3}\n``` {.python .cell-code}\nprint(tensor.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([2, 3])\n```\n:::\n:::\n\n\nY `tensor.dtype()` para mostrar su tipo de datos, aquí un entero de 64 bits.\n\n::: {#f4d80cde .cell execution_count=4}\n``` {.python .cell-code}\nprint(tensor.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.int64\n```\n:::\n:::\n\n\nVerificar la forma y el tipo de datos garantiza que los tensores se alineen correctamente con nuestro modelo y tarea, y puede ayudarnos en caso de depuración.\n\n#### Operaciones con Tensores\n\nSe pueden sumar o restar tensores de PyTorch, siempre que sus formas sean compatibles.\n\n::: {#e0a62b70 .cell execution_count=5}\n``` {.python .cell-code}\na = torch.tensor([[1,1], [2,2]])\nb = torch.tensor([[2,2],[3,3]])\nc = torch.tensor([[2,2,2], [3,3,5]])\n```\n:::\n\n\n::: {#cf185c4d .cell execution_count=6}\n``` {.python .cell-code}\nprint(a + b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[3, 3],\n        [5, 5]])\n```\n:::\n:::\n\n\nCuando las dimensiones no son compatibles, obtendremos un error.\n\nTambién podemos realizar la multiplicación por elemento, lo que implica multiplicar cada elemento correspondiente.\n\n::: {#8d76806d .cell execution_count=7}\n``` {.python .cell-code}\nprint(a*b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[2, 2],\n        [6, 6]])\n```\n:::\n:::\n\n\nTambién esta incluida la multiplicación de matrices, que no es más que uno forma de combinar dos matrices para crear una nueva.\n\n::: {#2d50bd16 .cell execution_count=8}\n``` {.python .cell-code}\nprint(a @ b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[ 5,  5],\n        [10, 10]])\n```\n:::\n:::\n\n\nDetras de escena, los modelos de aprendizaje profundo realizan innumerables operaciones como la suma y multiplicación para procesar datos y aprender patrones.\n\n# Redes Neuronales y Capas\n\nVamos a contruir nuestra primer red neuronal usando tensores de PyTorch.\n\nUna red neuronal consta de capas de entrada, ocultas y de salida.\n\n![](img/fig7.png){fig-align=\"center\" width=\"200\"}\n\nLa **capa de entrada** contiene las características del conjunto de datos,\n\n![](img/fig8.png){fig-align=\"center\" width=\"200\"}\n\nLa **capa de salida** contiene las predicciones,\n\n![](img/fig9.png){fig-align=\"center\" width=\"200\"}\n\nY hay **capas ocultas (hidden layers)** en el medio\n\n![](img/fig10.png){fig-align=\"center\" width=\"200\"}\n\nSi bien una red puede tener cualquier cantidad de capas ocultas, comenzaremos construyendo una red sin capas ocultas donde la capa de salida es una capa lineal.\n\n![](img/fig11.png){fig-align=\"center\" width=\"200\"}\n\n-   Aquí, cada neurona de entrada se conecta a cada neurona de salida, lo que se denomina una red \"totalmente conectada\".\n\n-   Esta red es equivalente a un modelo lineal y nos ayuda a comprender los fundamentos antes de agregar complejidad.\n\nUsaremos el módulo `torch.nn` para construir nuestras redes. Esto hace que el código de la red sea más conciso y flexible y se importa convencionalmente como `nn`.\n\n::: {#475ed02c .cell execution_count=9}\n``` {.python .cell-code}\nimport torch.nn as nn\n```\n:::\n\n\nAl diseñar una red neuronal, las dimensiones de las capas de entrada y salida están predefinidas.\n\n-   La cantidad de neuronas en la capa de entrada es la cantidad de características en nuestro conjunto de datos.\n\n-   Y el número de neuronas en la capa de salida es el número de clases que queremos predecir.\n\nDigamos que creamos un input_tensor con forma de $1\\times 3$.\n\n::: {#d085aba8 .cell execution_count=10}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\ninput_tensor = torch.tensor(\n  [[0.3471, 0.4547, -0.2356]]\n)\n```\n:::\n\n\nPodemos pensar en esto como una fila con tres *\"carectísticas\"* o *\"neuronas\"* .\n\nA continuación, pasamos este input_tensor a una capa lineal, que aplica una función lineal para realizar predicciones.\n\n![](img/fig12.png){fig-align=\"center\" width=\"100\"}\n\nPara ello usaremos `nn.Linear()` toma dos argumentos: `int_features` es el número de características en nuestra entrada ( en este caso, tres) y `out_features` es el tamaño del tensor de salida (en este caso, dos).\n\n![](img/fig13.png){fig-align=\"center\" width=\"100\"}\n\n::: {#d8788657 .cell execution_count=11}\n``` {.python .cell-code}\nlinear_layer = nn.Linear(\n  in_features = 3,\n  out_features = 2\n)\n```\n:::\n\n\nEspecificar correctamente `in_features` garantiza que nuestra capa lineal pueda recibir el input_tensor.\n\nPor último, pasamos input_tensor a linear_layer para generar una salida.\n\n::: {#d67e84ca .cell execution_count=12}\n``` {.python .cell-code}\noutput = linear_layer(input_tensor)\nprint(output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[ 0.1243, -0.2168]], grad_fn=<AddmmBackward0>)\n```\n:::\n:::\n\n\nTenga en cuenta que esta salida tiene dos características o neuronas debido a las `out_features` especificadas en nuestra capa lineal.\n\nCuando input_tensor se pasa a linear_layer, se realiza una operación lineal para incluir pesos y sesgos.\n\n![](img/fig14.png){fig-align=\"center\" width=\"500\"}\n\n## Pesos (weights) y Sesgos (biases) \n\nCada capa lineal tiene un conjunto de pesos y sesgos asociados. Estas son las cantidades clave que definen una neurona.\n\n::: {#0b3c3943 .cell execution_count=13}\n``` {.python .cell-code}\nprint(linear_layer.weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter containing:\ntensor([[-0.0124,  0.5532,  0.4667],\n        [-0.2455,  0.3488,  0.2299]], requires_grad=True)\n```\n:::\n:::\n\n\n::: {#d24055c1 .cell execution_count=14}\n``` {.python .cell-code}\nprint(linear_layer.bias)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter containing:\ntensor([-0.0130, -0.2360], requires_grad=True)\n```\n:::\n:::\n\n\n-   Los pesos reflejan la importancia de diferentes características.\n\n-   El sesgos es un término adicional que es independiente de los pesos, y proporciona a las neurona una salida de referencia.\n\nAl principio, la capa lineal asigna pesos y sesgos aleatorios; estos se ajustan posteriormente.\n\nImaginemos nuestra red totalmente conectada en acción.\n\nDigamos que tenemos un conjunto de datos meteorológicos con tres características: *temperatura (temperature), humedad (humidity) y viento (wind).* Y queremos predecir si *lloverá (rain) o estará nublado (cloudy).*\n\n1.  La característica humeda tendrá un peso más significativo en comparación a las demás características, ya que es un fuerte predictor de lluvia y nubes.\n\n2.  Los datos meteorológicos corresponden a una región tropical con alta probabilidad de lluvia, por lo que agrega un sesgo para tener en cuenta esta información de referencia.\n\nCon esta información, nuestro modelo hace una predicción.\n\n## Capaz y Parámetros Ocultos\n\nHasta ahora, hemos utilizado una capa de entrada y una capa de lineal. Ahora, agregaremos más capas para ayudar a la red a aprender patrones complejos.\n\n### Apilamiento de capaz con nn.Sequential()\n\nApilaremos tres capas lineales usando `nn.Sequential()`, un contenedor de PyTorch para apilar capas en secuencia. Esta red toma la entrada, la pasa a cada capa lineal en secuencia y devuelve la salida.\n\n\n```{bash}\nmodel = nn.Sequential(\n  nn.Linear(n_features, 8),\n  nn.Linear(8, 4),\n  nn.Linear(4, n_classes)\n)\n```\n\n\n-   En este caso, las capas dentro de `nn.Sequential()` son capas ocultas.\n\n-   `n_features` representa el número de características de entrada y `n_classes` representa el número de clases de salida, ambas definidas por el conjunto de datos.\n\n### Adición de capas \n\nPodemos añadir tantas capas ocultas como queramos.\n\n![](img/fig17.png){width=\"450\"}\n\nLa dimensión de cada capa coincide con la dimensión de salida de la anterior.\n\n::: {#d6b39754 .cell execution_count=15}\n``` {.python .cell-code}\nmodel = nn.Sequential(\n  nn.Linear(10, 18),\n  nn.Linear(18, 20),\n  nn.Linear(20, 5)\n)\n```\n:::\n\n\nEn nuestro ejemplo de tres capas, la primera capa toma 10 características y genera 18. La segunda toda 18 y genera 20. Finalmente, la tercera toma 20 y genera 5.\n\n### Las capas están hechas de neuronas\n\n![](img/fig18.png){fig-align=\"center\" width=\"200\"}\n\nUna capa está completamente conectada cuando cada neurona se vincula a todas las neuronas de la capa anterior, como se muestra en rojo en la figura.\n\nCada neurona es una capa lineal:\n\n-   realiza una operación lineal utilizando todas las neuonras de la capa anterior.\n\n-   Por tanto, una sola neurona tiene $N+1$ parámetros que se puede aprender, siendo la dimensión de salida la capa anterior, más 1 para el sesgo.\n\n### Parámetros y Capacidad del Modelo\n\nAumetar el número de capas ocultas aumenta el número total de parámetros en el modelo, también conocido como capacidad del modelo. Los modelos de mayor capacidad pueden manejar conjuntos de datos más complejos, pero su entrenamiento puede llevar más tiempo.\n\nUna forma eficaz de evaluar la capacidad de un modelo es calcular su número total de parámetros.\n\nVamos a desglosarlo con una red de dos capas,\n\n::: {#d757a4c8 .cell execution_count=16}\n``` {.python .cell-code}\nmodel = nn.Sequential(\n  nn.Linear(8, 4),\n  nn.Linear(4, 2)\n)\n```\n:::\n\n\n-   **La primera capa** tiene 4 neuronas, cada neurona tiene 8 pesos y un sesgo, lo que da como resultado 36 parámetros.\n\n-   La segunda capa tiene 2 neuronas, cada neurona tiene 4 pesos y un sesgo, para un total de 10 parámetros.\n\n-   Sumándolos todos, este modelo tiene 46 parámetros que se pueden aprender en total\n\nTambién podemos calcular esto en PyTorch usando el método `.numel()`. Este método devuelve el número de elementos de un tensor.\n\n::: {#3645caad .cell execution_count=17}\n``` {.python .cell-code}\ntotal = 0\nfor parameter in model.parameters():\n  total += parameter.numel()\n  \nprint(total)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n46\n```\n:::\n:::\n\n\n### Balance entre complejidad y eficiencia\n\n![](img/fig19.png){fig-align=\"center\" width=\"400\"}\n\nComprender el recuento de parámetros nos ayuda a equilibrar la complejidad y la eficiencia del modelo. Demasiados parámetros pueden dar lugar a tiempos de entrenamiento largos o sobreajuste, mientras que muy pocos pueden limitar la capacidad de aprendizaje.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}