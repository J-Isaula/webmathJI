{
  "hash": "0592815a0ee63824452951c2c43c6e4c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deep Learning\"\nsubtitle: \"PyTorch\"\nauthor: \"Juan Isaula\"\ndate: \"2025-07-01\"\ncategories: [Python, PyTorch]\nimage: \"fondo.webp\"\n---\n\n\nEl Deep Learning está en todas partes, desde las cámaras de los smartphones hasta los asistentes de vos o los vehículos autónomos. En este curso, descubriras esta potente tecnología y aprenderás a aprovecharla con `PyTorch`, una de las bibliotecas de aprendizaje profundo más populares. Al finalizar tu recorrido por este documento, serás capaz de aprovechar PyTorch para resolver problemas de clasificación y regresión utilizando el aprendizaje profundo.\n\n# Introducción a PyTorch (biblioteca de Deep Learning)\n\nAntes de comenzar a crear modelos complejos, te haré conocer PyTorch, un librería de aprendizaje profundo. Aprenderás a manipular tensores, crear estructuras de datos de PyTorch y construir tu primera red neuronal en PyTorch con capas lineales.\n\nEl Deep Learning impulsa muchas innovaciones recientes y emocionantes, tales como la *traducción de idiomas*, *coches autónomos*, *diagnósticos médicos y chatbots.*\n\n![](img/fig1.png){fig-align=\"center\" width=\"600\"}\n\n## Qué es Deep Learning?\n\n![](img/fig2.png){fig-align=\"center\" width=\"200\"}\n\nDeep Learning (aprendizaje profundo) es un subconjunto del aprendizaje automático (machine learning). La estructura del modelo es una red de entradas (input), capas ocultas (hidden layers) y salidas (output), como se muestra en la siguiente imagen:\n\n![](img/fig3.png){fig-align=\"center\" width=\"200\"}\n\nComo apreciamos en la figura, una red puede tener una o muchas capas ocultas\n\n![](img/fig4.png){fig-align=\"center\" width=\"250\"}\n\nLa intuición original detrás del aprendizaje profundo era crear modelos inspirados en el cerebro humano, sobre todo por cómo aprende el cerebro humano: a través de células interconectadas llamadas neuronas. Es por esto que llamamos a los modelos de aprendizaje profundo **`Redes Neuronales`**.\n\n![](img/fig5.png){fig-align=\"center\" width=\"150\"}\n\nEstas estructuras de modelos en capas requieren muchos más datos en comparación con otros modelos de aprendizaje automático para derivar patrones. Generalmente hablamos de al menos cientos de miles de puntos de datos.\n\n## PyTorch: un framework del deep learning\n\n![](img/fig6.png){fig-align=\"center\" width=\"180\"}\n\nSi bien existen varios framework y paquetes para implementar el aprendizaje profundo en cuanto a algoritmos, nos centraremos en PyTorch, uno de los frameworks más populares y mejor mantenidos. *PyTorch fue desarrollado originalmente por Meta IA como parte del laboratorio de investigación de inteligencia artificial de Facebook antes de que pasara a depender de la fundación Linux.*\n\nEstá diseñado para ser intuitivo y fácil de usar, compartiendo muchas similitudes con la biblioteca de Python NumPy.\n\n#### PyTorch Tensors\n\nPodemos importar el módulo PyTorch llamando a\n\n::: {#f1c3b879 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\n-   La estructura de datos fundamental en PyTorch es un tensor, que es similar a una matriz.\n\n-   Puede soportar muchas operaciones matemáticas y constituye un componente básico para nuestras redes neuronales.\n\n-   Se pueden crear tensores a partir de listas de Python o matrices NumPy utilizando la clase `torch.tensor()` esta clase convierte los datos a un formato compatible para el aprendizaje profundo.\n\n::: {#13f4d526 .cell execution_count=2}\n``` {.python .cell-code}\nmi_lista = [[1,2,3], [4,5,6]]\ntensor = torch.tensor(mi_lista)\nprint(tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n```\n:::\n:::\n\n\n### Atributos de los Tensores\n\nPodemos llamar a `tensor.shape` para mostrar la forma de nuestro objeto recién creado.\n\n::: {#5f1045d0 .cell execution_count=3}\n``` {.python .cell-code}\nprint(tensor.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([2, 3])\n```\n:::\n:::\n\n\nY `tensor.dtype()` para mostrar su tipo de datos, aquí un entero de 64 bits.\n\n::: {#6083b2cd .cell execution_count=4}\n``` {.python .cell-code}\nprint(tensor.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.int64\n```\n:::\n:::\n\n\nVerificar la forma y el tipo de datos garantiza que los tensores se alineen correctamente con nuestro modelo y tarea, y puede ayudarnos en caso de depuración.\n\n#### Operaciones con Tensores\n\nSe pueden sumar o restar tensores de PyTorch, siempre que sus formas sean compatibles.\n\n::: {#5654d979 .cell execution_count=5}\n``` {.python .cell-code}\na = torch.tensor([[1,1], [2,2]])\nb = torch.tensor([[2,2],[3,3]])\nc = torch.tensor([[2,2,2], [3,3,5]])\n```\n:::\n\n\n::: {#e3080301 .cell execution_count=6}\n``` {.python .cell-code}\nprint(a + b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[3, 3],\n        [5, 5]])\n```\n:::\n:::\n\n\nCuando las dimensiones no son compatibles, obtendremos un error.\n\nTambién podemos realizar la multiplicación por elemento, lo que implica multiplicar cada elemento correspondiente.\n\n::: {#cfb9c190 .cell execution_count=7}\n``` {.python .cell-code}\nprint(a*b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[2, 2],\n        [6, 6]])\n```\n:::\n:::\n\n\nTambién esta incluida la multiplicación de matrices, que no es más que uno forma de combinar dos matrices para crear una nueva.\n\n::: {#287cb982 .cell execution_count=8}\n``` {.python .cell-code}\nprint(a @ b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[ 5,  5],\n        [10, 10]])\n```\n:::\n:::\n\n\nDetras de escena, los modelos de aprendizaje profundo realizan innumerables operaciones como la suma y multiplicación para procesar datos y aprender patrones.\n\n## Redes Neuronales y Capas\n\nVamos a contruir nuestra primer red neuronal usando tensores de PyTorch.\n\nUna red neuronal consta de capas de entrada, ocultas y de salida.\n\n![](img/fig7.png){fig-align=\"center\" width=\"200\"}\n\nLa **capa de entrada** contiene las características del conjunto de datos,\n\n![](img/fig8.png){fig-align=\"center\" width=\"200\"}\n\nLa **capa de salida** contiene las predicciones,\n\n![](img/fig9.png){fig-align=\"center\" width=\"200\"}\n\nY hay **capas ocultas (hidden layers)** en el medio\n\n![](img/fig10.png){fig-align=\"center\" width=\"200\"}\n\nSi bien una red puede tener cualquier cantidad de capas ocultas, comenzaremos construyendo una red sin capas ocultas donde la capa de salida es una capa lineal.\n\n![](img/fig11.png){fig-align=\"center\" width=\"200\"}\n\n-   Aquí, cada neurona de entrada se conecta a cada neurona de salida, lo que se denomina una red \"totalmente conectada\".\n\n-   Esta red es equivalente a un modelo lineal y nos ayuda a comprender los fundamentos antes de agregar complejidad.\n\nUsaremos el módulo `torch.nn` para construir nuestras redes. Esto hace que el código de la red sea más conciso y flexible y se importa convencionalmente como `nn`.\n\n::: {#c9e62e3a .cell execution_count=9}\n``` {.python .cell-code}\nimport torch.nn as nn\n```\n:::\n\n\nAl diseñar una red neuronal, las dimensiones de las capas de entrada y salida están predefinidas.\n\n-   La cantidad de neuronas en la capa de entrada es la cantidad de características en nuestro conjunto de datos.\n\n-   Y el número de neuronas en la capa de salida es el número de clases que queremos predecir.\n\nDigamos que creamos un input_tensor con forma de $1\\times 3$.\n\n::: {#0449bd94 .cell execution_count=10}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\ninput_tensor = torch.tensor(\n  [[0.3471, 0.4547, -0.2356]]\n)\n```\n:::\n\n\nPodemos pensar en esto como una fila con tres *\"carectísticas\"* o *\"neuronas\"* .\n\nA continuación, pasamos este input_tensor a una capa lineal, que aplica una función lineal para realizar predicciones.\n\n![](img/fig12.png){fig-align=\"center\" width=\"100\"}\n\nPara ello usaremos `nn.Linear()` toma dos argumentos: `int_features` es el número de características en nuestra entrada ( en este caso, tres) y `out_features` es el tamaño del tensor de salida (en este caso, dos).\n\n![](img/fig13.png){fig-align=\"center\" width=\"100\"}\n\n::: {#e69b41b5 .cell execution_count=11}\n``` {.python .cell-code}\nlinear_layer = nn.Linear(\n  in_features = 3,\n  out_features = 2\n)\n```\n:::\n\n\nEspecificar correctamente `in_features` garantiza que nuestra capa lineal pueda recibir el input_tensor.\n\nPor último, pasamos input_tensor a linear_layer para generar una salida.\n\n::: {#234ea639 .cell execution_count=12}\n``` {.python .cell-code}\noutput = linear_layer(input_tensor)\nprint(output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[-0.2629,  0.7848]], grad_fn=<AddmmBackward0>)\n```\n:::\n:::\n\n\nTenga en cuenta que esta salida tiene dos características o neuronas debido a las `out_features` especificadas en nuestra capa lineal.\n\nCuando input_tensor se pasa a linear_layer, se realiza una operación lineal para incluir pesos y sesgos.\n\n![](img/fig14.png){fig-align=\"center\" width=\"500\"}\n\n## Pesos (weights) y Sesgos (biases) \n\nCada capa lineal tiene un conjunto de pesos y sesgos asociados. Estas son las cantidades clave que definen una neurona.\n\n::: {#c6426c16 .cell execution_count=13}\n``` {.python .cell-code}\nprint(linear_layer.weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter containing:\ntensor([[ 0.0809,  0.3966, -0.1009],\n        [ 0.3420,  0.2257, -0.0206]], requires_grad=True)\n```\n:::\n:::\n\n\n::: {#e24f86b0 .cell execution_count=14}\n``` {.python .cell-code}\nprint(linear_layer.bias)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter containing:\ntensor([-0.4951,  0.5586], requires_grad=True)\n```\n:::\n:::\n\n\n-   Los pesos reflejan la importancia de diferentes características.\n\n-   El sesgos es un término adicional que es independiente de los pesos, y proporciona a las neurona una salida de referencia.\n\nAl principio, la capa lineal asigna pesos y sesgos aleatorios; estos se ajustan posteriormente.\n\nImaginemos nuestra red totalmente conectada en acción.\n\nDigamos que tenemos un conjunto de datos meteorológicos con tres características: *temperatura (temperature), humedad (humidity) y viento (wind).* Y queremos predecir si *lloverá (rain) o estará nublado (cloudy).*\n\n1.  La característica humeda tendrá un peso más significativo en comparación a las demás características, ya que es un fuerte predictor de lluvia y nubes.\n\n2.  Los datos meteorológicos corresponden a una región tropical con alta probabilidad de lluvia, por lo que agrega un sesgo para tener en cuenta esta información de referencia.\n\nCon esta información, nuestro modelo hace una predicción.\n\n## Capaz y Parámetros Ocultos\n\nHasta ahora, hemos utilizado una capa de entrada y una capa de lineal. Ahora, agregaremos más capas para ayudar a la red a aprender patrones complejos.\n\n### Apilamiento de capaz con nn.Sequential()\n\nApilaremos tres capas lineales usando `nn.Sequential()`, un contenedor de PyTorch para apilar capas en secuencia. Esta red toma la entrada, la pasa a cada capa lineal en secuencia y devuelve la salida.\n\n\n```{bash}\nmodel = nn.Sequential(\n  nn.Linear(n_features, 8),\n  nn.Linear(8, 4),\n  nn.Linear(4, n_classes)\n)\n```\n\n\n-   En este caso, las capas dentro de `nn.Sequential()` son capas ocultas.\n\n-   `n_features` representa el número de características de entrada y `n_classes` representa el número de clases de salida, ambas definidas por el conjunto de datos.\n\n### Adición de capas \n\nPodemos añadir tantas capas ocultas como queramos.\n\n![](img/fig17.png){width=\"450\"}\n\nLa dimensión de cada capa coincide con la dimensión de salida de la anterior.\n\n::: {#17fb152d .cell execution_count=15}\n``` {.python .cell-code}\nmodel = nn.Sequential(\n  nn.Linear(10, 18),\n  nn.Linear(18, 20),\n  nn.Linear(20, 5)\n)\n```\n:::\n\n\nEn nuestro ejemplo de tres capas, la primera capa toma 10 características y genera 18. La segunda toda 18 y genera 20. Finalmente, la tercera toma 20 y genera 5.\n\n### Las capas están hechas de neuronas\n\n![](img/fig18.png){fig-align=\"center\" width=\"200\"}\n\nUna capa está completamente conectada cuando cada neurona se vincula a todas las neuronas de la capa anterior, como se muestra en rojo en la figura.\n\nCada neurona es una capa lineal:\n\n-   realiza una operación lineal utilizando todas las neuonras de la capa anterior.\n\n-   Por tanto, una sola neurona tiene $N+1$ parámetros que se puede aprender, siendo la dimensión de salida la capa anterior, más 1 para el sesgo.\n\n### Parámetros y Capacidad del Modelo\n\nAumetar el número de capas ocultas aumenta el número total de parámetros en el modelo, también conocido como capacidad del modelo. Los modelos de mayor capacidad pueden manejar conjuntos de datos más complejos, pero su entrenamiento puede llevar más tiempo.\n\nUna forma eficaz de evaluar la capacidad de un modelo es calcular su número total de parámetros.\n\nVamos a desglosarlo con una red de dos capas,\n\n::: {#1bdc12a9 .cell execution_count=16}\n``` {.python .cell-code}\nmodel = nn.Sequential(\n  nn.Linear(8, 4),\n  nn.Linear(4, 2)\n)\n```\n:::\n\n\n-   **La primera capa** tiene 4 neuronas, cada neurona tiene 8 pesos y un sesgo, lo que da como resultado 36 parámetros.\n\n-   La segunda capa tiene 2 neuronas, cada neurona tiene 4 pesos y un sesgo, para un total de 10 parámetros.\n\n-   Sumándolos todos, este modelo tiene 46 parámetros que se pueden aprender en total\n\nTambién podemos calcular esto en PyTorch usando el método `.numel()`. Este método devuelve el número de elementos de un tensor.\n\n::: {#31162a0f .cell execution_count=17}\n``` {.python .cell-code}\ntotal = 0\nfor parameter in model.parameters():\n  total += parameter.numel()\n  \nprint(total)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n46\n```\n:::\n:::\n\n\n### Balance entre complejidad y eficiencia\n\n![](img/fig19.png){fig-align=\"center\" width=\"400\"}\n\nComprender el recuento de parámetros nos ayuda a equilibrar la complejidad y la eficiencia del modelo. Demasiados parámetros pueden dar lugar a tiempos de entrenamiento largos o sobreajuste, mientras que muy pocos pueden limitar la capacidad de aprendizaje.\n\n# Hiperparámetros y arquitectura de redes neuronales \n\nPara entrenar una red neuronal en PyTorch, primero tendremos que entender componentes adicionales, como las funciones de activación y pérdida. Entonces nos daremos cuenta de que entrenar una red requiere reducir mínimo esa función de pérdida, lo que se hace calculando gradientes. Aprenderemos a utilizar estos gradientes para actualizar los parámetros de tu modelo.\n\n## Funciones de Activación \n\nHasta ahora hemos visto redes neuronales formadas únicamente por capas lineales.\n\n![](img/fig20.png){fig-align=\"center\" width=\"250\"}\n\nPodemos agregar **no linealidad** a nuestros modelos usando funciones de activación. Discutiremos dos funciones de activación:\n\n-   **Sigmoid** para clasificación binaria y,\n\n-   **Softmax** para clasificación multiclase.\n\nEsta no linealidad permite que las redes aprendan cosas más complejas, interacciones entre entradas y objetivos que son relaciones **no linealeales.**\n\nLlamaremos a la salida de la última capa lineal la **\"pre-activación\".** Salida, que pasaremos a funciones de activación para obtener la salida transformada.\n\n### Función Sigmoid\n\nLa función de activación sigmoidea se utiliza ampliamente para problemas de clasificación binaria. Digamos que estamos tratando de clasificar un animal como mamífero o no?. Tenemos tres datos: el número de extremidades, si pone huevos y si tiene pelo. Las dos últimas son variables binarias: 1 si es si, 0 si no.\n\n![](img/fig21.png){width=\"700\"}\n\nPasar la entrada a un modelo con dos capas lineales devuelve una única salida: el número 6, tal como apreciamos en la siguiente figura:\n\n![](img/fig22.png){fig-align=\"center\" width=\"700\"}\n\nEste número aún no es interpretable. **Tenemos que pasar el número 6 por la función sigmoide**, transformandolo en un rango que represente la probabilidad entre cero y uno.\n\n![](img/fig23.png){fig-align=\"center\" width=\"700\"}\n\nSi el resultado está más cerca de uno (mayor que 0.5), lo etiquetamos como clase uno (mamífero). Si fuese menor que 0.5 la predección sería cero (no un mamifero).\n\n![](img/fig24.png){fig-align=\"center\" width=\"700\"}\n\nAhora, implementemos sigmoide en PyTorch.\n\n::: {#179824cc .cell execution_count=18}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\ninput_tensor = torch.tensor([[6]])\nsigmoid = nn.Sigmoid()\noutput = sigmoid(input_tensor)\nprint(output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.9975]])\n```\n:::\n:::\n\n\nNormalmente, `nn.Sigmoid()` se agrega como el último paso en `nn.Sequential()`, transformando automáticamente la salida de la capa lineal final.\n\n::: {#da516419 .cell execution_count=19}\n``` {.python .cell-code}\nmodel = nn.Sequential(\n  nn.Linear(6, 4), # Primera capa lineal \n  nn.Linear(4, 1), # Segunda capa lineal\n  nn.Sigmoid()     # Función de activación\n)\n```\n:::\n\n\nCuriosamente, una red neuronal con solo capas lineales y una activación sigmoidea se comporta como una **Regresión Logística.** Más adelante agregaremos más capas y activaciones para comprender realmente el verdadero potencial del Deep Learning.\n\n### Función Softmax\n\nUsamos softmax, otra función de activación popular, para clasificación multiclase que implica más de dos etiquetas de clase.\n\nDigamos que tenemos tres clases:\n\n1.  Pajaro o Bird (0)\n\n2.  Mamífero o Mammal (1)\n\n3.  Reptil o Reptile (2)\n\n![](img/fig25.png){fig-align=\"center\" width=\"400\"}\n\nEn esta red, Softmax toma una dimensión tridimensional, salida de preactivación y genera una salida de la misma forma, una por tres.\n\nLa salida es una distribución de probabilidad:\n\n-   Por cada elemento está entre cero y uno, y\n\n-   los valores suman uno.\n\n![](img/fig26.png){fig-align=\"center\" width=\"350\"}\n\nAquí, la predicción es para la segunda clase, mamíferos, que tiene la probabilidad más alta 0.842.\n\n![](img/fig27.png){fig-align=\"center\" width=\"350\"}\n\nEn PyTorch, usamos `nn.Softmax()`\n\n::: {#25d2abc6 .cell execution_count=20}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\ninput_tensor = torch.tensor(\n  [[4.3, 6.1, 2.3]]\n)\n\nprobabilities = nn.Softmax(dim=-1)\n# dim = -1 indica que softmax se aplica a la última dimensión de input_tensor\noutput_tensor = probabilities(input_tensor)\nprint(output_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.1392, 0.8420, 0.0188]])\n```\n:::\n:::\n\n\nSimilar a sigmoide, softmax puede ser la última capa en nn.Sequential. \\\n\n## Paso hacia adelante\n\nHemos explorado tensores, redes pequeñas y funciones de activación. Ahora profundicemos en la generación de predicciones.\n\nEste proceso se llama **\"ejecutar un paso hacia adelante\"** a través de una red.\n\n### Qué es una paso hacia adelante (Forward Pass)?\n\nEs cuando los datos de entrada fluyen a través de una red neuronal en dirección hacia adelante, para producir resultados o predicciones, pasa a través de cada capa de red.\n\n![](img/fig28.png){fig-align=\"center\" width=\"350\"}\n\nLos calculos transforman los datos en nuevas representaciones en cada capa, que pasa a la siguiente capa hasta que se produce el resultado final.\n\nEl propósito del paso hacia adelante es pasar datos de entrada a través de la red y producir predicciones o resultados basados en los parámetros aprendidos del modelo, también conocidos como pesos y sesgos.\n\nEste proceso es esencial tanto para el entrenamiento como para realizar nuevas predicciones.\n\nEl resultado final puede ser clasificaciones binarias, clasificaciones multiclase o predicciones numéricas (regresiones).\n\n![](img/fig29.png){fig-align=\"center\" width=\"400\"}\n\nVeremos un ejemplo de cada uno.\n\nDigamos que tenemos datos de entrada de cinco animales, con seís características o neuronas por punto de datos.\n\n::: {#bb3358e2 .cell execution_count=21}\n``` {.python .cell-code}\ninput_data = torch.tensor(\n  [[-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n  [-0.9155, -0.0475, -1.3645, -0.6336, -1.9520, -0.3398],\n  [0.7406, 1.6763, -0.8511, 0.2432, 0.1123, -0.0633],\n  [-1.6630, -0.0718, -0.1285, 0.5396, -0.0288, -0.8622],\n  [-0.7413, 1.7920, -0.0883, -0.6685, 0.4745, -0.4245]]\n)\n```\n:::\n\n\nCreamos una pequeña red con dos capas lineales y una función de activación sigmoidea en secuencia.\n\n::: {#42c129ef .cell execution_count=22}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nmodel = nn.Sequential(\n  nn.Linear(6, 4),\n  nn.Linear(4, 1),\n  nn.Sigmoid()\n)\n```\n:::\n\n\n-   La primera capa toma seis características como entrada, genera cuatro.\n\n-   La segunda capa procesa esto para obtener una probailidad final.\n\nEl resultado de nuestra clasificación binaria es una única probabilidad entre cero y uno para cada uno de nuestros cinco animales.\n\n::: {#d3963d4e .cell execution_count=23}\n``` {.python .cell-code}\noutput = model(input_data)\nprint(output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.6410],\n        [0.3207],\n        [0.5740],\n        [0.3893],\n        [0.4906]], grad_fn=<SigmoidBackward0>)\n```\n:::\n:::\n\n\nRecuerde que comúnmente utilizamos un umbral de 0.5 para convertirlos en etiquetaas de 0 y 1, es decir:\n\n-   Class = 1 para $output \\geq 0.5$\n\n-   Class = 0 para $output \\leq 0.5$\n\nEsta salida no será significativa hasta que usemos retropropagación para actualizar los pesos y sesgos de las capas. Hablaremos más sobre esto más adelante.\n\n### Clasificación Multi-Class: Forward Pass\n\nEl modelo sería similar si quisiéramos ejecutar una clasificación de múltiples clases.\n\nDigamos que estamos prediciendo tres clases: *mamíferos (Class 1), aves (Class 2) o reptiles (Class 3).*\n\nEspecíficamos que nuestro modelo tiene tres clases, estableciendo este valor como la dimensión de salida de la última capa lineal.\n\n::: {#90cdef17 .cell execution_count=24}\n``` {.python .cell-code}\nn_classes = 3\nmodel = nn.Sequential(\n  nn.Linear(6, 4),\n  nn.Linear(4, n_classes),\n  nn.Softmax(dim=-1)\n)\n```\n:::\n\n\nUsamos Softmax en lugar de Sigmoid, con $dim = -1$ para indicar los 5 animales. Los anímales tiene la misma última dimensión que la salida de la última capa lineal.\n\n::: {#1f299b16 .cell execution_count=25}\n``` {.python .cell-code}\noutput = model(input_data)\nprint(output.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([5, 3])\n```\n:::\n:::\n\n\nUtilizando la misma entrada que antes, la forma de salida es $5\\times 3$.\n\n::: {#4d8313e4 .cell execution_count=26}\n``` {.python .cell-code}\nprint(output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.2687, 0.5419, 0.1894],\n        [0.2703, 0.5203, 0.2093],\n        [0.2233, 0.6265, 0.1503],\n        [0.2927, 0.4669, 0.2404],\n        [0.1486, 0.7431, 0.1084]], grad_fn=<SoftmaxBackward0>)\n```\n:::\n:::\n\n\nNote que cuando imprimimos la salida, cada fila representa las probabilidades de tres clases, que suman uno. La etiqueta prevista para cada fila se asigna a la clase con la mayor probabilidad.\n\nEn nuestro ejemplo, todas las filas son mamíferos.\n\n### Regresión: Forward Pass\n\nEl último modelo que analizaremos es la regresión: predecir valores numéricos continuos.\n\nAhora usaremos las mismos datos para predecir el peso de los animales en función de sus propiedades.\n\n::: {#e3d8861c .cell execution_count=27}\n``` {.python .cell-code}\nmodel = nn.Sequential(\n  nn.Linear(6, 4), \n  nn.Linear(4, 1)\n)\n\noutput = model(input_data)\nprint(output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[-0.1960],\n        [ 0.2619],\n        [-0.2026],\n        [ 0.2988],\n        [-0.3395]], grad_fn=<AddmmBackward0>)\n```\n:::\n:::\n\n\nEsta vez no hay función de activación al final, y la última dimensión de la última capa lineal devuelve una salida con una característica.\n\nLas dimensiones de salida son $5\\times 1$: cinco valores continuos, uno para cada fila.\n\n## Funciones de Pérdida para Evaluar las Predicciones del Modelo \n\nHemos generado predicciones ejecutando un paso hacia adelante, el siguiente paso es ver qué tan buenas son nuestras predicciones en comparación con los valores reales.\n\n#### Función de pérdida\n\nLa función de pérdida, otro componente de las redes neuronales, nos dicen qué tan bueno es nuestro modelo para hacer predicciones durante el entrenamiento.\n\nToma una predicción del modelo $(\\hat{y})$ y una etiqueta verdadera $y$, o dato real, y genera un dato flotante, tal como se puede apreciar en el siguiente esquema\n\n![](img/fig30.png){fig-align=\"center\" width=\"800\"}\n\nUtilicemos nuestra multiclase\n\n| Hair | Feathers | Eggs | Milk | Fins | Legs | Tail | Domestic | Catsize | Class |\n|------|----------|------|------|------|------|------|----------|---------|-------|\n| 1    | 0        | 0    | 1    | 0    | 4    | 0    | 0        | 1       | 0     |\n\nmodelo de clasificación que predice si un animal es un mamífero (0), ave (1) o reptil (2).\n\n-   Si nuestro modelo predice que la clase es igual a cero, es correcto y el valor de la pérdida será bajo.\n\n-   Una predicción incorrecta haría que el valor de la pérdida fuera alto.\n\n-   Nuestro objetivo es minimizar las pérdidas.\n\n#### Calculo de la pérdida \n\nLa pérdida se calcula utilizando una función de pérdida, $F$, que toma el dato real y el predicho, es decir,\n\n$$\nLoss = F(y, \\hat{y})\n$$\n\n-   En nuestro ejemplo de los animales, los valores posibles para nuestra verdadera clase de $y$ son los n[úmeros enteros 0, 1 o 2, es decir,]{.underline} $y \\in \\{0, 1 , 2\\}$.\n\n-   $\\hat{y}$ es un tensor con las mismas dimensiones que el número de clases $N$, es decir, $\\hat{y}\\in \\{-5.2, 4.6, 0.8\\}$. Si $N=3$ entonces la salidad softmax es un tensor de forma $1\\times 3$.\n\n#### Codificación one-hot\n\nUsamos codificación one-hot para convertir un entero $y$ en un tensor de ceros y unos para que podamos comparar para evaluar el rendimiento del modelo.\n\n![Figura 1](img/fig31.png){fig-align=\"center\" width=\"300\"}\n\nPor ejemplo, si $y=0$ con tres clases, la forma codificada es 1, 0, 0 como se aprecia en **Figura 1.**\n\nPodemos importar `torch.nn.functional` como `F` para evitar la codificación one-hot manual.\n\n::: {#9bc18695 .cell execution_count=28}\n``` {.python .cell-code}\nimport torch.nn.functional as F\n\nprint(F.one_hot(torch.tensor(0), num_classes = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([1, 0, 0])\n```\n:::\n:::\n\n\nEn el primer ejemplo, la verdad fundamental es cero (la primera clase). Tenemos 3 clases, por lo que la función genera un tensor de tres elementos con uno en la primera posición y ceros en el resto.\n\n::: {#1453319f .cell execution_count=29}\n``` {.python .cell-code}\nprint(F.one_hot(torch.tensor(1), num_classes = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([0, 1, 0])\n```\n:::\n:::\n\n\nNotemos ahora que si $y=1$ (la segunda clase), el tensor de salida tiene un uno en la segunda posición y ceros en caso contrario.\n\n::: {#d508467a .cell execution_count=30}\n``` {.python .cell-code}\nprint(F.one_hot(torch.tensor(2), num_classes = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([0, 0, 1])\n```\n:::\n:::\n\n\nPor último, si $y=2$ (tercera clase), el tensor de salida tiene un uno en la última posición y ceros en el resto de los casos.\n\n### Función de Pérdida Cross Entropy en PyTorch\n\nUna vez completada la codificación, podemos pasarla junto con nuestras predicciones a una función de pérdida. Lo que almacenaríamos será el tensor de *\"puntuaciones\"*.\n\n::: {#65b15a06 .cell execution_count=31}\n``` {.python .cell-code}\nfrom torch.nn import CrossEntropyLoss\n\nscores = torch.tensor([-5.2, 4.6, 0.8])\none_hot_target = torch.tensor([1,0,0])\n```\n:::\n\n\nLa función de pérdida más comunmente utilizada para la clasificaci´øn es la pérdida de entropía cruzada.\n\nComencemos definiendo nuestra función de pérdida como *\"criterio\".* Luego le pasamos el método `.double()` del tensor de puntuaciones y del tensor `one_hot_target.`\n\n::: {#bb0ef84f .cell execution_count=32}\n``` {.python .cell-code}\ncriterion = CrossEntropyLoss()\nprint(criterion(scores.double(), one_hot_target.double()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor(9.8222, dtype=torch.float64)\n```\n:::\n:::\n\n\nEsto garantiza que los tensores estén en el formato correcto para la función de pérdida. La salida es el valor de pérdida calculado.\n\nEn resumen, la función de pérdida toma como entrada el tensor de puntuaciones, que es el modelo, predice antes de la función softmax final y la etiqueta de verdad codificada one-hot. Emite un único flotante, la pérdida de esa muestra.\n\n![](img/fig32.png){fig-align=\"center\" width=\"550\"}\n\nRecordemos que nuestro objetivo es minimizar esa pérdida.\n\n## Utilizar derivadas para Actualizar los Parámetros del Modelo \n\nVeamos ahora cómo podemos minímizar la pérdida. Sabemos que un modelo predice mal cuando la pérdida es alta. Podemos utilizar derivadas o gradientes para minimizar esta pérdida.\n\n![](img/fig33.png){fig-align=\"center\" width=\"300\"}\n\nImaginemos la función de pérdida como un valle. La derivada representa la pendiente, es decir qué tan pronunciada sube o baja la curva.\n\n-   Las pendientes pronunciadas, mostradas con flechas rojas, indican derivadas altas y pesos grandes.\n\n-   Las pendientes más suaves, representadas por flechas verdes, tienen derivadas pequeñas y pesos más pequeños.\n\n-   En el fondo del valle, mostrado por la flecha azul, la pendiente es plana y la derivada es cero. **Este punto es el mínimo de la función de pérdida que pretendemos alcanzar.**\n\n### Funciones Convexas y No-Convexas \n\nLas funciones convexas tienen un mínimo global.\n\n![](img/fig34.png){fig-align=\"center\" width=\"1000\"}\n\nLas funciones no convexas, como las funciones de pérdida, tienen múltiples mínimos locales, donde el valor es inferior al de los puntos cercanos pero no el más bajo en general.\n\nAl minimizar las funciones de pérdida, nuestro objetivo es localizar el mínimo global cuando $x$ es aproximadamente uno.\n\n### Conexión de derivadas y entrenamiento de modelos \n\nDurante el entrenamiento, ejecutamos un ***paso hacia adelante*** en las características y calculamos la pérdida comparando las predicciones con el valor objetivo.\n\n![](img/fig38.png){fig-align=\"center\" width=\"500\"}\n\nRecuerde que los ***pesos*** y ***sesgos*** de las capas se inicializan aleatoriamente cuando se crea un modelo. Los actualizamos durante el entrenamiento mediante un ***paso hacia atrás*** o ***retropropogación.***\n\nEn el Deep Learning, **las derivadas se conocen como gradientes.**\n\n![](img/fig39.png){fig-align=\"center\" width=\"600\"}\n\nCalculamos los gradientes de la función de pérdida y los usamos para actualizar los parámetros del modelo. Incluyendo pesos y sesgos, con retropropagación, repitiendo hasta que las capas esten sintonizadas.\n\n### Backpropagation (Retropropagación)\n\nDurante la retropropagación, si consideramos una red de tres capas lineales:\n\n-   podemos calcular gradientes de pérdida locales con respecto a $L_2$\n\n-   Usamos $L_2$ para calcular el gradiente $L_1$\n\n-   Y repetimos hasta llegar a la capa inicial $L_0$.\n\n![](img/fig40.png){fig-align=\"center\" width=\"300\"}\n\nVeamos esto con PyTorch:\n\n\n```{bash}\nmodel = nn.Sequential(nn.Linear(16, 8),\nnn.Linear(8, 4),\nnn.Linear(4, 2))\nprediction = model(sample)\n\ncriterion = CrossEntropyLoss()\nloss = criterion(prediction, target)\nloss.backward\n```\n\n\nDespués de ejecutar un paso hacia adelante, definimos una función de pérdida, aquí `CrossEntropyLoss()` y úselo para comparar predicciones con valores objetivo.\n\nUsando `.backward()`, calculamos gradientes basados en esta pérdida, que se almacenan en los atributos `.grad` de los pesos y `.bias` de los sesgos de cada capa.\n\n\n```{bash}\nmodel[0].weight.grad\nmodel[0].bias.grad\nmodel[1].weight.grad\nmodel[1].bias.grad\nmodel[2].weight.grad\nmodel[2].bias.grad\n```\n\n\nCada capa del modelo se puede indexar comenzando desde cero para acceder a sus pesos, sesgos y gradientes.\n\n### Actualizar Manualmente los Parámetros del Modelo \n\nPara actualizar manualmente los parámetros del modelo, accedemos al gradiente de cada capa.\n\n\n```{bash}\n# Tasa de aprendizaje tipicamente pequeña\nlr = 0.001\n\n# updater the pesos\nweight = model[0].weight\nweight_grad = model[0].weight.grad\n\n# update de sesgos \nbias = model[0].bias\nbias_grad = model[0].bias.grad \n```\n\n\nluego multiplicamos por la tasa de aprendizaje y restamos este producto del peso.\n\n\n```{bash}\nbias = bias - lr*bias_grad \n```\n\n\nLa tasa de aprendizaje es otro parámetros ajustable. Discutiremos esto y el ciclo de entranamiento más adelante en este documento.\n\n### Gradiente Descendente \n\n-   Utilizamos un mecanismo llamado *\"gradiente desendiente\"* para encontrar el mínimo global de las funciones de pérdida.\n\n-   PyTorch simplifica esto con optimizadores, como el descenso de gradiente estocástico (SGD).\n\n::: {#7a8356e2 .cell execution_count=33}\n``` {.python .cell-code}\nimport torch.optim as optim\n\n# Creamos el optimizador\noptimizer = optim.SGD(model.parameters(), lr = 0.001)\n```\n:::\n\n\n-   Usamos `optim` para instanciar `SGD.`\n\n-   `.parameters()` devuelve un iterable de todos los parámetros del modelo, que pasamos al optimizador.\n\n-   Aquí utilizamos una tasa de aprendizaje estándar, \"lr\".\n\nEl optimizador calcula automáticamente los gradientes y actualiza los parámetros del modelo con `.step()`\n\n::: {#08bb476f .cell execution_count=34}\n``` {.python .cell-code}\noptimizer.step()\n```\n:::\n\n\n# Entrenar una red neuronal con PyTorch\n\nAhora que hemos visto los componentes clave de una red neuronal, entrenaremos una utilizando un bucle de entrenamiento. Exploraremos posibles problemas, como la fuga de gradiente, y aprenderemos estrategías para resolverlos, como funciones de activación alternativas y el análisis de la tasa de aprendizaje.\n\n### Inmmersión profunda en la carga de datos \n\nEl manejo eficiente de datos es clave para entrenar modelos de aprendizaje profundo (Deep Learnig)\n\n| animal_name | hair | feathers | eggs | milk | predator | legs | tail | type |\n|-------------|------|----------|------|------|----------|------|------|------|\n| sparrow     | 0    | 1        | 1    | 0    | 0        | 2    | 1    | 0    |\n| eagle       | 0    | 1        | 1    | 0    | 1        | 2    | 1    | 0    |\n| cat         | 1    | 0        | 0    | 1    | 1        | 4    | 1    | 1    |\n| dog         | 1    | 0        | 0    | 1    | 0        | 4    | 1    | 1    |\n| lizard      | 0    | 0        | 1    | 0    | 1        | 4    | 1    | 2    |\n\nNuestros datos de clasificación de animales estan en un archivo csv y se pueden cargar utilizando `pd.read_csv()`.\n\n::: {#b23e84a7 .cell execution_count=35}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nanimals = pd.read_csv('animal_dataset.csv', sep = \";\")\n```\n:::\n\n\nUsaremos pelo, plumas, huevos, leche, depredador, patas y cola como características para predecir todo el tipo de animal.\n\n-   La columna **animal_name** no es necesaria ya que los nombres no determinan la clasificación.\n\n-   Tenga en cuenta que la columna **type** tiene tres categorías: ave (0), mamífero (1) y reptil (2).\n\n::: {#fa0acd60 .cell execution_count=36}\n``` {.python .cell-code}\nfeatures = animals.iloc[:, 1:-1]\nX = features.to_numpy()\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0 1 1 0 0 2 1]\n [0 1 1 0 1 2 1]\n [1 0 0 1 1 4 1]\n [1 0 0 1 0 4 1]\n [0 0 1 0 1 4 1]]\n```\n:::\n:::\n\n\n-   Usaremos `.iloc` para seleccionar todas las columnas excepto la primera y la (animal_name) última (type), lo que nos dará nuestras características de entrada.\n\n-   Estos se convierten en una matriz NumPy (X), para un manejo más sencillo con PyTorch.\n\nDe manera similar, podemos extraer la última columna (type) y almacenarla como una matriz de nuestros valores objetivo, que representan las etiquetas de clase para cada animal, a esto lo llamaremos **y**.\n\n::: {#e83cde0c .cell execution_count=37}\n``` {.python .cell-code}\ntarget = animals.iloc[:, -1]\ny = target.to_numpy()\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0 0 1 1 2]\n```\n:::\n:::\n\n\n### TensorDataset\n\nUsaremos TensorDataset para preparar datos para los modelos de PyTorch.\n\n::: {#743205f2 .cell execution_count=38}\n``` {.python .cell-code}\nimport torch \nfrom torch.utils.data import TensorDataset\n```\n:::\n\n\nEsto nos permite almacenar nuestras características (X) y etiquetas destino (y) como tensores, lo que hace que sean fáciles de administrar.\n\n1.  Convertimos $X$ e $y$ en tensores usando el método tensor de PyTorch y los pasamos a TensorDataset\n\n\n    ::: {#df945f40 .cell execution_count=39}\n    ``` {.python .cell-code}\n    dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n    ```\n    :::\n    \n    \n2.  Para acceder a una muestra individual, utilizamos la indexación de corchetes.\n\n\n    ::: {#7181a8db .cell execution_count=40}\n    ``` {.python .cell-code}\n    input_sample, label_sample = dataset[0]\n    print('input sample:', input_sample)\n    print('input_sample:', label_sample)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    input sample: tensor([0, 1, 1, 0, 0, 2, 1])\n    input_sample: tensor(0)\n    ```\n    :::\n    :::\n    \n    \n    \\\n    \\\n    \\\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}