{
  "hash": "e7ea1f8cf4c3b805a55962a615e7577e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Aprendizaje No Supervisado\"\nsubtitle: \"Python\"\nauthor: \"Juan Isaula\"\ndate: \"2025-09-07\"\ncategories: [Python]\nimage: \"fondo.png\"\n---\n\n\nSupongamos que tiene una colección de clientes con diversas características, como *edad, ubicación e historial financiero*, y deseas descubrir patrones y clasificarlos en grupos. O quizás tengas un conjunto de textos, como páginas de Wikipedia, y quieras segmentarlos en categorías en función de su contenido. Este es el mundo del aprendizaje no supervizado, llamado así porque no estás guiando, o supervisando, el descubrimiento de patrones mediante alguna tarea de predicción, sino descubriendo la estructura oculta a partir de datos no etiquetados. El aprendizaje no supervizado engloba diversas técnicas de aprendizaje automático, desde la agrupación hasta la reducción de dimensiones y la factorización de matrices. En este artículo, aprenderás los fundamentos de **aprendizaje no supervizado** e implementarás los algoritmos esenciales utilizando `scikit-learn` y `SciPy`. Aprenderás a agrupar, transformar, visualizar y extraer información de conjuntos de datos no etiquetados.\n\n# Agrupación para la exploración de conjuntos de datos\n\nEl objetivo de esta sección es aprender a descubrir los grupos subyacentes (o clústeres) en un conjunto de datos. En esta sección aprendera a agrupar empresas utilizando sus cotizaciones bursátiles,y distinguir diferentes especies agrupando sus medidas.\n\n## Aprendizaje no supervizado \n\nEl aprendizaje no supervizado es una clase de técnica de aprendizaje automático para descrubrir patrones en los datos. Por ejemplo:\n\n-   Encontrar los grupos naturales de clientes en función de sus historiales de compras o buscar:\n\n-   Patrones y correlaciones entre estas compras, y utilizar estos patrones para expresar los datos en forma comprimida.\n\nEstos son ejemplos de técnicas de aprendizaje no supervizadas llamadas *agrupación* y *reducción de dimensiones.*\n\n> El aprendizaje no supervizado se define en oposición al aprendizaje supervizado.\n\n-   Un ejemplo de ***aprendizaje supervisado*** es utilizar las medidas de los tumores para clasificarlos como benignos o cancerosos. En este caso, el descubrimiento de patrones es guiado o supervizado, de modo que los patrones son lo más utilies posible para predecir la etiqueta: benigno o canceroso.\n\n-   El ***aprendizaje no supervisado***, por el contrario, es un aprendizaje sin etiquetas. Es puro descubrimiento de patrones, sin la guía de una tarea de predicción.\n\n### Iris dataset \n\nEl conjunto de datos iris consta de mediciones de muchas plantas iris de tres especies diferentes:\n\n-   Setosa\n\n-   Versicolor\n\n-   Virginica\n\nHay cuatro medidas:\n\n1.  Largo de pétalo (Petal length)\n\n2.  Ancho de pétalo (Petal width)\n\n3.  Largo de sépalo (Sepal length)\n\n4.  Ancho de sépalo (sepal width)\n\nEstas son las características del conjunto de datos.\n\n#### Matrices, características y muestras \n\nConjuntos de datos como este (iris) se escribiran como matrices numerosas bidimensionales.\n\n-   Las columnas de la matriz corresponderán a las características.\n\n-   Las medidas de plantas individuales son las muestras del conjunto de datos, estas corresponden a filas de la matriz.\n\n#### Iris datases es 4-dimensional \n\nLas muestras del conjunto de datos iris tienen cuatro medidas y, por lo tanto, corresponden a puntos en un espacio de cuatro dimensiones. Es decir:\n\n-   Dimensiones = número de características o features.\n\nNo podemos visualizar cuatro dimensiones directamente, pero utilizando técnicas de aprendizaje no supervizado aún podemos obtener información.\n\n### K-Means Clustering \n\nAgruparemos estas muestras utilizando la agrupación de k-Means. K-Means encuentra un número específico de grupos en las muestras. Está implementado en la biblioteca de `scikit-learn` o `sklearn`.\n\nVeamos KMeans en acción con el conjunto de datos iris.\n\n::: {#81e2484e .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd         \nfrom sklearn import datasets    # Librería utilizada para importar iris dataset\niris = datasets.load_iris()     # importamos el conjunto de datos iris\n\niris = pd.DataFrame(iris.data, columns = iris.feature_names) # Convertimos iris a dataframe\n```\n:::\n\n\nPara comenzar:\n\n1.   Importamos KMeans de scikit-learn.\n\n2.  Luego creamos un modelo KMeans, especificando la cantidad de clústeres que deseamos encontrar con `n_clusters` especificamos n_cluster = 3, ya que hay tres especies de iris.\n\n3.  Posteriormente, llamamos el método de ajuste del modelo `.fit()`, pasando la matriz del iris dataset. Esto ajusta el modelo a los datos, localizando y recordando las regiones donde ocurren los diferentes grupos.\n\n4.  Por último, podemos utilizar el método de predicción del modelo en este mismo conjunto de datos.\n\n::: {#fff6791a .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters = 3, random_state = 42)\nmodel.fit(iris)\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```{=html}\n<style>#sk-container-id-4 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-4 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-4 pre {\n  padding: 0;\n}\n\n#sk-container-id-4 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-4 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-4 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-4 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-4 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-4 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-4 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-4 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-4 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-4 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-4 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n#sk-container-id-4 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-4 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-4 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-4 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-4 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-4 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-4 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=3, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KMeans</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.cluster.KMeans.html\">?<span>Documentation for KMeans</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>KMeans(n_clusters=3, random_state=42)</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\n::: {#b5a28338 .cell execution_count=3}\n``` {.python .cell-code}\nlabels = model.predict(iris)\nprint(labels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0\n 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0\n 0 2]\n```\n:::\n:::\n\n\nEsto nos devuelve una etiqueta de grupo para cada muestra, que indica a qué grupo pertenece una muestra.\n\n#### Etiquetas de clúster para nuevas muestras \n\nSi alguien viene con un algún dataset iris nuevo, kMeans puede determinar a qué grupos pertenecen sin tener que empezar de nuevo.\n\nKMeans hace esto recordando la media (o promedio) de las muestras en cada grupo. Estos se llaman **centroides** se asignan nuevas muestras al grupo cuyo centroide esté más cercano.\n\nVoy a tomar 3 registros aleatorios del dataset iris y asumiremos que estas son muestras nuevas.\n\n::: {#cea91353 .cell execution_count=4}\n``` {.python .cell-code}\nnew_samples = iris.sample(n = 3, random_state = 42)\nprint(new_samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n73                 6.1               2.8                4.7               1.2\n18                 5.7               3.8                1.7               0.3\n118                7.7               2.6                6.9               2.3\n```\n:::\n:::\n\n\nPara asignar las nuevas muestras a los grupos existentes, pasaremos el conjunto de nueva muestra al método de predicción del modelo kmeans.\n\n::: {#dd2a2d48 .cell execution_count=5}\n``` {.python .cell-code}\nnew_labels = model.predict(new_samples)\nprint(new_labels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2 1 0]\n```\n:::\n:::\n\n\nComo puede observar, esto devuelve las etiquetas de grupo para los datos nuevos o muestra de datos nueva.\n\n> En la siguiente sección aprenderá cómo evaluar la calidad de su agrupación.\n\n#### Scatter Plots\n\nPor ahora, visualizaremos nuestra agrupación de las muestras de iris usando diagramas de dispersión. A continuación se muestra un diagrama de dispersión de la longitud del sépalo frente a la longitud del pétalo del dataset iris.\n\n![](img/fig1.png){fig-align=\"center\" width=\"300\"}\n\nCada punto representa una muestra de iris y está coloreada según el grupo de la muestra. Para crear un diagrama de dispersión como este, usaremos `PyPlot`.\n\n-   La longitud del sépalo está en la columna 0 de la matriz iris, mientras que la longitud de los pétalos está en la segunda columna.\n\n-   Y labels o etiquetas que encontramos previamente lo usamos para colorear por etiqueta de clúster como un paramétro en `.scatter()`.\n\n::: {#c27c3e5d .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt \n\nxs = iris.iloc[:, 0]   # Longitud de los sépalos \nys = iris.iloc[:, 2]   # Longitud de los pétalos\n\nplt.scatter(xs, ys, c = labels)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=558 height=411}\n:::\n:::\n\n\nAhora, calculemos las coordenadas de los centroides utilizando el atributo `.cluster_centers_` de `model`. y asignaremos la columna 0 de `centroids` a `centroids_x`, y la columna 2 de `centroids` a `centroids_y`. Posterior a ello, realizaremos un diagrama de dispersión de centroids_x y centroids_y, utilizando `marker = 'D'` (un rombo) como marcador especificando el parámetro marker. El tamaño de los marcadores en 50 utilizando `s = 50`.\n\n::: {#3df547d5 .cell execution_count=7}\n``` {.python .cell-code}\nxs = iris.iloc[:, 0]   # Longitud de los sépalos \nys = iris.iloc[:, 2]   # Longitud de los pétalos\n\nplt.scatter(xs, ys, c = labels, alpha = 0.5)\ncentroids = model.cluster_centers_\n\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,2]\n\nplt.scatter(centroids_x, centroids_y, marker = 'D', s = 50)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=558 height=411}\n:::\n:::\n\n\n## Evaluar una agrupación \n\nEn la sección anterior, utilizamos KMeans para agrupar el dataset de iris en tres grupos. Pero **¿cómo podemos evaluar la calidad de esta agrupación?**\n\nUn enfoque directo es comparar los grupos con las especies de iris. Primero aprenderá sobre esto, antes de considerar el problema de cómo medir la calidad de una agrupación de una manera que no requiera que nuestras muestras vengan preagrupadas en especies. Esta medida de calidad puede utilizarse más adelante para tomar una decisión informada sobre el número de conglomerados a buscar.\n\n### Iris: Clusters vs Especies\n\nEn primer lugar, comprobemos si los 3 grupos de muestras de iris tienen alguna correspondencia con la especie de iris.\n\nLa correspondencia se describe en esta tabla:\n\n![](img/fig2.png){fig-align=\"center\" width=\"400\"}\n\nExiste una columna para cada una de las tres especies de iris: setosa, versicolor y virginica, y una fila para cada una de las tres etiquetas de grupo: 0, 1 y 2.\n\nLa tabla muestra el número de muestras que tienen cada combinación posible de etiquetas de grupo/especie. Por ejemplo:\n\n-   Vemos que el grupo 0 se corresponde perfectamente con la especie setosa.\n\n-   Por otro lado, mientras que el cluster 1 contiene principalmente muestras de virginica en el grupo 2.\n\nTablass como esta, se denominan **tabulaciones cruzadas** o **Cross tabulation**\n\n### Cross Tabulation con Pandas\n\nPara construir uno, usareos la biblioteca `pandas.` Como podemos observar en el bloque de código siguiente, creamos un dataframe de dos columnas, donde la primera columna son las etiquetas del grupo y la segunda son las especies de iris, de modo que cada fila proporciona la etiqueta del grupo y la especie de una sola muestra.\n\n::: {#59479dcb .cell execution_count=8}\n``` {.python .cell-code}\niris_raw = datasets.load_iris() \niris = pd.DataFrame(iris_raw.data, columns = iris_raw.feature_names)\n# Agregamos columna de especie verdadera al dataframe iiris\nspecies = pd.Categorical.from_codes(iris_raw.target, iris_raw.target_names)\n\n# \n\ndf = pd.DataFrame({'labels':labels, 'species':species})\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     labels    species\n0         1     setosa\n1         1     setosa\n2         1     setosa\n3         1     setosa\n4         1     setosa\n..      ...        ...\n145       0  virginica\n146       2  virginica\n147       0  virginica\n148       0  virginica\n149       2  virginica\n\n[150 rows x 2 columns]\n```\n:::\n:::\n\n\nAhora usamos la función de tabla cruzuda de pandas para crear la tubulación cruzada, pasando las dos columnas del DataFrame.\n\n::: {#8bac645b .cell execution_count=9}\n``` {.python .cell-code}\nct = pd.crosstab(df['labels'], df['species'])\nprint(ct)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspecies  setosa  versicolor  virginica\nlabels                                \n0             0           3         36\n1            50           0          0\n2             0          47         14\n```\n:::\n:::\n\n\nTabulaciones cruzadas como estás proporcionan información valiosa sobre qué tipo de muestras se encuentran en qué grupo. Pero la mayoría de los conjuntos de datos, las muestras no están etiquetadas por especie.\n\n**¿Cómo se puede evaluar la calidad de un clustering en estos casos?**\n\n### Medición de la calidad de la agrupación \n\nUna buena agrupación tiene grupos compactos, lo que significa que las muestras de cada grupo están agrupadas, no dispersas.\n\nLa **inercia** puede medir la distribución de las muestras dentro de cada grupo. Intuitivamente, la inercia mide qué tan lejos están las muestras de sus centroides. Puede encontrar la definición precisa en la documentación de scikit-learn. Queremos grupos que no estén dispersos, por lo que los valores más bajos de inercia son mejores.\n\nLa inercia de un modelo KMeans se mide automaticamente cuando se llama al metodo de ajuste .fit y luego están disponibles como atributos de `inertia_`.\n\nDe hecho, KMeans pretende colocar los clusters de forma que se minimice la inercia.\n\n::: {#a95d0d50 .cell execution_count=10}\n``` {.python .cell-code}\nmodel = KMeans(n_clusters = 3, random_state = 42)\nmodel.fit(iris)\nprint(model.inertia_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n78.8556658259773\n```\n:::\n:::\n\n\nA continuación se muestra un gráfico de los valores de inercia de las agrupaciones del conjunto de datos de iris con diferentes números de agrupaciones.\n\n![](img/fig3.png){fig-align=\"center\" width=\"400\"}\n\nNuestro modelo KMeans con 3 grupos tiene una inercia relativamente baja, lo cual es genial. Pero observe que la incercia continúa disminuyendo lentamente. Entonces, **¿cuál es la mejor cantidad de clústeres para elegir?** En última instacia se trata de una compensación.\n\nUna buena agrupación tiene agrupaciones estrechas (lo que significa inercia baja). Pero tampoco tiene demasiados grupos.\n\nUna buena regla general es elegir un codo en el gráfico de inercia, es decir, un punto donde la inercia comienza a disminuir más lentamente.\n\nPor ejemplo, según este criterio, 3 es un buen número de grupos para el conjunto de datos del iris.\n\n::: {#e8d16808 .cell execution_count=11}\n``` {.python .cell-code}\nks = range(1, 11)\ninertias = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k, random_state=42)\n    model.fit(iris)\n    inertias.append(model.inertia_)   # suma de distancias cuadradas intracluster\n\nplt.plot(ks, inertias, '-o')\nplt.xlabel('número de clusters')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=593 height=429}\n:::\n:::\n\n\n## Transformación de Características para mejorar agrupaciones \n\nVeamos ahora otro conjunto de datos, el conjunto de datos de los vinos del Piamonte.\n\n-   Disponemos de 178 muestras de vino tinto de la región de Piamonte de Italia.\n\n-   Las entradas o features miden la composición química (como el contendio del alcohol) y propiedades visuales como la intensidad del color.\n\n-   Las muestras proceden de 3 variedades distintas de vino.\n\n::: {#fb6522d8 .cell execution_count=12}\n``` {.python .cell-code}\nsamples = pd.read_csv(\"wine.csv\")\nvarieties = samples['class_name']\nsamples = samples.drop(\"class_name\", axis = 1)\nsamples\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class_label</th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>3</td>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>3</td>\n      <td>13.40</td>\n      <td>3.91</td>\n      <td>2.48</td>\n      <td>23.0</td>\n      <td>102</td>\n      <td>1.80</td>\n      <td>0.75</td>\n      <td>0.43</td>\n      <td>1.41</td>\n      <td>7.30</td>\n      <td>0.70</td>\n      <td>1.56</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>3</td>\n      <td>13.27</td>\n      <td>4.28</td>\n      <td>2.26</td>\n      <td>20.0</td>\n      <td>120</td>\n      <td>1.59</td>\n      <td>0.69</td>\n      <td>0.43</td>\n      <td>1.35</td>\n      <td>10.20</td>\n      <td>0.59</td>\n      <td>1.56</td>\n      <td>835</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>3</td>\n      <td>13.17</td>\n      <td>2.59</td>\n      <td>2.37</td>\n      <td>20.0</td>\n      <td>120</td>\n      <td>1.65</td>\n      <td>0.68</td>\n      <td>0.53</td>\n      <td>1.46</td>\n      <td>9.30</td>\n      <td>0.60</td>\n      <td>1.62</td>\n      <td>840</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>3</td>\n      <td>14.13</td>\n      <td>4.10</td>\n      <td>2.74</td>\n      <td>24.5</td>\n      <td>96</td>\n      <td>2.05</td>\n      <td>0.76</td>\n      <td>0.56</td>\n      <td>1.35</td>\n      <td>9.20</td>\n      <td>0.61</td>\n      <td>1.60</td>\n      <td>560</td>\n    </tr>\n  </tbody>\n</table>\n<p>178 rows × 14 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Clustering con Vinos \n\nTomaremos la matriz de muestra de los vinos y usaremos KMeans para encontrar 3 grupos\n\n::: {#d6b0c0b2 .cell execution_count=13}\n``` {.python .cell-code}\nmodel = KMeans(n_clusters = 3, random_state = 42)\nlabels = model.fit_predict(samples)\n```\n:::\n\n\n#### Cluster vs Varieties \n\nHay tres variedades de vinom así que usamos pandas para crear la tabla cruzada para comprobar las correspondencia entre la etiqueta del clúster y la variedad de vino.\n\n::: {#4e089f9f .cell execution_count=14}\n``` {.python .cell-code}\ndf = pd.DataFrame({'labels': labels, 'varieties':varieties})\nct = pd.crosstab(df['labels'], df['varieties'])\nprint(ct)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvarieties  Barbera  Barolo  Grignolino\nlabels                                \n0               37       1          64\n1               11      31           7\n2                0      27           0\n```\n:::\n:::\n\n\nComo podemos ver, esta vez las cosas no han salido tan bien. Los clusters KMeans no se corresponden bien con las variedades de vino.\n\n#### Variación de Características\n\nEl problema es que las características del conjunto de datos del vino tiene variaciones muy diferentes.\n\n> La varianza de una característica mide la dispersión de sus valores\n\n![](img/fig4.png){fig-align=\"center\" width=\"400\"}\n\nPor ejemplo, la característica del ácido málico (`malic_acid`) tiene una mayor varianza que la característica `0d280`, y esto también se puede ver en su diagrama de dispersión.\n\nLas diferencias en algunas de las variaciones de características son enormes, por ejmplo, en el diagrama de dispersión de características `od280` y `prolina`.\n\n![](img/fig5.png){fig-align=\"center\" width=\"300\"}\n\n### StandarScaler\n\nEn la agrupación en clústeres de KMeans, la varianza de una característica corresponde a su influencia en el algoritmo de agrupación en clústeres. Es decir:\n\n-   <div>\n\n    > Varianza de Carasterística = Influencia de Característica\n\n    </div>\n\nPara darle una oportunidad a cada característica, los datos deben transformarce para que las características tengan la misma varianza. Esto se puede lograr con el `StandarScaler` de scikit-learn. Transforma cada característica para que tenga media 0 y varianza 1.\n\nLas características **estandarizadas** resultantes pueden ser muy informativas.\n\nSi utilizamos, por ejemplo, los valores estandarizados de `od280` y `proline`, las tres variedades de vino son mucho más distintas.\n\n![](img/fig6.png){fig-align=\"center\" width=\"800\"}\n\nVeamos StandarScaler en acción:\n\n::: {#4d9c7d71 .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() # Creamos el objeto StandarScaler \nscaler.fit(samples)\nStandardScaler(copy = True, with_mean = True, with_std = True)\nsamples_scaled = scaler.transform(samples)\n```\n:::\n\n\nEl método de transformación ahora se puede utilizar para estandarizar cualquier muestra, ya sean las mismas o completamente nuevas.\n\n#### Métodos similares\n\n> Las API de StandarScaler y KMeans son similares, pero hay una diferencia importante:\n>\n> -   StandardScaler transforma datos y por eso tiene un método de transformación.\n>\n> -   KMeans por el contrario, asigna etiquetas de clúster a las muestras y esto se hace utilizando el método de predicción.\n\n### StandardScaler luego KMeans\n\nVolvamos al problema del agrupamiento de los vinos. Necesitamos realizar dos pasos:\n\n1.  Estandarizar los datos utilizando StandardScaler y\n\n2.  Tomar los datos estandarizados y agruparlos utilizando KMeans\n\nEsto se hace fácilmente combinando los dos pasos mediante un pipeline de scikit-learn, luego, los datos fluyen de un paso al siguiente de forma automática.\n\n#### Pipelines combinando multiples pasos\n\n::: {#40a52039 .cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\n\nscaler = StandardScaler()\nkmeans = KMeans(n_clusters = 3, random_state = 42)\n\n# Aplicamos los pasos que queremos combinar\npipeline = make_pipeline(scaler, kmeans)\npipeline.fit(samples)\n\n# Utilizamos el método de predicción para obtener las etiquetas del clúster\n\nlabels = pipeline.predict(samples)\n\n```\n:::\n\n\nAhora, realicemos la comprobación de la correspondencia entre las etiquetas de los cluster y las variedadesd de vino\n\n::: {#2e7592f0 .cell execution_count=17}\n``` {.python .cell-code}\ndf = pd.DataFrame({'labels': labels, 'varieties':varieties})\nct = pd.crosstab(df['labels'], df['varieties'])\nprint(ct)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvarieties  Barbera  Barolo  Grignolino\nlabels                                \n0                0       0          67\n1               48       0           1\n2                0      59           3\n```\n:::\n:::\n\n\n> Y como podemos observar, esto nos revela que al incorporar estandarización la nueva agrupación es fantástica. Sus tres clúster corresponden casi exactamente a las tres variedades de vino. Por tanto, esto es una gran mejora con respecto a la agrupación sin estandarización.\n\n**StandardScaler** es un ejemplo de un paso de *preprocesamiento*. Existen varios de estos disponibles en Scikit-Learn, por ejemplo, MaxAbsScaler y Normalizer.\n\n# **Visualización con agrupamiento Jerárquico y t-SNE**\n\nEn este capítulo, aprenderás dos técnicas de aprendizaje no suprvisado para la visualización de datos: la **agrupación jerárquica** y **t-SNE**.\n\n**La agrupación jerárquica** fusiona las muestras de datos en grupos cada vez más amplios, lo que da como resultado una visualización en forma de árbol de la jerarquía de grupos resultantes.\n\n**t-SNE** mapea las muestras de datos en un espacio bidimensional para que se pueda visualizar la proximidad antre ellas.\n\n## Visualizar Jerarquías \n\nUna gran parte del trabajo de cualquier cientifico de datos es la comunicación de los resultados o conocimiento a otras personas. Las visualizaciones son una excelente manera de compartir sus hallazgos especialmente con una audiencia no técnica. En este nuevo capítulo, aprenderá sobre dos métodos de aprendizaje no supervisado correspondientes a visualización:\n\n-   t-SNE\n\n-   Agrupamiento Jerárquico\n\n**t-SNE**, que lo veremos más adelante, crea un mapa 2D de cualquier conjunto de datos y transmite información útil sobre la proximidad de las muestras entre sí. Pero primero aprendamos sobre la agrupación jerárquica.\n\nYa has visto muchas agrupaciones jerárquicas en el mundo real. Por ejemplo, los seres vivos pueden organizarse en grupos pequeños y estrechos como los humanos, los simios, las serpientes y los lagartos, o en grupos más grandes y amplios, como los mamíferos y los reptiles, o incluso grupos más amplios, como los animales y las plantas. Como puede visualizar en la figura de abajo, estos grupos están contenidos unos dentro de otros y forman una jerarquía.\n\n![](img/fig7.png){fig-align=\"center\" width=\"450\"}\n\nDe manera análoga, la agrupación jerárquica organiza las muestras en una jerarquía de grupos.\n\nLa **agrupación jerárquica** puede organizar cualquier tipo de datos en una jerarquía, no sólo muestras de plantas y animales.\n\n### Dataset de scoring Eurovision\n\nConsideremos un nuevo tipo de conjunto de datos que describe cómo los países calificaron sus actuaciones en el Festival de la Canción de Eurovisión 2016. Los datos se organizan en una matriz rectangular, donde las filas de la matriz muestra cuántos puntos le dio un país a cada canción.\n\n![](img/fig8.png){fig-align=\"center\" width=\"400\"}\n\nLas *muestras* en este caso son los países. El resultado de aplicar la agrupación jerárquica al Festival de Eurovisión:\n\n![](img/fig9.png){fig-align=\"center\" width=\"500\"}\n\nLas puntuaciones se pueden visualizar como u diagrama en forma de árbol llamado ***dendrograma***. Esta única imagen revela mucha información sobre el comportamiento electoral de los países en Eurovisión. El dendrograma agrupa a los países en grupos cada vez más grandes, y muchos de ellos los grupos se reconocen inmediatamente como compuestos por países cercanos entre si geográficamente, o que tienen estrechos vínculos culturales o político, o que pertenecen a un solo grupo linguístico. De esta manera, la agrupación jerárquica puede generar excelentes visualizaciones. **¿Pero cómo funciona?**\n\nLa agrupación jerárquica se realiza en pasos:\n\n-   Al principio cada país es su propio clúster,por lo que hay tantos clústeres como países.\n\n-   En cada paso se fusionan los dos clústeres más cercanos. Esto disminuye el número de clústeres, y\n\n-   Al final, sólo queda un grupo, que contiene todos los países.\n\nEste proceso es en realidad un tipo particular de agrupamiento jerárquico llamdo **agrupamiento aglomerativo,** también existe el **agrupamiento divisivo**, que funciona a la inversa.\n\nAún no hemos definido qué significa que dos clústeres estén cerca, pero volveremos a abordar este tema más adelante.\n\n### Dendrograma de una Clusterización Jerárquica \n\nTodo el proceso de agrupamiento jerárquico está codificado en el dendrograma.\n\n![](img/fig9.png){fig-align=\"center\" width=\"500\"}\n\nEn la parte inferior, cada país se encuentra en un grupo propio. Luego el agrupamiento continúa desde abajo hacia arriba.\n\nLos grupos se representan como líneas verticales y una unión de líneas verticales indica una fusión de grupos.\n\nPara comprender mejor, hagamos un acercamiento:\n\n![](img/fig10.png){fig-align=\"center\" width=\"500\"}\n\nObservemos sólo una parte de este dendrograma,\n\n![](img/fig11.png){fig-align=\"center\" width=\"174\"}\n\nAl principio hay seis grupos , cada uno de los cuales contiene sólo un país. La primera fusión se produce aquí:\n\n![](img/fig12.png){fig-align=\"center\" width=\"174\"}\n\ndonde los grupos que contienen a Cyprus y Grecia se fusionan en un solo grupo. Posteriormente, este nuevo clúster se fusiona con el clúster que contiene Bulgaria:\n\n![](img/fig13.png){fig-align=\"center\" width=\"174\"}\n\nPoco después, los grupos que incluyen a Moldova y Rusia son fusionados,\n\n![](img/fig14.png){fig-align=\"center\" width=\"174\"}\n\nque más tarde a su vez se fusiona con el grupo que contiene Armenia.\n\n![](img/fig15.png){fig-align=\"center\" width=\"174\"}\n\nMás tarde aún, los dos grupos compuestos se fusionan, Este proceso continua hasta que sólo quede un grupo, y éste contenga todos los países.\n\n![](img/fig16.png){fig-align=\"center\" width=\"500\"}\n\n### Clusterización Jerárquica con SciPy\n\nEn el capítulo anterior, utilizamos la agrupación KMeans para agrupar empresas según los movimientos de sus cotizaciones bursátiles. Ahora, realizaremos la agrupación jerárquica de las empresas.\n\nlinkage() de SciPy realiza una agrupación jerárquica en una matriz de muestras.\n\n::: {#6102dfc6 .cell execution_count=18}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.preprocessing import normalize\nimport numpy as np\n\nempresas = pd.read_csv(\"/Users/juanisaulamejia/Documents/webmathJI/posts/Unsupervised_Learning/empresas.csv\")\n\ncompanies = empresas.iloc[:, 0].tolist()\n\nempresas = empresas.select_dtypes(include=\"number\").copy()\n\narr = empresas.to_numpy()\nnormalized_movements = normalize(arr)\n\n# Calculate the linkage: mergings\nmergings = linkage(normalized_movements, method = 'complete')\n\n# Plot the dendrogram\ndendrogram(mergings, labels = companies,\nleaf_rotation = 90,\nleaf_font_size = 6)\nplt.show()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=571 height=550}\n:::\n:::\n\n\nComo podemos observar, podemos crear visualizaciones geniales como esta con el clustering jerárquico, pero se puede usar ara más que solo visualizaciones.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}